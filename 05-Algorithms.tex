\include{templete}
\begin{document}
\part{Algorithms}
	\chapter{Computational Complexity}
		\section{Asymptotic Notation}
			\subsection{Asymptotic Analysis}
				\begin{definition}[asymptotically positive function]
					$f: \mathbb{N} \rightarrow \mathbb{R}$ is an asymptotically positive function if $\exists n_0 > 0$ such that $\forall n > n_0$ we have $f(n) > 0$
				\end{definition}

			\subsection{\texorpdfstring{$O$}{O}-Notation, \texorpdfstring{$\Omega$}{Omega}-Notation and \texorpdfstring{$\Theta$}{Theta}-Notation}
				\begin{definition}[$O$-Notation]
					For a function $g(n)$, 
					\begin{equation*}
						O(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that } f(n) \le cg(n), \forall n\ge n_0\}
					\end{equation*}
					$O$-Notation also known as asymptotic upper bound. 
				\end{definition}				

				\begin{definition}[$\Omega$-Notation]
					For a function $g(n)$, 
					\begin{equation*}
						\Omega(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that } f(n) \ge cg(n), \forall n\ge n_0\}
					\end{equation*}
					$\Omega$-Notation also known as asymptotic lower bound.
				\end{definition}
				
				\begin{definition}[$\Theta$-Notation]
					For a function $g(n)$, 
					\begin{equation*}
						\Theta(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that }  c_1g(n) \le f(n) \le c_2g(n), \forall n\ge n_0\}
					\end{equation*}
					$\Omega$-Notation and $\Theta$-Notation are not used very often when we talk about running times.
				\end{definition}
				
				\begin{definition}[$o$-Notation]
					For a function $g(n)$, 
					\begin{equation*}
						o(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that } f(n) < cg(n), \forall n\ge n_0\}
					\end{equation*}					
				\end{definition}

				\begin{definition}[$\omega$-Notation]
					For a function $g(n)$, 
					\begin{equation*}
						\omega(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that } f(n) > cg(n), \forall n\ge n_0\}
					\end{equation*}					
				\end{definition}

				\notice{$O(g)$, $\Omega(g)$ and $\Theta(g)$ are sets, we use ``$=$'' to represent ``$\in$''. In here ``$=$'' is asymmetric. Equality such as $O(n^3) = n^3 + n$ is incorrect.}

				\begin{example}
					The following are some examples
					\begin{align}
						\centering
						\begin{tabular}{|c|c||c|c|c|}
							\hline
							$f(n)$ & $g(n)$ & $O$ & $\Omega$ & $\Theta$\\
							\hline
							$4n^2 + 3n$ & $n^3 - 2n + 3$ & Yes & No & No \\
							\hline
							$\lg^{10} n$ & $n^{0.1}$ & Yes & No & No \\
							\hline
							$\log_{10} n$ & $\lg(n^3)$ & Yes & Yes & Yes \\
							\hline
							$\lceil \sqrt{10n + 100} \rceil$ & $n$ & Yes & No & No \\
							\hline
							$n^3 -100n$ & $10n^2\lg n$ & No & Yes & No\\
							\hline
							$2^n$ & $2^{\frac n2}$ & No & Yes & No \\
							\hline
							$\sqrt{n}$ & $n^{\sin n}$ & No & No & No\\
							\hline
						\end{tabular}
					\end{align}
				\end{example}

				\begin{theorem}
					Let $f$ and $g$ be two functions that
					\begin{equation*}
						\lim_{n\rightarrow \infty} \frac{f(n)}{g(n)} = c > 0
					\end{equation*}
					Then $f(n) = \Theta(g(n))$
				\end{theorem}

				\begin{proof}
					Since $\lim_{n\rightarrow \infty} \frac{f(n)}{g(n)}$ exists and positive, there is some $n_0$ beyond which the ratio is always between $\frac12 c$ and 2c. Thus,
					\begin{align}
						& \forall n > n_0, f(n) \le 2cg(n) \Rightarrow f(n) = O(g(n)) \\
						& \forall n > n_0, f(n) \ge \frac12cg(n) \Rightarrow f(n) = \Omega(g(n))
					\end{align}
				\end{proof}

				A set of properties:
				\begin{align}
					& f(n) = O(g(n)) \iff g(n) = \Omega(f(n))\\
					& f(n) = \Theta(g(n)) \iff f(n) = O(g(n)) \text{ and } g(n) = O(f(n))
				\end{align}

				Another set of properties:
				\begin{align}
					& f = O(g), g = O(h) \Rightarrow f=O(h) \\
					& f = \Omega(g), g = \Omega(h) \Rightarrow f=\Omega(h) \\
					& f = \Theta(g), g = \Theta(h) \Rightarrow f=\Theta(h)
				\end{align}

				\begin{theorem}
					If $f_i = O(h)$, for finite number of $i \in K$, then $\sum_{i\in K} f_i = O(h)$
				\end{theorem}

				Proof is trivia.

				\notice{Function $f$ and $g$ not necessarily have relation $f=O(g)$ or $g=O(f)$. E.g., for $f = \sqrt{n}$ and $g = n^{\sin n}$, $f \notin O(g)$ and $g \notin O(f)$ }

		\section{Common Running Times}
			The following are examples of common running times.
			\begin{align*}
				\centering
				\begin{tabular}{|c|p{10cm}|}
					\hline
					Running Time & Examples\\
					\hline
					$O(n)$ & Scan through a list to find a element matching with input\\
					\hline
					$O(\lg n)$ & Binary search\\
					\hline
					$O(n^2)$ & Scan through every pair of elements, $\binom{n}{2}$\\
					\hline
					$O(n^3)$ & Matrix multiplication by definition\\
					\hline
					$O(n\lg n)$ & Many divide and conquer algorithm which in each step iteratively divide the problem into two part and solve the subproblem, for example mergesort.\\
					\hline
					$O(n!)$ & Enumerate all permutation, for example Hamiltonian Cycle Problem\\
					\hline
					$O(c^n)$ & Enumerate all elements in power set. ($O(2^n)$)\\
					\hline
					$O(n^n)$ & Enumerate all combinations. (Can't find good example yet)\\
					\hline
				\end{tabular}
			\end{align*}
			Here is a comparison between running times: $\lg n < \sqrt{n} < n < n\lg n < n^2 < n^{\sqrt{n}} < 2^n < e^n < n! < n^n$

	\chapter{General Strategies}
		\section{Greedy Algorithms}
			\subsection{Introduction}
				Greedy algorithm solve the problem incrementally. Its often for optimization problems. Solving optimization problem typically requires a sequences of steps, at each step, an irrevocable decision will be made, and makes the choice looks the best at the moment. Based on that, a small instance will be added to the problem and we solve it again.

				Greedy algorithm do not always yield optimal solution, but it usually can provide a relatively acceptable computational complexity. They often run in polynomial time due to the incrementally of instances. Sometimes even if we cannot guarantee the solution is optimal, we still use it in optimization, because of its cheap computation burden. One of the examples will be the constructive heuristic of VRP problems.

				Greedy algorithm usually gives polynomial time complexity, but that is not all the cases. In simplex method, each pivot is greedily searching through for the extreme point. Although simplex method usually gives us traceable computation running time, however, it is not a polynomial algorithm.

				The following is a very rough sketch of generic greedy algorithm
				\begin{algorithm}[!ht]
					\caption{Generic Greedy Algorithm}
					\begin{algorithmic}[1]
						\While {The instance is non-trivial}
							\State Make the choice using the greedy strategy
							\State Reduce the instance
						\EndWhile
					\end{algorithmic}
				\end{algorithm}

				If we can prove the following, we can claim the greedy algorithm. First, it need to prove that for ``current moment'', the strategy is safe, i.e., there is always an optimum solution that agrees with the decision made according to the strategy, this is usually difficult. Then, it need to show that the remaining task after applying the strategy is to solve a/many smaller instance(s) of the same problem.

				Although greedy algorithm is intuitive and usually leads to satisfactory complexity, however, for most of the problems there is \textbf{no} natural greedy algortihm that works.
			\subsection{Examples}
				\subsubsection{Interval Scheduling}
					For $n$ jobs, job $i$ starts at time $s_i$, and finishes at time $f_i$. $i$ and $j$ are compatible if $[s_i, f_i)$ and $[s_j, f_j)$ are disjoint. Find the maximum-size of mutually compatible jobs.

					The following is the greedy algorithm to solve this problem
					\begin{algorithm}[!ht]
						\caption{Interval Scheduling, $S(s, f, n)$}
						\begin{algorithmic}[1]
							\State Sort jobs by $f$
							\State $t \gets 0$, $S\gets \emptyset$
							\For {every $j\in[n]$ according to non-decreasing order of $f_j$}
								\If {$s_j \ge t$}
									\State $S \gets S \cup \{j\}$
									\State $t \gets f_j$
								\EndIf
							\EndFor
							\Return S
						\end{algorithmic}
					\end{algorithm}

					Now we proof the that it is safe to schedule the job $j$ with the earliest finish time, i.e., there is an optimum solution where the job $j$ with the earliest finish time is scheduled.

					\begin{proof}
						For arbitary optimum solution $S$, one of the following cases will happen:\\
						\textbf{Case 1: } $S$ contains $j$, done\\
						\textbf{Case 2: } $S$ does not contain $j$. Then the first job in $S$ can be replaced by $j$ to obtain another optimum schedule $S^\prime$.
					\end{proof}

				\subsubsection{Unit-length interval covering}
					Given a set of $n$ points $X = \{x_1, x_2, \cdots, x_n\}$ on the real line, WLOG, assuming the points have already been sorted. We want to use the smallest number of unit-length closed intervals to cover all the points in $X$. 

					The following is a greedy algorithm to find the set of unit-length intervals that cover all the points in real line.
					\begin{algorithm}[!ht]
						\caption{Cover points with unit-length intervals}
						\begin{algorithmic}[1]
							\State Initial, $S \gets \emptyset$
							\For {$i = 1, 2, ..., n$}
								\If {$x_i$ is not covered by any unit-length intervals}
									\State Add one unit-length interval starting from $x_i$, i.e., $S \gets S \cup \{x_i\}$
								\EndIf
							\EndFor
						\end{algorithmic}
					\end{algorithm}

					This strategy (algorithm) is a greedy algorithm because it build up the solution in steps, in each iteration it considers one more point to be covered, i.e., it is optimal for each step in the iteration. Now we prove this algorithm is ``safe''.

					\begin{proof}
						First we consider the case where there is only one point on the real line. Then the optimal number of unit-length interval will be 1, according to the algorithm, that interval will be started at that point.

						Then assuming for the case where there are $k$ points from left to right, i.e., $X = \{x_1, x_2, \cdots, x_k\}$, and $p$ unit-intervals is already the minimal number and placed by the algorithm, then the $(k+1)^{th}$ point can only be one of the following cases:

						\textbf{Case 1:} The $(k+1)^{th}$ point is covered by the $p^{th}$ unit-length interval. According to the strategy, no new unit-length interval will be needed, the number of unit-length interval for $k+1$ points will be the same as when there are $k$ points. Therefore in this case for $k+1$ points, $p$ is the minimal (optimal) number. So the strategy is ``safe'' in this case.

						\textbf{Case 2:} The $(k+1)^{th}$ point is not covered by the $p^{th}$ unit-length interval. According to the strategy, there will be one new unit-length interval added. Notice that $p$ unit-length intervals will not be feasible to cover $k+1$ points in this case, because if we move the $p^{th}$ unit-length interval to the right, it will not be able to cover at least one point which overlapped with the starting point of that unit-length interval. Since $p$ is infeasible and $p+1$ unit-length interval is feasible, then $p+1$ is the minimal (optimal) number. So the strategy is ``safe'' in this case.

						Notice that for the $k$ we have mentioned above, $k$ can start from 1 to infinite number of integer. So this strategy is ``safe'' in every cases.
					\end{proof}

		\section{Divide and Conquer}
			\subsection{Introduction}
				The divide-and-conquer algorithm contains three steps: divide, conquer and combine. Step one, divide instances into many smaller instances. Step two, conquer small instance by solving each of smaller instances recursively and separately. Step three, combine solutions to small instances to obtain a solution for the origin large instance.

				Divide and conquer can sometimes solve the problems that greedy algorithm cannot solve, but they often not strong enough to reduce exponential brute-force search down to polynomial time. What usually happen is that they reduce a running time that unnecessarily large, but already polynomial, down to a faster running time.

			\subsection{Master Theorem}
				The Master Theorem is useful in analyzing the running time of divide and conquer algorithm. Assume at each step we divide the origin problem of size $n$ into subproblems of size $n / b$, run for $a$ times of ``itself'' to conquer those subproblems with a combine operation of $O(n^c)$, then the total running time can be derived by the Master Theorem as following:

				\begin{theorem}[Master Theorem]
					For running time in forms of $T(n) = aT(n/b) + O(n^c)$, where $a \ge 1$, $b > 1$, $c \ge 0$ are constants. Then
					\begin{equation}
						T(n) = \begin{cases}
							O(n^{\lg_b a}), \quad & c < \lg_b a\\
							O(n^c\lg n), \quad & c = \lg_b a\\
							O(n^c), \quad & c > \lg_b a
						\end{cases}
					\end{equation}
				\end{theorem}

				Proof of Master theorem using recursion tree
				\begin{figure}[h]
					\centering
					\begin{tikzpicture}[iv/.style={draw,rectangle,minimum size=20pt,inner sep=0pt,text=black}]
						\node[iv]{$n^c$}
							child{
								node[iv]{$(n/b)^c$}
								child{
									node[iv]{$(n/b^2)^c$}
								}
								child{
									node[iv]{$(n/b^2)^c$}
								}
							}
							child [missing]
							child{
								node[iv]{$(n/b)^c$}
								child{
									node[iv]{$(n/b^2)^c$}
								}
								child{
									node[iv]{$(n/b^2)^c$}
								}
							};
					\end{tikzpicture}
				\end{figure}
				\begin{proof}
					The $i^{th}$ level has $a^{i - 1}$ nodes. For the following cases, we can derive the time complexity:\\
					\textbf{Case 1: } $c < \lg_b a$ bottom-level dominates $(\frac{a}{b^c}^{\lg_b n})n^c = O(n^{\lg_b a})$\\
					\textbf{Case 2: } $c = \lg_b a$ all levels have same time $n^c \lg_b n = O(n^c \lg n)$\\
					\textbf{Case 3: } $c > \lg_b a$ top-level dominates $O(n^c)$
				\end{proof}
			\subsection{Examples}
				\subsubsection{Counting Inversions}

		\section{Dynamic Programming}
			\subsection{Introduction}
				The principal of dynamic programming is essentially opposite of the greedy algorithm. Dynamic programming implicitly explores the space of all possible solutions, by carefully decomposing the origin problem into subproblems, and store the solution of those subproblems. Base on those subproblems, then build up larger and larger problem until the origin problem is solved.

			\subsection{Examples}
				\subsubsection{Weighted Interval Scheduling}
					Consider $n$ jobs, job $i$ starts at time $s_i$ and finishes at time $f_i$, each job has a weight of $v_i > 0$. Job $i$ and job $j$ are compatible if $[s_i, f_i)$ and $[s_j, f_j)$ are disjoint. Find the maximum-size subset of mutually compatible jobs. A special case of this problem is when the values of all jobs are equal, which will be the interval scheduling problem as discussed in the greedy algorithm examples.

					Define $p(j)$ as for a job $j$, the largest index $i < j$ such that job $j$ is disjoint with job $i$. Define $opt(i)$ as the optimal value for instance only containing jobs $\{1, 2, \cdots, i\}$.

					Before the algorithm for weighted interval scheduling, sort the jobs by non-decreasing order of finishing time first, in $O(n \lg n)$ The dynamic programming algorithm is as following:
					\begin{algorithm}[h]
						\caption{ComputeOpt(i)}
						\begin{algorithmic}[1]
							\If {$i == 0$}
								\State \Return 0
							\Else
								\State \Return $\max\{v_i + ComputeOpt(p(i)), ComputeOpt(i - 1)\}$
							\EndIf
						\end{algorithmic}
					\end{algorithm}

					For finding $p(i)$ for one job, it takes $O(\lg n)$ by binary search. For $n$ jobs the complexity will be $O(n\lg n)$.

					The running time of this algorithm can be exponential in $n$, if each time $ComputeOpt(i)$ is computed repeatedly. However, if we store the value of each $ComputeOpt(i)$ and reuse it, we can reduce the running time to $O(n)$.

					We can recover the set of jobs for given (valid) $opt(i)$ by the following algorithm, assuming the jobs has been sorted by non-decreasing order of finishing time.
					\begin{algorithm}[h]
						\caption{RecoverJobs()}
						\begin{algorithmic}[1]
							\State Compute $p_1, p_2, \cdots, p_n$
							\State $opt(0) \gets 0$
							\For {$i \gets 1$ to $n$}
								\If {$opt(i - 1) \ge v_i + opt(p_i)$}
									\State $opt(i) \gets opt(i - 1)$
									\State $b[i] \gets N$								
								\Else
									\State $opt(i) \gets v_i + opt(p_i)$
									\State $b[i] \gets Y$
								\EndIf
							\EndFor
							\State $i \gets n, S\gets \emptyset$
							\While {$i \ne 0$}
								\If {$b[i] == N$}
									\State $i \gets i - 1$
								\Else
									\State $S \gets S \cup \{i\}$
									\State $i \gets p_i$
								\EndIf
							\EndWhile
							\State \Return S
						\end{algorithmic}
					\end{algorithm}

					The above algorithm is using memorized recursion to solve the problem, there is a second efficient algorithm to solve the Weighted Interval Scheduling Problem.

				\subsubsection{Subset Sum Problem}
					Given an interger bound $W > 0$ and a set of $n$ items, each with an integer weight $w_i > 0$, find a subset $S$ of items that
					\begin{align}
						\max \quad &\sum_{i \in S} w_i\\
						\text{s.t.} \quad & \sum_{i \in S} w_i \le W
					\end{align}

					Consider this instance, $i$ items, $(w_1, w_2, \cdots, w_i)$, budget is $W^\prime$. For $opt[i, W^\prime]$ there can only be one of the following cases:

					\textbf{Case 1: }The value of optimum solution does not contain $w_i$, then $opt[i, W^\prime] = opt[i - 1, W^\prime]$, else

					\textbf{Case 2: }The value of optimum solution contains $w_i$, then $opt[i, W^\prime] = opt[i - 1, W^\prime - w_i] + w_i$

					The algorithm is as following
					\begin{algorithm}[h]
						\caption{Optimum Subset}
						\begin{algorithmic}[1]
							\For {$W^\prime \gets 0$ to $W$}
								\State $opt[0, W^\prime] \gets 0$
							\EndFor
							\For {$i \gets 1$ to $n$}
								\For {$W^\prime \gets 0$ to $W$}
									\State $opt[i, W^\prime] \gets opt[i - 1, W^\prime]$
									\State $b[i, W^\prime] \gets N$
									\If {$w_i \le W^\prime$ and $opt[i - 1, W^\prime - w_i] + w_i \ge opt[i, W^\prime]$}
										\State $opt[i, W^\prime] \gets opt[i - 1, W^\prime - w_i] + w_i$
										\State $b[i, W^\prime] \gets Y$
									\EndIf
								\EndFor
							\EndFor
							\State \Return $opt[n, W]$
						\end{algorithmic}
					\end{algorithm}

					\begin{algorithm}[h]
						\caption{Recover the Optimum Set}
						\begin{algorithmic}[1]
							\State $i \gets n, W^\prime \gets W, S \gets 0$
							\While {$i > 0$}
								\If {$b[i, W^\prime] == Y$}
									\State $W^\prime \gets W^\prime - w_i$
									\State $S \gets \cup \{i\}$
								\EndIf
								\State $i \gets i - 1$
							\EndWhile
							\State \Return S
						\end{algorithmic}
					\end{algorithm}

				\subsubsection{Optimum Binary Search Tree}
					Given $n$ elements $e_1 < e_2 < \cdots < e_n$, $e_i$ has frequency $f_i$, the goal is to build a binary search tree for $\{e_1, e_2, \cdots, e_n\}$ with the minimum accessing cost
					\begin{equation}
						\sum_{i = 1}^n f_i d_i
					\end{equation}
					Where $d_i$ is the depth of $e_i$ in the tree.

					Suppose we choose $e_k$ to be the root, than $e_1, e_2, \cdots, e_{k-1}$ are in left-sub tree, and $e_{k+1}, \cdots, e_n$ will be in right-sub tree, then, denote the cost for the tree and subtrees to be $C, C_L, C_R$ respectively
					\begin{align}
						C &= \sum_{j = 1}^n f_j d_j\\
						&= \sum_{j = 1}^n f_j + \sum_{j = 1}^n f_j(d_j - 1)\\
						&= \sum_{j = 1}^n f_j + \sum_{j = 1}^{k - 1} f_j(d_j - 1) + \sum_{j = k + 1}^n f_j(d_j - 1)\\
						&= \sum_{j = 1}^n f_j + C_L + C_R
					\end{align}

					Denote $opt(i, j)$ to be the optimal value for the instance of $(e_i, e_{i + 1}, \cdots, e_j)$, then, for every $i, j$ such that $1 \le i \le j \le n$
					\begin{equation}
						opt(i, j) = \sum_{k = i}^j f_k + \min_{k:i \le k \le j}\{opt(i, k - 1) + opt(k + 1, j)\}
					\end{equation}

					Here is an example
					\begin{example}
						Consider the following optimum binary search tree instance. We have 5 elements $e_1, e_2, e_3, e_4$ and $e_5$ with $e_1 < e_2 < e_3 < e_4 < e_5$ and their frequencies are $f_1 = 5, f_2 = 25, f_3 = 15, f_4 = 10$ and $f_5 = 30$.  Recall that the goal is to find a binary search tree for the 5 elements so as to minimize $\sum_{i=1}^5 \textrm{depth}(e_i) f_i$, where $\textrm{depth}(e_i)$ is the depth of the element $e_i$ in the tree.  You need to output the best tree as well as its cost. You can try to complete the following tables and show the steps.  In the two tables, $opt(i,j)$ is the cost of the best tree for the instance containing $e_i, e_{i+1}, ..., e_j$ and $\pi(i, j)$ is the root of the best tree.
						\begin{table}[H]
							\centering
							\begin{tabular}{|c|c|c|c|c|c|}
								\hline
								\backslashbox{$i$}{$opt(i, j)$}{$j$} &  1 & 2  & 3 & 4 & 5 \\ \hline
								1 & 5 & 35 & 65 & 95 & 170\\\hline
								2&  & 25 & 55 & 85 & 155 \\\cline{1-1}\cline{3-6}
								3& \multicolumn{2}{c|}{} & 15 & 35 & 90\\\cline{1-1}\cline{4-6}
								4& \multicolumn{3}{c|}{} & 10 & 50 \\\cline{1-1}\cline{5-6}
								5& \multicolumn{4}{c|}{} & 30 \\\cline{1-1}\cline{6-6}
							\end{tabular}\qquad\qquad
							\begin{tabular}{|c|c|c|c|c|c|}
								\hline
								\backslashbox{$i$}{$\pi(i, j)$}{$j$} &  1 & 2  & 3 & 4 & 5 \\ \hline
								1 & 1 & 2 & 2 & 2 & 3\\\hline
								2&  & 2 & 2 & 2 (or 3)& 3 \\\cline{1-1}\cline{3-6}
								3& \multicolumn{2}{c|}{} & 3 & 3& 5\\\cline{1-1}\cline{4-6}
								4& \multicolumn{3}{c|}{} & 4 & 5\\\cline{1-1}\cline{5-6}
								5& \multicolumn{4}{c|}{} & 5 \\\cline{1-1}\cline{6-6}
							\end{tabular}
							\caption{$opt$ and $\pi$ tables for the optimum binary search tree instance. For cleanness of the table, we assume $opt(i, j) = 0$ if $j < i$ and there are not shown in the left table.}
						\end{table}
						\vspace*{-30pt}
						\begin{align*}
							opt(1, 2) &= \min\{0 + opt(2, 2), opt(1, 1) + 0\} + (f_1 + f_2) = \min\{25, 5\} + 5 + 25 = 35\\
							opt(2, 3) &= \min\{0 + opt(3, 3), opt(2, 2) + 0\} + (f_2 + f_3) = \min\{15, 25\} + 25 + 15 = 55\\
							opt(3, 4) &= \min\{0 + opt(4, 4), opt(3, 3) + 0\} + (f_3 + f_4) = \min\{10, 15\} + 15 + 10 = 35\\
							opt(4, 5) &= \min\{0 + opt(5, 5), opt(4, 4) + 0\} + (f_4 + f_5) = \min\{30, 10\} + 10 + 30 = 50\\
							opt(1, 3) &= \min\{0 + f(2, 3), f(1, 1) + f(3, 3), f(1, 2) + 0\} + (f_1 + f_2 + f_3) \\
							&= \min\{55, 20, 35\} + 5 + 25 + 15 = 65 \\
						    opt(2, 4) &= \min\{0 + f(3, 4), f(2, 2) + f(4, 4), f(2, 3) + 0\} + (f_2 + f_3 + f_4) \\ 
						    &= \min\{35, 35, 55\} + 25 + 15 + 10 = 85 \\
							opt(3, 5) &= \min\{0 + f(4, 5), f(3, 3) + f(5, 5), f(3, 4) + 0\} + (f_3 + f_4 + f_5) \\
							&= \min\{50, 45, 35\} + 15 + 10 + 30 = 90 \\
							opt(1, 4) &= \min\{0 + f(2, 4), f(1, 1) + f(3, 4), f(1, 2) + f(4, 4), f(1, 3) + 0\} \\ & \qquad + (f_1 + f_2 + f_3 + f_4) = 95\\
							&= \min\{85, 40, 45, 65\} + 5 + 25 + 15 + 10\\
							opt(2, 5) &= \min\{0 + f(3, 5), f(2, 2) + f(4, 5), f(2, 3) + f(5, 5), f(2, 4) + 0\} \\ & \qquad + (f_2 + f_3 + f_4 + f_5) = 155\\
							&= \min\{90, 75, 85, 85\} + 25 + 15 + 10 + 30 \\
							opt(1, 5) &= \min\{0 + f(2, 5), f(1, 1) + f(3, 5), f(1, 2) + f(4, 5), f(1, 3) + f(5, 5) + f(1, 4) + 0\} \\ & \qquad + (f_1 + f_2 + f_3 + f_4 + f_5) \\
							&= \min\{155, 95, 85, 95, 95\} + 5 + 25 + 15 + 10 + 30 = 170
						\end{align*}
					\end{example}

		\section{Compare between three paradigms}
			Greedy algorithm
			\begin{itemize}
				\item Make a greedy choice
				\item Prove that the greedy choice is safe
				\item Reduce the problem to a sub-problem and solve it iteratively
				\item Usually for optimization problems.
			\end{itemize}

			Divide-and-Conquer
			\begin{itemize}
				\item Break a problem into many independent sub-problems
				\item Solve each sub-problem separately
				\item Combine solutions for sub-problems to form a solution for the origin one
				\item Usually used to design more efficient algorithm
			\end{itemize}

			Dynamic Programming
			\begin{itemize}
				\item Break up a problem into many overlapping sub-problems
				\item Build solutions for larger and larger sub-problems
				\item Use a table to store solutions for sub-problems for reuse
			\end{itemize}

	\chapter{Sorting}
		\section{Exchange Sorts}
			\subsection{Bubble Sort}

			\subsection{Cocktail Shaker Sort}

			\subsection{Odd-even Sort}

			\subsection{Comb Sort}

			\subsection{Gnome Sort}

			\subsection{Quicksort}
				The idea of quicksort is to recursively divide an array into two subarrays, one with smaller number and one with large number, and concatenate  the subarrays after all subarrays are singletons.

				The most ideal way is to divide the subarrays equally, which requires an algorithm to find the median of an array of size $n$ in $O(n)$ time.

				The quicksort algorithm is as following
				\begin{algorithm}[h]
					\caption{Quicksort(A, n)}
					\begin{algorithmic}[1]
						\If {$n = 1$}
							\Return A
						\EndIf
						\State Initial, $A_L \gets \emptyset, A_H \gets \emptyset$
						\State $x \gets$ \texttt{Median(A)}
						\For {$element \in A$}
							\If {$element \le x$}
								\State $A_L \gets A_L \cup \{element\}$
							\Else
								\State $A_H \gets A_H \cup \{element\}$
							\EndIf
						\EndFor
						\State $B_L \gets$ \texttt{Quicksort($A_L$, $A_L$.size)}
						\State $B_H \gets$ \texttt{Quicksort($A_H$, $A_H$.size)}
						\State $t \gets$ number of times $element$ appear in $A$
						\Return $B_L + element^t + B_H$
					\end{algorithmic}
				\end{algorithm}

				Running time $T(n) = 2T(n/2) + O(n)$, $T(n) = O(n\lg n)$

				For the median finding algorithm
				\begin{algorithm}[h]
					\caption{Median(A)}
					\begin{algorithmic}[1]
						\State (To be finished)
					\end{algorithmic}
				\end{algorithm}

				If we don't use the median finding algorithm, we can modify the \texttt{Quicksort(A, n)} to be a random algorithm by replacing line 3 by $x \gets$ \texttt{RandomElement(A)}. This modified algorithm has an expected running time of $O(n\lg n)$. The worse case running time is $O(n^2)$.

				Based on Quicksort algorithm, we can define an $O(n)$ algorithm to find the $i$th smallest number in $A$, given that we have an $O(n)$ algorithm to find median of array.

				The selection algorithm is as follows
				\begin{algorithm}[h]
					\caption{Selection(A, n, i)}
					\begin{algorithmic}[1]
						\If {$n = 1$}
							\State \Return A
						\Else
							\State $x \gets$ \texttt{Median(A)}
							\For {$element \in A$}
								\If {$element \le x$}
									\State $A_L \gets A_L \cup \{element\}$
								\Else
									\State $A_H \gets A_H \cup \{element\}$
								\EndIf
							\EndFor
							\If {$i \le A_L.size$}
								\State \Return \texttt{Selection($A_L, A_L.size, i$)}
							\ElsIf {$i > n - A_R.size$}
								\State \Return \texttt{Selection($A_R, A_R.size, i - (n - A_R.size)$)}
							\Else
								\State \Return $x$
							\EndIf
						\EndIf
					\end{algorithmic}
				\end{algorithm}

				Similarly, without \texttt{Median(A)}, we can replace line 4 by $x \gets$ \texttt{RandomElement(A)}. Then the expected running time will be $O(n)$

		\section{Selection Sorts}
			\subsection{Selection Sort}

			\subsection{Heapsort}

			\subsection{Smoothsort}

			\subsection{Cartesian Tree Sort}

			\subsection{Tournament Sort}

			\subsection{Cycle Sort}

			\subsection{Weak-heap Sort}

		\section{Insertion Sorts}
			\subsection{Insertion Sort}

			\subsection{Shell Sort}

			\subsection{Splaysort}

			\subsection{Tree Sort}

			\subsection{Library Sort}

			\subsection{Patience Sorting}

		\section{Merge Sorts}
			\subsection{Merge Sort}
				Merge sort is a typical divide and conquer algorithm. It recursively separate an array into two subarrays, and sort while merging them. The algorithm is as following

				\begin{algorithm}[h]
					\caption{MergeSort(A, n)}
					\begin{algorithmic}[1]
						\If {$n = 1$}
							\Return A
						\Else
							\State $B \gets$ \texttt{MergeSort($A[0 .. \lfloor n/2 \rfloor], \lfloor n/2 \rfloor$)}
							\State $C \gets$ \texttt{MergeSort($A[\lceil n/2 \rceil .. n], \lceil n/2 \rceil$)}
						\EndIf
						\Return \texttt{Merge($B, C, \lfloor n/2 \rfloor, \lceil n/2 \rceil$}
					\end{algorithmic}
				\end{algorithm}

			\subsection{Cascade Merge Sort}

			\subsection{Oscillating Merge Sort}

			\subsection{Polyphase Merge Sort}

		\section{Distribution Sorts}
			\subsection{American Flag Sort}

			\subsection{Bead Sort}

			\subsection{Bucket Sort}

			\subsection{Burstsort}

			\subsection{Counting Sort}

			\subsection{interpolation Sort}

			\subsection{Pigenhole Sort}

			\subsection{Proxmap Sort}

			\subsection{Radix Sort}

			\subsection{Flashsort}

		\section{Concurrent Sorts}
			\subsection{Bitonic Sorter}

			\subsection{Batcher Odd-even Mergesort}

			\subsection{Pairwise Sorting Network}

			\subsection{Samplesort}

		\section{Hybird Sorts}
			\subsection{Block Merge Sort}

			\subsection{Timsort}

			\subsection{Spreadsort}

			\subsection{Merge-insertion Sort}

		\section{Please Don't Do that Sorts}
			\subsection{Slowsort}

			\subsection{Bogosort}

			\subsection{Stooge Sort}

	\chapter{Mathematical Algorithm}
		\section{Polynomial Multiplication}
			For given two polynomials of degree $n - 1$, the algorithm outputs the product of two polynomials.
			\begin{example}
				\begin{align}
					&(3x^3 + 2x^2 - 6x + 9) \times (-2x^3 + 7x^2 - 8x + 4)\\
					=& -6x^6 + 17x^5 + 24x^4 -60x^3 +119x^2 -96x +36
				\end{align}
				Then for input as (3, 2, -6, 9) and (-2, 7, -8, 4), the output will be (-6, 17, 2, -60, 119, -96, 36)
			\end{example}

			A naive algorithm to solve this problem will be $O(n^2)$
			\begin{algorithm}[h]
				\caption{PolyMultNaive(A, B, n)}
				\begin{algorithmic}[1]
					\State Let $C[k] = 0$ for every $k = 0, 1, \cdots, 2n - 2$
					\For {$i \gets 0$ to $n - 1$}
						\For {$j \gets 0$ to $n - 1$}
							\State $C[i + j] \gets C[i + j] + A[i] \times B[j]$
						\EndFor
					\EndFor
					\Return $C$
				\end{algorithmic}
			\end{algorithm}

			Use divide and conquer can reduce the running time. The idea is to divide the polynomial with degree of $n - 1$ (WLOG, let $n$ be even number) by two polynomials, i.e.
			\begin{equation}
				p(x) = p_H(x)x^{\frac{n}{2}} + p_L(x)
			\end{equation}
			Both $p_H$ and $p_L$ are polynomials with degree of $\frac{n}{2} - 1$, then
			\begin{align}
				p(x)q(x) &= (p_H(x)x^{\frac{n}{2}} + p_L(x)) \times (q_H(x)x^{\frac{n}{2}} + q_L(x))\\
						 &= p_Hq_H x^n + (p_Hq_L + p_Lq_H) x^{\frac{n}{2}} + p_Lq_L
			\end{align}
			Therefore
			\begin{align}
				multiply(p, q) &= multiply(p_H, q_H) x^n\\
							   &+ (multiply(p_H, q_L) + multiply(p_L, q_H)) x^{\frac{n}{2}} + multiply(p_L, q_L)\\
							   &= multiply(p_H, q_H) x^n\\
							   &+ (multiply(p_H + p_L, q_H + q_L) - multiply(p_H, q_H) - multiply(p_L, q_L)) x^{\frac{n}{2}} \\
							   &+ multiply(p_L, q_L)\\
			\end{align}

			The algorithm is as following
			\begin{algorithm}[h]
				\caption{PolyMultiDC(A, B, n)}
				\begin{algorithmic}[1]
					\If {$n = 1$}
						\Return $A[0]B[0]$
					\EndIf
					\State $A_L \gets A[0 .. n/2 - 1]$, $A_H \gets A[n/2 .. n - 1]$
					\State $B_L \gets B[0 .. n/2 - 1]$, $B_H \gets B[n/2 .. n - 1]$
					\State $C_L \gets $ \texttt{PolyMultiDC($A_L$, $B_L$, n/2)}
					\State $C_H \gets $ \texttt{PolyMultiDC($A_H$, $B_H$, n/2)}\
					\State $C_M \gets $ \texttt{PolyMultiDC($A_H$ + $A_L$, $B_H$ + $B_L$, n/2)}
					\State $C \gets $ 0 array of length $2n-1$
					\For {$i \gets 0$ to $n - 2$}
						\State $C[i] \gets C[i] + C_L[i]$
						\State $C[i + n] \gets C[i + n] + C_H[i]$
						\State $C[i + n/2] \gets C[i + n/2] + C_M[i] - C_L[i] - C_H[i]$
					\EndFor
					\Return $C$
				\end{algorithmic}
			\end{algorithm}

			The running time $T(n) = 3T(n/2) + O(n)$, $T(n) = O(n^{\lg_2 3})$
		\section{Matrices Multiplication}

		\section{Gaussian Elimination}

		\section{Curve Fitting}

		\section{Integration}

	\chapter{Searching}

	\chapter{String}
		\section{String Searching}

		\section{Pattern Matching}

		\section{Parse}

		\section{Optimal Caching}			
			\subsection{Offline Caching}
				\begin{definition}[Furthest-in-Future]
					(Les Belady, 1960s) When 
				\end{definition}

		\section{File Compression}

		\section{Cryptology}

	\chapter{Data Structures}
		\section{Elementary Data Structures}

		\section{Hash Tables}

		\section{Binary Search Trees}

		\section{Red-Black Trees}

		\section{B-Trees}

		\section{Fibonacci Heaps}

		\section{van Emde Boas Trees}

	\chapter{NP and PSPACE}
\end{document}