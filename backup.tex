\include{templete}
\begin{document}
	\maketitle
	\today

	\clearpage
	\thispagestyle{plain}
	\par\vspace*{.35\textheight}{\centering \textit{To My Beloved Motherland China}\par}

	\tableofcontents

	\part{Integer Programming}\label{IP}
		\chapter{Branch and Bound}
			\section{LP based Branch and Bound}
				\subsection{Idea of Divide and Conquer}
					For each iteration, divide the feasible region of LP into two feasible parts and an infeasible part, solve the LP in those parts.\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.5]
							\draw [<->] (10, 0) -- (0,0) -- (0,10);
							\draw (0, 8.5) -- (10, 2.5);
							\draw (2, 8) -- (9, 0);
							\draw [dashed] (3,0) -- (3,10);
							\draw [dashed] (4,0) -- (4,10);
							\node at (1.5, 5) {$S_1$};
							\node at (3.5, 4) {$S_2$};
							\node at (5, 3) {$S_3$}; 
						\end{tikzpicture}
						\caption{Divide and Conquer}
					\end{figure}
					In this iteration, the original feasible region have been partition into three parts, where $S_2$ is infeasible for IP because there is not integer point in it. We continue the iteration for $S_1$ and $S_2$. Each partition is suppose to give a new upper bound / lower bound and reduce the infeasible space.\\
					If the temp optimal integer in $S_1$ is larger than the LP relaxation in $S_3$, we can cut $S_3$.\\
					For each iteration, we use dual simple method, for the following two reasons:\\
					\indent - We can process new constraint very fast\\
					\indent - Always gives us a valid bound.\\

				\subsection{Relation Between LP Relaxation and IP}
					Let
					\begin{align}
						Z_{IP} &=\max_{x\in S} cx, \quad \text{where } s \text{ is a set of integer solutions} \\
						Z_{LP} &=\max cx, \quad \text{the LP relaxation of IP}  
					\end{align}
					then
					\begin{align}
						Z_{IP} &= \max_{1\le i \le k} \{\max_{x \in S_i} cx \} \\
						\text{iff} \quad S&=\bigcup_{1\le i \le k} S_i 
					\end{align}
					Notice that $S_i$ don\rq{}t need to be disjointed.\\
					\textbf{Important!} (For maximization problem)\\
					\indent - Any feasible solution provides a lower bound $L$, which is also the \textit{Prime Bound}\\
					\begin{equation}\hat{x}\in S \rightarrow Z_{IP}\ge c\hat{x}\end{equation}
					\indent - After branching, solving the LP relaxation over sub-feasible-region $S_i$ produces an upper bound, which is also the \textit{Dual Bound}, on each sub-problem\\
					\indent - If $u(S_i)\le L$, remove $S_i$\\
					\indent - LP can produce the first upper bound, but there might be possible to find other upper bound with other method (e.g. Lagrangian relaxation)

				\subsection{LP feasibility and IP(or MIP) feasibility}
					Solve the LP relaxation, one of the following things can happen\\
					\indent - LP is infeasible $\rightarrow$ MIP is infeasible\\
					\indent - LP is unbounded $\rightarrow$ MIP is infeasible or unbounded\\
					\indent - LP has optimal solution $\hat{x}$ and $\hat{x}$ are integer ($\hat{x} \in S$), $\rightarrow$ $Z_{IP} = Z_{LP}$\\
					\indent - LP has optimal solution $\hat{x}$ and $\hat{x}$ are not integer ($\hat{x} \notin S$), now defines a new upper bound, $Z_{LP} \ge Z_{IP}$\\
					If the first three happens, stop, if the fourth happens, we branch and recursively solve the sub-problems.

			\section{Terminology in Branch and Bound}
				- If we picture the sub-problems, they will form a \textbf{search tree} (typically a binary tree)\\
				- Each node in the search tree is a \textbf{sub-problem}\\
				- Eliminating a node is called \textbf{pruning}, we also stop considering its children\\
				- A sub-problem that has not being processed is called a \textbf{candidate}, we keep a list of candidates

			\section{Bounding}
			 	\textbf{Notice!} this section is for maximization, if it is for minimization, reverse upper bound and lower bound.
				\subsection{Upper Bound}
					- Upper bound it the Prime bound. which means it has to be a feasible solution\\
					- Some methods to get an upper bound:\\
					\indent - Rounding\\
					\indent - Heuristic\\
					\indent - Meta-heuristic

				\subsection{Lower Bound}
					- Lower Bound is the Dual bound,we can use LP relaxation to get it\\
					- The tighter the better, LP is better\\

			\section{Branch and Bound Algorithm}
				\begin{algorithm}[!ht]
					\caption{Branch and Bound}
					\begin{algorithmic}[1]
						\State find a feasible solution as the initial Lower bound $L$
						\State put the original LP relaxation in candidate list $S$
						\While {$S \ne \emptyset$}
							\State select a problem $\hat{S}$ from $S$
							\State solve the LP relaxation of $\hat{S}$ to obtain $u(\hat{S})$
							\If {LP is infeasible}
								\State $\hat{S}$ pruned by infeasibility
							\ElsIf {LP is unbounded}
								\State $\hat{S}$ pruned by unboundness or infeasibility
							\ElsIf{LP $u(\hat{S}) \le L$}
								\State $\hat{S}$ pruned by bound
							\ElsIf{LP $u(\hat{S}) > L$}
								\If {$\hat{x}\in S$}
									\State $u(\hat{S})$ becomes new $L$, $L=u(\hat{S})$
								\ElsIf {$\hat{x}\notin S$}
									\State branch and add the new sub-problems to $S$
									\If {LP $u(\hat{S})$ is at current best upper bound}
										\State set $U=u(\hat{S})$
									\EndIf
								\EndIf
							\EndIf
						\EndWhile
						\If {Lower bound exists}
							\State find the optimal at $L$
						\Else
							\State Infeasible
						\EndIf
					\end{algorithmic}
				\end{algorithm}

			\section{The goal of Branching}
				- Divide the problem into easier sub-problems\\
				- We want to chose the branching variables that minimize the sum of the solution times of the sub-problems\\
				- If after branching the $u(S_i)$ changes a lot,\\
				\indent - I can find a good L first\\
				\indent - The branch may get worse than the current bound first\\
				- Instead of solving the potential two branches for all candidates to optimality, solve a few iterations of the dual simplex, each iteration of pivoting yields an upper bound.

			\section{Choose Branching Variables}
				\subsection{The Most Violated Integrality constraint}
					Pick the $j$ of which $x_j - \lfloor \hat{x_j} \rfloor$ is closer to 0.5

				\subsection{Strong Branching}
					Select a few candidates $(K)$, create all sub-problems for each of these variables, run a few dual simplex iterations to see the improved bounds, select the variable with the best bounds.\\
					for variable $x_j\in K$, we branch and do a few iterations to find two reductions of gaps, i.e. $D_j^+$ and $D_j^-$,
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\hat{x_j}$};
							\node [left] at (-2.5, 2.5) {$x_j \le \lfloor \hat{x_j} \rfloor$};
							\node [right] at (2.5, 2.5) {$x_j \ge \lceil \hat{x_j} \rceil$};
							\draw [->] (-5, -1) -- (-5, -3);
							\draw [->] (5, -1) -- (5, -3);
							\node [below] at (-5, -3) {$D_j^+$};
							\node [below] at (5, -3) {$D_j^-$};
						\end{tikzpicture}
						\caption{Strong Branching}
					\end{figure}

				\subsection{pseudo-cost Branching}
					Pseudo-cost is an estimate of per-unit change in the objective function, for each variable
					\begin{equation}\begin{cases}P_j^+, & \text{bound reduction if rounded up} \\ P_j^-, & \text{bound reduction if rounded down}\end{cases}\end{equation}
					define $f_j = x_j -\lfloor x_j \rfloor$
					\begin{equation}\begin{cases}D_j^+ = P_j^+ (1-f_j) \\ D_j^- = P_j^- f_j\end{cases}\end{equation}

			\section{Choose the Node to Branch}
				\subsection{Update After Branching}
					For those variables in $K$ find the \\
					- $\max \{\min\{ {D_j^+},  {D_j^-}\}\}$, or\\
					- $\max \{\max\{ {D_j^+},  {D_j^-}\}\}$, or\\
					- $\max \{\frac{D_j^+ + D_j^-}{2}\}$, or\\
					- $\max \{\alpha_1\min\{ {D_j^+},  {D_j^-}\} + \alpha_2\max\{ {D_j^+},  {D_j^-}\}\}$\\
					to branch.

				\subsection{Branch on Important Variables First}
					Branch on variables that affects many decisions.

				\subsection{Some Search Strategy}
					- Best Bound First: select the node with the largest bound (good for closing the gap)\\
					- Deep First: Good for finding Lower bound and easier to do dual simplex\\
					- Mix: Start with \lq\lq{}Deep First\rq\rq{} until we find a good bound and do \lq\lq{}Best Bound First\rq\rq{}

			\section{Types of Branching}
				\subsection{Traditional Branching}
					For $\hat{x} \notin S$, $\exists j \in N$ such that
					\begin{equation}\hat{x_j} -\lfloor\hat{x_j}\rfloor > 0 \end{equation}
					Create two sub-problems
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\hat{x_j}$};
							\node [left] at (-2.5, 2.5) {$x_j \le \lfloor \hat{x_j} \rfloor$};
							\node [right] at (2.5, 2.5) {$x_j \ge \lceil \hat{x_j} \rceil$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

				\subsection{Constraint Branching}
					Use parallel constraints to branch, e.g.
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\hat{x}$};
							\node [left] at (-2.5, 2.5) {$x_1 - x_2 \ge 0$};
							\node [right] at (2.5, 2.5) {$-x_1 + x_2 \ge 1$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

				\subsection{SOS}
					For SOS1,
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\sum_{i=1}^{n}x_i=1$};
							\node [left] at (-2.5, 2.5) {$\sum_{i=1}^{k}x_i=1$};
							\node [right] at (2.5, 2.5) {$\sum_{i=k+1}^{n}x_i=1$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}
					For SOS2 (using the first definition),
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\sum_{i=1}^{n}x_i \le 1$};
							\node [left] at (-2.5, 2.5) {$\sum_{i=1}^{k-1}x_i = 0$};
							\node [right] at (2.5, 2.5) {$\sum_{i=k+1}^{n}x_i =  0$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

				\subsection{GUB}
					This is where $x_i \in \{0, 1\}$, at most one variable can be 1,
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\sum_{i=1}^{n}x_i\le 1$};
							\node [left] at (-2.5, 2.5) {$\sum_{i=1}^{k}x_i=0$};
							\node [right] at (2.5, 2.5) {$\sum_{i=k+1}^{n}x_i=0$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

				\subsection{Ryan-Foster}
					Ryan-Foster is for Set covering problem. The typical model is
					\begin{align}
						\min \quad & \sum_{i \in C} x_i \\
						\text{s.t.} \quad & \sum_{i \in C} a_{ij}x_{i} \ge 1, \quad \forall j \in U \\
								& x_i \in \{0, 1\}, \quad \forall i \in C 
					\end{align}
					\textbf{Observation} For any fractional solution, there are at least two elements $(i,j)$ so that $i$ and $j$ are both partially covered by the same set $S$, but there is another set $T$ that only covers $i$
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [left] at (-7, 2.5) {$(i,j)$ are};
							\node [left] at (-5, 0.5) { in the same set};
							\node [left] at (-4, 4.5) {if $a_{ik}a_{jk}=0 \Rightarrow x_k = 0$};
							\node [right] at (7, 2.5) {$(i,j)$ are};
							\node [right] at (5, 0.5) { in the different set};
							\node [right] at (3, 4.5) {if $a_{ik}=a_{jk}=1 \Rightarrow x_k=0$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

		\chapter{Branch and Cut}
			\section{Separation Algorithm}
				Basic idea is to separate the feasible region so that the current "solution" (which is an fractional solution) is not included in the feasible region.
				\subsection{Vertices Packing}
					The current solution is $\bar{x} \in [0,1]^n$, we have two options to do the separation:\\
					\textbf{Option 1 - find the maximum clique:}\\
					(This approach is as hard as the original problem)\\
					denote
					\begin{equation}
						y_i=\begin{cases}1, \text{ if } i \in C \\ 0, \text{ otherwise}\end{cases}
					\end{equation}
					Find the maximum clique via:
					\begin{align}
						\max \quad &\sum \bar{x_i} y_i  \\
						\text{s.t.} \quad & y_i + y_j \le 1, \forall \{i, j\} \notin E 
					\end{align}
					\textbf{Option 2 - Heuristic:}
					\begin{algorithm}[!ht]
						\caption{Heuristic method to find a clique}
						\begin{algorithmic}[1]
							\State find $v=\text{argmax}_{i\in V} \{\bar{x_i}\}, C=\{v\}$
							\While {$u\in \text{argmax}_{i \in \cap_{i \notin C}N_{(i, j)}\notin C} \{\bar {x_1}\}$ exists}
								\State C.add(u)
							\EndWhile
							\State return C
						\end{algorithmic}
					\end{algorithm}\\
					If $\sum_{i\in C} \bar{x_i} > 1$ then add cut $\sum_{i\in C} x_i \le 1$

				\subsection{TSP}
					When we have a solution, i.e. $\bar{x}$, perform the sub-tour searching algorithm, if there exists any sub-tour, add the corresponded constraint. That is the separation.

			\section{Optional v.s. Essential Inequalities}
				\subsection{Valid (Optional) Inequalities}
					See Figure \ref{OptInq}
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 2 cm]
							\node (LR) [circleNode] {LR};
							\node (CG) [process, below of=LR] {Cut Generation};
							\node (CLimit) [decision, below of=CG] {Generated?};
							\node (Cont) [process, below of=CG, xshift=4 cm] {Continue};
							\node (NLR) [circleNode, below of=CLimit] {New LR};
							\node (LNLR) [circleNode, below of=NLR, xshift=-2 cm] {Branch 1};
							\node (RNLR) [circleNode, below of=NLR, xshift=2 cm] {Branch 2};
							\draw [arrow] (LR) -- (CG);
							\draw [arrow] (CG) -- (CLimit);
							\draw [arrow] (CLimit) -- node [right] {yes} (NLR);
							\draw [arrow] (CLimit) -- node [below] {no} (Cont);
							\draw [arrow] (Cont) |- (CG);
							\draw [arrow] (NLR) -- (LNLR);
							\draw [arrow] (NLR) -- (RNLR);
						\end{tikzpicture}
						\caption {Branch and Cut for Optional Inequality}\label{OptInq}
					\end{figure}

				\subsection{Essential Inequalities (Lazy Cuts)}
					See Figure \ref{EssInq}
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1.8 cm]
							\node (IS) [circleNode, label = above:integer solution] {IS};
							\node (CG) [process, below of=IS] {Cut Generation};
							\node (CLimit) [decision, below of=CG] {Generated?};
							\node (FI) [circleNode, below of=CLimit, label = below:feasible integer solution] {FI};
							\node (LP) [process, below of=CG, xshift = 3.5 cm] {Solve Linear Relaxation};
							\node (LPF) [decision, below of=LP] {Feasible?};
							\node (NLR) [circleNode, below of=LPF] {New LR};
							\node (LNLR) [circleNode, below of=NLR, xshift=-2 cm] {Branch 1};
							\node (RNLR) [circleNode, below of=NLR, xshift=2 cm] {Branch 2};
							\node (Cont) [process, below of=LP, xshift=2.5 cm] {Continue};
							\draw [arrow] (IS) -- (CG);
							\draw [arrow] (CG) -- (CLimit);
							\draw [arrow] (CLimit) -- node [right] {no} (FI);
							\draw [arrow] (CLimit) -- node [below] {yes} (LP);
							\draw [arrow] (LP) -- (LPF);
							\draw [arrow] (LPF) -- node [right] {no} (NLR);
							\draw [arrow] (LPF) -- node [below] {yes} (Cont);
							\draw [arrow] (NLR) -- (LNLR);
							\draw [arrow] (NLR) -- (RNLR);
							\draw [arrow] (Cont) |- (CG);
						\end{tikzpicture}
						\caption {Branch and Cut for Essential Inequality}\label{EssInq}
					\end{figure}

			\section{Chvatal-Gomory Cut}
				\subsection{Chvatal-Gomory Rounding Procedure}
					For $x=P\cap \mathbb{Z}_+^n$, where $P=\{x\in \mathbb{R}_+^n|Ax \le b\}$, A is an mxn matrix with columns $\{a_1, ..., a_n\}$ and $u \in \mathbb{R}_+^n$\\
					- The inequality
					\begin{equation}
						\sum_{j=1}^n ua_jx_j\le ub 
					\end{equation}
					is valid\\
					- Therefore the inequality
					\begin{equation}
						\sum_{j=1}^n \lfloor ua_j \rfloor x_j \le ub 
					\end{equation}
					is valid\\
					- Furthermore, the inequality
					\begin{equation}
						\sum_{j=1}^n \lfloor ua_j \rfloor x_j \le \lfloor ub \rfloor 
					\end{equation}
					is valid.

				\subsection{Gomory Cutting Plane}
					For a IP problem
					\begin{align}
						\max \quad & cx  \\
						\text{s.t.} \quad & Ax=b  \\
							& x \in \mathbb{B}^n 
					\end{align}
					let $\bar{x}$ be an optimal basic solution for the LR of P.
					\begin{equation}
						\bar{x} = \left[\begin{matrix} B^{-1}b \\ 0 \end{matrix}\right] = \left[ \begin{matrix}x_B \\ x_N\end{matrix}\right] 
					\end{equation}
					We have
					\begin{align}
						& Bx_B + Nx_N = b \\
						\Rightarrow \quad & x_B + B^{-1}Nx_N=B^{-1}b  \\
						\Rightarrow \quad & x_B + [\bar{a}_1, \bar{a}_2, ...]x_N = \bar{b} \\
						\Rightarrow \quad & x_i + \sum_{j\in NB} \bar{a}_{ij}x_j = \bar{b}_i \quad \text{(for the $i$th row)} 
					\end{align}
					Assume that $x_i \in \{0, 1\}$, use CG-Procedure
					\begin{equation}
						x_i + \sum_{j \in NB} \lfloor \bar{a}_{ij} \rfloor x_j \le \lfloor \bar{b}_i \rfloor 
					\end{equation}
					is a valid constraint for $P$, furthermore,
					\begin{equation}
						(\bar{b}_i - \sum_{j\in NB} \bar{a}_{ij}x_j) + \sum_{j\in NB}\lfloor \bar{a}_{ij} \rfloor x_j\le \lfloor \bar{b}_i \rfloor 
					\end{equation}
					Move the item, we get a new Gomory Cutting Plane
					\begin{equation}
						\sum_{j\in NB} (\bar{a}_{ij} - \lfloor \bar{a}_{ij} \rfloor)x_j \ge \bar{b}_i - \lfloor \bar{b}_i \rfloor  
					\end{equation}
					Add this inequality to the LR, use the dual simplex method to do one pivot, we get a new solution. Use Gomory cutting plane iteratively and we can find the optimal solution for IP.

		\chapter{Packing and Matching}
			\section{Vertices Packing and Matching Formulation}
				Given a graph $G=(V, E)$, with $|V|=n$. A vertices packing solution is that no two neighboring vertices can be chosen at the same time.
				\begin{equation}
					PACK(G) = \{x\in \mathbb{B}^n|x_i + x_j \le 1, \forall (i, j)\in E\} 
				\end{equation}
				\begin{example}
					The following is an example:\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1.8 cm]
							\node (1) [circleNode] {1};
							\node (2) [circleNode, below of=1] {2};
							\node (3) [circleNode, below of=2, xshift=-2 cm] {3};
							\node (4) [circleNode, below of=2, xshift=2 cm] {4};
							\draw (1) -- (2);
							\draw (1) -- (3);
							\draw (1) -- (4);
							\draw (2) -- (4);
							\draw (3) -- (4);
						\end{tikzpicture}
						\caption{Example of vertices packing problem}
					\end{figure}
					The PACK of this graph is\\
					\begin{equation}
						PACK = conv\left(
							\left(\begin{matrix}0 \\ 0 \\ 0 \\ 0\end{matrix}\right),
							\left(\begin{matrix}1 \\ 0 \\ 0 \\ 0\end{matrix}\right),
							\left(\begin{matrix}0 \\ 1 \\ 0 \\ 0\end{matrix}\right),
							\left(\begin{matrix}0 \\ 0 \\ 1 \\ 0\end{matrix}\right),
							\left(\begin{matrix}0 \\ 0 \\ 0 \\ 1\end{matrix}\right),
							\left(\begin{matrix}0 \\ 1 \\ 1 \\ 0\end{matrix}\right)
							\right)
					\end{equation}
				\end{example}

				Given a graph $G=(V, E)$, denote $\delta(i)$ as the set of all the edges introduced to vertice $i\in V$. A matching solution is that no two edges introduced to the same vertice can be chosen at the same time.
				\begin{equation}
					MATCH(G) = \{\sum_{e\in \delta(i)}x_e \le 1|i\in V\}
				\end{equation}

			\section{Dimension of PACK(G)}
				The dimension of PACK, i.e. $dim(PACK(G))$ is (full-dimensional)
				\begin{equation}
					dim(PACK(G)) = |V| 
				\end{equation}
				To prove that $dim(PACK(G)) = |V|$, we need to find $|V| + 1$ affinely independent vectors.\\
				\begin{proof}
					\begin{equation}
						rank\left(\left[\begin{matrix}0 & I_{|V|} \\ 1 & 1\end{matrix}\right]\right) = |V| + 1 
					\end{equation}
					Therefore, in PACK, $rank(A^=,b^=)=0$ 
				\end{proof}

			\section{Clique}
				- A \textbf{clique} is a subset of a graph that in the clique every two vertices linked with each other (complete sub-graph).
				- A \textbf{maximum clique} is a clique that any other vertice can not form a clique with all the points in this clique.

			\section{Inequalities and Facets of conv(VP)}
				\framebox{\textbf{Example:}}\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1.2 cm]
							\node (1) [circleNode] {1};
							\node (2) [circleNode, below of=1, xshift=-2 cm] {2};
							\node (3) [circleNode, below of=1, xshift=2 cm] {3};
							\node (4) [circleNode, below of=2, xshift=2 cm] {4};
							\node (5) [circleNode, below of=4, xshift=-0.9 cm] {5};
							\node (6) [circleNode, below of=4, xshift=0.9 cm] {6};
							\draw (1) -- (2);
							\draw (1) -- (3);
							\draw (1) -- (4);
							\draw (3) -- (4);
							\draw (2) -- (5);
							\draw (5) -- (6);
							\draw (6) -- (3);
						\end{tikzpicture}
						\caption{Example}
					\end{figure}

				\subsection{Type 1 (Nonnegative Constraints)}
					$x_i \ge 0$ induce facets.\\
					\framebox{\textbf{Proof:}}
					\begin{equation}
						rank\left(\left[\begin{matrix}0 & 0 \\ 0 & I_{|V|}\end{matrix}\right]\right) = |V| + 1 
					\end{equation}

				\subsection{Type 2 (Neighborhood Constraints)}
					$x_i + x_j \le 1$ is a valid constraint, but it \textbf{DOES NOT} always induce facet.

				\subsection{Type 3 (Odd Hole)}
					$H$ is an odd hole if it contains circle of $k$ nodes, such that $k$ is odd and there is no cords. e.g. \{1, 2, 5, 6, 3\}. Then, the following inequality is valid,
					\begin{equation}
						\sum_{i\in H}x_i\le \frac{|H|-1}2 
					\end{equation}
					Odd Hole inequality \textbf{DOES NOT} always induce facets.\\
					This inequality can be derived from Gomory cut.

				\subsection{Type 4 (Maximum Clique)}
					$C$ is a maximum clique, then the following inequality is valid and induce a facet,
					\begin{equation}
						\sum_{i\in C} x_i \le 1 
					\end{equation}
					\framebox{\textbf{Proof:}}\\
						First, if $C=V$
						\begin{equation}
							rank\left(\left[I\right]\right) = |C| = |V| 
						\end{equation}
						Second, if $C$ is a subset of $V$, for each vertice in $V \setminus C$, there should be at least one vertice in $C$ that is not linked with it. Therefore for each vertice in $C$ we can find a packing.

			\section{Gomory Cut in Set Covering}
				Consider a graph $G=(V, E)$, the covering problem is
				\begin{equation}
					\sum_{e\in \delta(i)}x_e \le 1, i\in V, x_e\in \{0, 1\}, e\in E
				\end{equation}
				For $T\subset V$, denote $\delta(i)$ as all edges induce to $i\in V$, denote $E(T) \subset E$ as all the edges linked between $(i, j), i\in T, j\in T$, therefore we have
				\begin{equation}
					\sum_{i\in T}\sum_{e\in \delta(i)}x_e \le |T| 
				\end{equation}
				For edges linking $i \in T, j \in T$, count them twice, for edges linking $i\in T, j\notin T$, count them once.We can have a new constraint
				\begin{equation}
					2\sum_{e\in E(T)}x_e + \sum_{e\in \delta(V\setminus T, T)}x_e \le |T| 
				\end{equation}
				Perform the Gomory Cut, the following constraint is a valid:
				\begin{equation}
					\sum_{e\in E(T)}x_e \le \lfloor \frac{|T|}2 \rfloor 
				\end{equation}

		\chapter{Knapsack Problem}
			\section{Knapsack Problem Formulation}
				Consider the knapsack set KNAP
				\begin{equation}conv(KNAP)= conv(\{x\in \mathbb{B}^n|\sum_{j\in N}a_jx_j\le b\})\end{equation}
				in where\\
				- $N = \{1, 2, ..., n\}$\\
				- With out lost of generality, assume that $a_j > 0, \forall j \in N$ and $a_j < b, \forall j \in N$

			\section{Valid Inequalities for a Relaxation}
				For $P=\{x\in \mathbb{B}^n | Ax\le b\}$, each row can be regard as a Knapsack problem, i.e. for row $i$
				\begin{equation}
					P_i = \{x\in \mathbb{B}^n | a_i^T x \le b_i\} 
				\end{equation}
				is a relaxation of $P$, therefore,
				\begin{equation}
					P\subseteq P_i, \forall i=1,2,...,m 
				\end{equation}
				\begin{equation}
					P\subseteq \cap_{i=1}^m P_i 
				\end{equation}
				So any inequality valid for a relaxation of an IP is also valid for IP itself.

			\section{Cover and Extended Cover}
				A set $C\subseteq N$ is a cover if $\sum_{j\in C} a_j > b$, a cover $C$ is minimal cover if
				\begin{equation}
					C\subseteq N | \sum_{j\in C}a_j>b, \sum_{j\in C\setminus k} a_j < b, \forall k \in C 
				\end{equation}
				For a cover $C$, we can have the cover inequality
				\begin{equation}
					\sum_{j\in C}x_j \le |C|-1
				\end{equation}
				The inequality is trivial considering the pigeonhole principle.\\
				$C\subseteq N$ is a minimal cover, then $E(C)$ is defined as following:
				\begin{equation}
					E(C) = C\cup \{j \in N | a_j \ge a_i, \forall i \in C\}
				\end{equation}
				is called an extended cover. Then we have,
				\begin{equation}
					\sum_{i\in E(C)} x_i \le |C| - 1 \text{ dominates } \sum_{i\in C} x_i \le |C| - 1
				\end{equation}
				and
				\begin{equation}
					\sum_{i\in E(C)} x_i \le |C| - 1 \text{ dominates } \sum_{i\in E(C)} x_i \le |E(C)| - 1
				\end{equation}
				Hereby we need to prove that $\sum_{i\in E(C)} x_i \le |C| - 1$ is valid, by contradiction.\\
				\framebox{\textbf{Proof:???}} Suppose $x^R \in KNAP$, $R$ is a feasible solution, Where
				\begin{equation}
					x^R_j = \begin{cases}1, \quad \text{if $j\in R$} \\ 0, \quad \text{otherwise}\end{cases} 
				\end{equation}
				Then
				\begin{equation}
					\sum_{j\in E(C)}x^R_j \ge |C| \Rightarrow |R \cap E(C)| \ge |C|  
				\end{equation}
				therefore
				\begin{equation}
					\sum_{j\in R}a_j \ge \sum_{j\in R \cap E(C)} a_j \ge \sum_{j\in C} a_j > b 
				\end{equation}
				which means $R$ is a cover, it is contradict to $\sum_{j\in E(C)}x^R_j \ge |C|$ so $x^R \notin KNAP$

			\section{Dimension of KNAP}
				$conv(KNAP)$ is full dimension, i.e. $dim(conv(KNAP))=n$.\\
				\framebox{\textbf{Proof:}} $0, e_j, \forall j\in N$ are $n + 1$ affinely independent points in $conv(KNAP)$\\

			\section{Inequalities and Facets of conv(KNAP)}
				\subsection{Type 1 (Lower Bound and Upper Bound Constraints):}
					- $x_k\ge 0$ is a facet of $conv(KNAP)$\\
					\framebox{\textbf{Proof:}} $0, e_j, \forall j\in N\setminus k$ are $n$ affinely independent points that satisfied $x_k=0$\\
					- $x_k\le 1$ is a facet iff $a_j + a_k \le b, \forall j\in N \setminus k$\\
					\framebox{\textbf{Proof:}} $e_k, e_j+e_k, \forall j \in N\setminus k$ are $n$ affinely independent points that satisfied $x_k = 1$

				\subsection{Type 2 (Extended Cover)}
					Order the variables so that $a_1 \ge a_2 \ge \dots \ge a_n$, therefore $a_1 = a_{max}$\\
					Let $C$ be a cover with $\{j_1, j_2, \dots, j_r\}$ where $j_1 < j_2 < \dots < j_r$ so that $a_{j_1} \ge a_{j_2} \ge \dots \ge a_{j_r}$\\
					Let $p = \min\{j | j\in N \setminus E(C)\}$\\
					Then
					\begin{equation}
						\sum_{j\in E(C)} x_j \le |C| - 1 
					\end{equation}
					is a facet of $conv(KNAP)$ if\\
					- $C = N$\\
					\framebox{\textbf{Proof:}}
					\begin{equation}
						R_k = C\setminus k, \forall k \in C = N \setminus k, \forall k \in N 
					\end{equation}
					have $|N|$ affinely independent vectors\\
					- $E(C) = N$ and $\sum_{j\in C \setminus \{j_1, j_2\}} a_j + a_{max} \le b$\\
					\framebox{\textbf{Proof:}} ($j_1, j_2$ are two heaviest elements in $C$)
					\begin{equation}
						S_k = C\setminus \{j_1, j_2\}\cup \{k\}, \forall k\in E(C)\setminus C 
					\end{equation}
					$R_k\cup S_k$ have $|C|+|E(C) \setminus C| = |E(C)| = |N|$ affinely independent vectors\\
					- $C = E(C)$ and $\sum_{j\in C \setminus j_1} a_j + a_p \le b$)\\
					\framebox{\textbf{Proof:}} ($j_1$ is the heaviest element in $C$, $k$ is the lightest element outside extended cover)
					\begin{equation}
						T_k = C \setminus j_i \cup \{k\}, \forall k\in N\setminus E(C) 
					\end{equation}
					$R_k \cup T_k$ have $|N \setminus E(C)| + |E(C)| = |N\setminus C| + |C| = |N|$ affinely independent vectors\\
					- $C \subset E(C) \subset N$ and $\sum_{j\in C \setminus \{j_1, j_2\}} a_j + a_{max} \le b$ and $\sum_{j\in C \setminus j_1} a_j + a_p \le b$\\
					\framebox{\textbf{Proof:}}$S_k \cup T_k$ have $|E(C) \setminus C| + |N \setminus E(C)| = |N|$ affinely independent vectors

			\section{Lifting}
				\subsection{Up Lifting}
					For KNAP problem
					\begin{equation}
						KNAP = \{x\in \mathbb{B}^n | \sum_j a_jx_j \le b\} 
					\end{equation}
					For $P=conv(KNAP)$ denote\\
					\begin{align}
						&P_{k_1, k_2, ..., k_m} \\
						&\quad =conv(KNAP\cap \{x\in \mathbb{B}^{n}|x_{k_1}=x_{k_2}=\dots=x_{k_m}=0\}) 
					\end{align}
					Therefore
					\begin{align}
						&P_{k_1, k_2, ..., k_m} \\
						&\quad=conv(KNAP\cap \{x\in \mathbb{B}^{n}|\sum_{j\in N\setminus \{k_1, k_2, ..., k_m\}} a_j x_j \le b\}) 
					\end{align}
					The $C=N$ cover inequality for $P_{k_1, k_2, ..., k_m}$ implies
					\begin{equation}
						\sum_{j\in N\setminus \{k_1, k_2, ..., k_m\}} x_j \le n-m-1 
					\end{equation}
					is a facet of $P_{k_1, k_2, ..., k_m}$\\
					The lifting process is to find a facet for $P_{k_1, k_2, ..., k_{m-1}}$ using facet of $P_{k_1, k_2, ..., k_m}$, i.e. find $\alpha_{m}$ for the following constraint to be a facet.
					\begin{equation}
						\alpha_m x_m + \sum_{j\in N\setminus \{k_1, k_2, ..., k_m\}} x_j \le n-m-1 
					\end{equation}
					If $x_m=0$, $\alpha_m \ge 0$,\\
					If $x_m=1$, $\alpha_m \le (n-m+1) - \gamma$ where
					\begin{align}
						\gamma &= \max\{\sum_{j\in N\setminus\{k_1, k_2, ..., k_m\}}x_j|x\in P_{k_1, k_2, ..., k_{m-1}}, x_m=1\} \\
						&= \max\{\sum_{j\in N\setminus\{k_1, k_2, ..., k_m\}}x_j|\sum_{j \in N \setminus \{k_1, k_2, ..., k_m\}}a_jx_j\le b-a_m\} 
					\end{align}
					Then let $\alpha_m = n-m+1-\gamma$, we uplifted a constraint. Repeat this procedure for $\{k_1, k_2, ..., k_m\}$ and finally we can find a family of facets for $conv(KNAP)$


				\subsection{Down Lifting}
					Similar to up lifting, we can perform the lifting in a different way.\\
					Denote
					\begin{align}
						&P_{k_1, k_2, ..., k_m}^\prime  \\
						&\quad =conv(KNAP\cap \{x\in \mathbb{B}^{n}|x_{k_1}=x_{k_2}=\dots=x_{k_m}=1\}) 
					\end{align}

			\section{Separation of a Cover Inequality}
				$C\in N$ is a cover if $\sum_{i\in C} a_i > b$, let $C$ be a minimal cover
				\begin{align}
					&\sum_{i\in C}x_i \le |C| - 1 \\
					\Rightarrow \quad & |C| - \sum_{i\in C}x_i = \sum_{i \in C}(1-x_i)\ge 1 \\
				\end{align}
				let $\bar{x}$ be a fractional solution of $\{\sum_{i\in N} a_ix_i \le b, x_i \in [0, 1], i\in N\}$, find a cover $C$ of which $\sum_{i\in C}(1-\bar{x_i})<1$\\
				Decision variable:
				\begin{equation}
					y_i = \begin{cases}1, \quad \text{if } i\in C\\ 0, \quad \text{otherwise}\end{cases}
				\end{equation}
				\begin{align}
					\min \quad & \sum_{i\in N} (1-\bar{x_i})y_i = z \\
					\text{s.t.} \quad & \sum_{i\in N}a_iy_i \ge b+1 \\
					&y_i \in \{0, 1\}, i\in N
				\end{align}
				if $z<1$, then the cover cut associated with $y$ is violation by $\bar{x}$

		\chapter{Network Flow Problem}
			\section{Shortest Path Problem}
				A graph $G=(A, N)$ is a directed graph\\
				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 1.8 cm]
						\node (1) [circleNode, label=above:start] {1};
						\node (2) [circleNode, right of=1, yshift=1 cm] {2};
						\node (3) [circleNode, right of=1, yshift=-1 cm] {3};
						\node (4) [circleNode, right of=2] {4};
						\node (5) [circleNode, right of=3] {5};
						\node (6) [circleNode, label=above:end, right of=4, yshift=-1 cm] {6};
						\draw [arrow] (1) -- (2);
						\draw [arrow] (1) -- (3);
						\draw [arrow] (2) -- (3);
						\draw [arrow] (2) -- (4);
						\draw [arrow] (3) -- (5);
						\draw [arrow] (4) -- (6);
						\draw [arrow] (5) -- (6);
						\draw [arrow] (5) -- (4);
						\draw [arrow] (2) -- (5);
					\end{tikzpicture}
					\caption{Example of directed graph}
				\end{figure}
				Denote:\\
				\begin{equation}
					x_{ij} = \begin{cases}1, &\text{if goes from } i \text{ to } j\\ 0, & \text{otherwise}\end{cases}
				\end{equation}
				The shortest path problem can be formulated as the following:\\
				\begin{align}
					\min &\sum_{(i, j)\in A} c_{ij}x_{ij} \\
					& \sum_{i \in N\setminus(\{S\}\cup\{E\}), (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 0 \\
					& \sum_{i=\{S\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 1 \\
					& \sum_{i=\{E\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = -1 \\
					& x_{ij} \in [0,1], (i,j)\in A 
				\end{align}
				Although initially $x_{ij} \in [0,1]$, in the optimized solution, $x\in \{0, 1\}$.

			\section{Maximum Flow Problem}
				\begin{align}
					\min &\sum_{(i, j)\in A} F \\
					& \sum_{i \in N\setminus(\{S\}\cup\{E\}), (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 0 \\
					& \sum_{i=\{S\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = F \\
					& \sum_{i=\{E\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = -F \\
					& l_{ij} \le x_{ij} \le u_{ij}, (i,j)\in A 
				\end{align}
				In where $F$ means the flow from source to target.

			\section{Minimum Cost Flow}
				The shortest path problem is a special case of Minimum Cost Flow Problem, which can be formulated as the following:\\
				\begin{align}
					\min &\sum_{(i, j)\in A} c_{ij}x_{ij} \\
					& \sum_{i \in N\setminus(\{S\}\cup\{E\}), (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 0 \\
					& \sum_{i=\{S\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 1 \\
					& \sum_{i=\{E\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = -1 \\
					& x_{ij} \in [0,1], (i,j)\in A 
				\end{align}

			\section{Unimodularity}
				\subsection{Unimodular Matrix and Total Unimodular Matrix}
					- A unimodular matrix $M$ is a squared matrix, where $det(M)=1$ or $-1$.\\
					- Total unimodular matrix is a matrix where all its sub-matrices are unimodular matrix.

				\subsection{Importance of Unimodular Matrix}
					Let $M_{m\times m}$ be a unimodular matrix, if $b\in \mathbb{Z}^m$, the solution for $Mx=b$ is always integer.\\
					\framebox{\textbf{Proof:}} By Cramer's Rule
					\begin{equation}
						x_i = \frac{det{M_i}}{det{M}} 
					\end{equation}
					in which $M_i$ is matrix $M$ replace $i$th column with $b$. Therefore $det(M_i)$ is integer. Also, $det(M)\ne 0$, so $det(M)=1$ or $det(M)=-1$. Proved.

				\subsection{Structures of Total Unimodular Matrix}
					\textbf{Structure 1:}
						Matrix $M$ that has only 1, -1, 0 enters and each column has at most 2 non-zeros is a TU matrix if it satisfies the following conditions:\\
						We can split the rows in to tops and bottoms, such that for all columns $j$ having 2 non-zeros\\
						- If the non-zeros have the same sign, then one value should be in top and the other should be in bottom\\
						- If the non-zeros have the different sign, then both of them should be in top or both of them should be in bottom\\
					\textbf{Structure 2:}
						If all the columns in matrix $M$ has only 0 or consecutive 1s (or -1s), matrix $M$ is a TU matrix

				\subsection{Construct a New Unimodular Matrix}
					Let $F$ be a unimodular matrix, then
					\begin{equation}
						\left[\begin{matrix}F \\ I\end{matrix}\right] 
					\end{equation}
					is a unimodular matrix, also
					\begin{equation}
						\left[\begin{matrix}F & 0 \\ I & I\end{matrix}\right] 
					\end{equation}
					is a unimodular matrix.

		\chapter{Duality and Relaxation}
			\section{Relaxation}
				\subsection{Why Rounding Can be Bad - IP Example}
					Rounding can be bad because the optimal of IP can be far away from optimal of LP. For example,
					\begin{align}
						\max \quad & z=x_1 +0.64x_2  \\
						\text{s.t.} \quad & 50x_1 +31x_2 \le 250  \\
									& 3x_1-2x_2\ge -4  \\
									& x_1,x_2\ge 0 \quad \text{(for LP)}\\
									& x_1,x_2 \in Z^+ \quad \text{(for IP)} 
					\end{align}
					\begin{figure}[h!]
						\centering
						\begin{tikzpicture}[scale=0.7]
							\draw [->] (1,1) -- (1, 7);
							\draw [->] (1,1) -- (7, 1);
							\draw (1,3) -- (2.948, 5.922);
							\draw (2.948, 5.922) -- (6, 1);
							\draw [->] (1,1) -- (2, 1.64);
							\node [right] at (2, 1.64) {z};
							\node [above] at (2.984, 5.922) {(1.948, 4.922)};
							\node [above] at (2.984, 6.422) {Optimal for LP};
							\node [below] at (6,1) {(5, 0)};
							\node [below] at (6, 0.5) {Optimal for IP};
						\end{tikzpicture}
						\caption{Optimal solution for LP / IP}
					\end{figure}

				\subsection{Why Rounding Can be Bad - QAP example}
					Rounding can make the LP useless. For example, for QAP problem, the IP model is
					\begin{align}
						\min \quad z &= \sum_{i\in D} \sum_{s\in O} c_is x_is + \sum_{i\in D} \sum_{j \in D} \sum_{s \in O} \sum_{t\in O} w_{ij}^{st}y_{ij}^{st}  \\
						\text{s.t.} \quad & \sum_{i \in D} x_{is} =1, \quad s\in D  \\
									&\sum_{s \in O} x_{is} = 1, \quad i \in D  \\
									&x_{is} \in \{0, 1\}, \quad i \in D, s\in O  \\
									& y_{ij}^{st} \ge x_{is} + x_{jt} - 1, \quad i\in D, j\in D, s\in O, t \in O  \\
									& y_{ij}^{st} \ge 0, \quad i\in D, j\in D, s\in O, t \in O  \\
									& y_{ij}^{st} \le x_{is}, \quad i\in D, j\in D, s\in O, t \in O  \\
									& y_{ij}^{st} \le x_{jt}, \quad i\in D, j\in D, s\in O, t \in O  
					\end{align}
					We can get the optimal solution for LP supposing $\forall i, s \quad x_{is}\in [0, 1]$
					\begin{align}
						x_{is} &= \frac{1}{|D|}, \quad i \in D, s\in O   \\
						y_{ij}^{st} & = 0, \quad i\in D, j\in D, s\in O, t \in O 
					\end{align}

				\subsection{IP and Convex Hull}
					For IP problem
					\begin{align}
						Z_{IP} \quad \max \quad &z = cx  \\
								\text{s.t.} \quad &Ax \le b \\
										&x\in {Z^n} 
					\end{align}
					In feasible region $S = \{x\in Z^n, Ax\le b\}$ , the optimal solution $Z_{IP} = \max\{cx: x\in S\}$.\\
					Denote $conv(S)$ as the convex hull of $S$ then\\
					\begin{equation}Z_{IP}(S) = Z_{IP}(conv(S))  \end{equation}

				\subsection{Local Optimal and Global Optimal}
					Let 
					\begin{align}
						Z_s &= \min \{f(x):x\in S\} \\
						Z_t &= \min \{f(x):x\in T\}  \\
						& S \subset T 
					\end{align}
					then\\
					\begin{equation}Z_t \le Z_s  \end{equation}
					\textbf{Notice} that if $x_T^* \in S$ then $x_S^*=x_T^*$, to generalized it, \\
					We have
					\begin{align}
						\begin{cases}x_T^* \in \text{arg min} \{f(x): x\in T\} \\ x_T^* \in S\end{cases} \\ \Rightarrow x_T^*\in \text{arg min} \{f(x): x\in S\} 
					\end{align}
					Especially for IP, we can take the LP relaxation as $T$ and the original feasible region of IP as $S$, therefore, if we find an optimal solution from LP relaxation $T$ which is also a feasible solution of $S$, then it is the optimal solution for IP ($S$)

				\subsection{LP Relaxation}
					To perform the LP relaxation, we expand the feasible region
					\begin{align}
						x \in \{0,1\} & \rightarrow x\in [0, 1]  \\
						y\in Z^+ & \rightarrow y \ge 0 
					\end{align}
					If we have $Z_LP(s) = conv(s)$ then
					\begin{equation} LP(s): {x\in R_+^n: Ax\le b}\end{equation}
					The closer $LP(s)$ is to $conv(s)$ the better. Interestingly, we only need to know the convex in the direction of $c$.\\
					There are several formulation problem have the property of $Z_{LP}(s) = conv(s)$, such as:\\
					\indent- Assignment Problem\\
					\indent- Spawning Tree Problem\\
					\indent- Max Flow Problem\\
					\indent- Matching Problem

		\chapter{Lagrangian Relaxation}

\end{document}