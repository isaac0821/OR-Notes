\include{templete}
\begin{document}
\part{Statistics and Simulation}
	\chapter{Random Number Generator}

	\chapter{Monte Carlo Integration}

	\chapter{DES ABS and SD}
		\section{Discrete Event Simulation}

		\section{Agent-based Simulation}

		\section{System Dynamics}

		\section{Comparison}

	\chapter{Bayesian Modeling}
		\section{Prior and Posterior Distribution}
			First, recall Bayes's Theorem
			\begin{definition}[Bayes's Theorem]
				Let $A$ be any event, then for any $1 \le k \le K$ we have
				\begin{equation}
					P(B_k | A) = \frac{P(A|B_k)P(B_k)}{P(A)} = \frac{P(A|B_k)P(B_k)}{\sum_{j = 1}^K P(A|B_j)P(B_j)}
				\end{equation}
			\end{definition}

	\chapter{Markov Chain Monte Carlo}
		\section{Metropolis-Hasting Sampling}
			The goal for Metropolis-Hasting Sampling is to draw samples from some distribution $p(\theta)$. $p(\theta)$ can be regarded as a posterior distribution. 

		\section{Metropolis Sampling}

		\section{Gibbs Sampling}

	\chapter{Markov Decision Process}
		\section{Introduction}
			\begin{definition}[Markov Decision Process]
				A (finite) Markov Decision Process is a 5-tuple $<\mathbb{S}, \mathbb{A}, \mathbb{T}, r, \gamma>$. In which
				\begin{itemize}
					\item $\mathbb{S}$ is a finite set of states
					\item $\mathbb{A}$ is a finite set of actions
					\item $\mathbb{T}$ is a state transition probability function
					\begin{equation}
						T(s^\prime|s, a) = \mathbb{P}(S_{t+1} = s^\prime|S_t = s, A_t = a)
					\end{equation}
					\item $r$ is a reward function
					\begin{equation}
						r(s, a) = \mathbb{E}(R_{t+1}|S_t = s, A_t = a)
					\end{equation}
					\item $\gamma$ is a discount factor $\gamma \in [0, 1]$
				\end{itemize}
			\end{definition}

			\begin{definition}[Return]
				The \textbf{return} $G_t$ is the total discounted reward from time-step $t$
				\begin{equation}
					G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k = 0}^\infty \gamma^k R_{t+k+1}
				\end{equation}
			\end{definition}

			\notice{The objective in Reinforce Learning is to maximize $G_\infty$, that is, to choose $A_t$ to maximize $R_{t+1}, R_{t+2}, \cdots$}

			\begin{definition}[Policy]
				A \textbf{policy} $\pi$ is a distribution over actions given states.
				\begin{equation}
					\pi(a|s) = \mathbb{P}(A_t = a|S_t = s)
				\end{equation}
				A policy fully defines the behavior of an agent.
			\end{definition}

	\chapter{Gaussian Process}
		

\end{document}