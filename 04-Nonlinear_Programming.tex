\part{Nonlinear Programming}
	\chapter{Convex Analysis}
		\section{Convex Sets}
			\begin{definition}
				A set $S\in \mathbb{R}^n$ is said to be convex if $\forall x_1, x_2 \in S, \lambda \in (0, 1) \Rightarrow \lambda x_1 + (1-\lambda)x_2 \in S$
			\end{definition}

			The following are some familes of convex sets.

			\begin{example}
				Empty set is by convention considered as convex.
			\end{example}

			\begin{example}
				Polyhedrons are convex sets.
			\end{example}

			\begin{example}
				Let $P = \{\mathbf{x}\in \mathbb{R}^n | \mathbf{x}^\top \mathbf{Ax} \le \mathbf{b}\}$ where $\mathbf{A} \in \mathbb{S}_+^{n\times n}$ and $\mathbf{b} \in \mathbb{R}_+$. The set $P$ is a convex subset of $\mathbb{R}^n$.
			\end{example}

			\begin{example}
				Let $\|.\|$ be any norm in $\mathbb{R}^n$. Then, the unit ball $B = \{\mathbf{x} \in \mathbb{R}^n | \|\mathbf{x}\| \le b, b > 0\}$ is convex.
			\end{example}

			Let	$S_1, S_2$ be convex set, then:
			\begin{itemize}
				\item $S_1 \cap S_2$ is convex set
				\item $S_1 \oplus S_2$ (Minkowski addition) is convex set, where
					\begin{equation}
						S_1 \oplus S_2 = \{\mathbf{x} \in \mathbb{R}^n| \mathbf{x} = \mathbf{x_1} + \mathbf{x_2}, \mathbf{x_1} \in S_1, \mathbf{x_2} \in S_2\}
					\end{equation}
				\item $S_1 \ominus S_2$ is convex set, where
					\begin{equation}
						S_1 \oplus S_2 = \{\mathbf{x} \in \mathbb{R}^n| \mathbf{x} = \mathbf{x_1} - \mathbf{x_2}, \mathbf{x_1} \in S_1, \mathbf{x_2} \in S_2\}
					\end{equation}
				\item $f(S_1)$ is convex iff $f(\mathbf{x}) = \mathbf{Ax} + \mathbf{b}, \mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{b} \in \mathbb{R}^m$
			\end{itemize}

			\begin{theorem}[Carath\'{e}odory's Theorem]
				Let $S \subseteq \mathbb{R}^n$. Then $\forall \mathbf{x} \in conv(S)$, there exists $\mathbf{x}^1, \mathbf{x}^2, ..., \mathbf{x}^p \in S$, where $p\le n+1$ such that $\mathbf{x} \in conv\{\mathbf{x}^1, \mathbf{x}^2, ... \mathbf{x}^p\}$.
			\end{theorem}

			\notice{This theorem means, any point $\mathbf{x} \in \mathbb{R}^n$ in a convex hull of $S$, i.e., $conv(S)$, can be included in a convex subset $S^\prime \subseteq conv(S)$ that has $n+1$ extreme points.}

			\begin{theorem}
				Let $S$ be a convex set with nonempty interior. Let $\mathbf{x}_1 \in cl(S)$ and $\mathbf{x}_2 \in int(S)$, then $\mathbf{y} = \lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2 \in int(S), \forall \lambda \in (0, 1)$
			\end{theorem}

		\section{Convex Functions}
			\begin{definition}
				Let $C \in \mathbb{R}^n$ be a convex set. A function $f: C \rightarrow \mathbf{R}$ is (resp. strictly) convex if
				\begin{align}
					f(\lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2) &\le \lambda f(\mathbf{x}_1) + (1 - \lambda) f(\mathbf{x}_2) \\ \forall \mathbf{x}_1, \mathbf{x}_2 &\in C, \forall \lambda \in (0, 1)
				\end{align}
				(resp.)
				\begin{align}
					f(\lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2) &< \lambda f(\mathbf{x}_1) + (1 - \lambda) f(\mathbf{x}_2) \\ \forall \mathbf{x_1} \neq \mathbf{x}_2 &\in C, \forall \lambda \in (0, 1)
				\end{align}
			\end{definition}

			\notice{When calling a function convex, we imply that its domain is convex.}

			\begin{example}
				Given any norm $\|.\|$ on $\mathbb{R}^n$, the function $f(x) = \|x\|$ is convex over $\mathbb{R}^n$.
			\end{example}

			\begin{definition}
				Let $S$ be a nonempty convex subset of $\mathbb{R}^n$, $f: S \rightarrow \mathbb{R}$ is (resp. strictly) \textbf{concave} if $-f(x)$ is (resp. strictly) convex.
			\end{definition}

			\notice{A function may be neither convex nor concave.}

			\begin{theorem}
				Consider $f: \mathbb{R}^n\rightarrow \mathbb{R}$. $\forall \bar{\mathbf{x}} \in \mathbb{R}^n$ and a nonzero direction $\mathbf{d} \in \mathbb{R}^n$. Define $F_{\bar{\mathbf{x}}, d}(\lambda) = f(\bar{\mathbf{x}} + \lambda \mathbf{d}$. Then $f$ is (resp. strictly) convex iff $F_{\bar{\mathbf{x}}, d}(\lambda)$ is (resp. strictly) convex for all $\bar{\mathbf{x}} \in \mathbb{R}^n, \forall \mathbf{d} \in \mathbb{R}^n \setminus \{0\}$.
			\end{theorem}

			\begin{definition}[Level-set]
				Given a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ and a scalar $\alpha \in \mathbb{R}$, we refer to the set $S_\alpha = \{\mathbf{x} \in S|f(\mathbf{x}) \le \alpha\} \subseteq \mathbb{R}^n$ as the \textbf{$\alpha$-level-set} of $f$.
			\end{definition}

			\begin{lemma}
				Let $S$ be a nonempty convex set in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}^n$ be a convex function, then the \textbf{$\alpha$-level-set} of $f$ is a convex set for each value of $\alpha \in \mathbb{R}$.
			\end{lemma}

			\notice{The converse is not necessarily true.}

			\begin{definition}[Epigraphs, Hypographs]
				Let $S \in \mathbb{R}^n$ be such that $S \neq \emptyset$. The \textbf{epigraph} of $f$, denoted by $epi(f)$ is
				\begin{equation}
					epi(f) = \{(\mathbf{x}, y) \in S|\mathbf{x} \in S, y \in \mathbb{R}, y \ge f(x)\} \in \mathbb{R}^{n+1}
				\end{equation}
				The \textbf{hypograph} of $f$, denoted by $hypo(f)$ is
				\begin{equation}
					hypo(f) = \{(\mathbf{x}, y) \in S|\mathbf{x} \in S, y \in \mathbb{R}, y \le f(x)\} \in \mathbb{R}^{n+1}
				\end{equation}
			\end{definition}

			\begin{theorem}
				Let $S$ be a nonempty convex subset in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$. Then $f$ is convex iff $epi(f)$ is convex.
			\end{theorem}

			\begin{theorem}
				Let $S$ be a nonempty convex subset in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be a convex function on $S$. Then $f$ is continuous in $int(S)$.
			\end{theorem}

		\section{Subgradients and Subdifferentials}
			\begin{definition}[Subgradient]
				Let $S$ be a nonempty convex set in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be a convex function, then $\xi$ is a \textbf{subgradient} of $f$ at $\bar{\mathbf{x}}$ if
				\begin{equation}
					f(\mathbf{x}) \ge f(\bar{\mathbf{x}}) + \xi^\top(\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x} \in S
				\end{equation}
			\end{definition}

			\begin{definition}[Subdifferential]
				The set of all subgradients of $f$ at $\bar{\mathbf{x}}$ is called \textbf{subdifferential} of $f$ at $\bar{\mathbf{x}}$, denoted as $\partial f(\bar{\mathbf{x}})$
			\end{definition}

			\begin{theorem}
				Let $S$ be a nonempty convex set in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be a convex function. Then for $\bar{\mathbf{x}} \in int(S)$, there exists a vector $\xi$ such that
				\begin{equation}
					f(\mathbf{x}) \ge f(\bar{\mathbf{x}}) + \xi^\top(\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x} \in S
				\end{equation}
				In particular, the hyperplane
				\begin{equation}
					\mathcal{H} = \{(\mathbf{x}, y)|y = f(\bar{\mathbf{x}}) + \xi^\top(\mathbf{x} - \bar{\mathbf{x}})\}
				\end{equation}
				is a supporting plane of $epi(f)$ at $(\bar{\mathbf{x}}, f(\bar{\mathbf{x}}))$
			\end{theorem}

			\begin{theorem}
				Let $S$ be a nonempty convex set in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be a convex function. Suppose that for each $\bar{\mathbf{x}} \in S$, there exists $\xi$ such that
				\begin{equation}
				 	f(\mathbf{x}) \ge f(\bar{\mathbf{x}}) + \xi^\top(\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x} \in S
				\end{equation} 
				Then $f$ is convex on $int(S)$
			\end{theorem}

			\notice{Not all convex functions are continuous, it has to be continuous in its interior, but it may not be continuous at the boundary.}

		\section{Differentiable Functions}
			\begin{definition}[Differentiable Functions]
			 	Let $S$ be a nonempty subset of $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$.Then $f$ is said to be \textbf{differentiable} at $\bar{\mathbf{x}} \in int(S)$ if there exists a vector $\nabla f(\bar{\mathbf{x}})$ and a function $\alpha: \mathbb{R}^n \rightarrow \mathbb{R}$ such that
			 	\begin{equation}
			 		f(\mathbf{x}) = f(\bar{\mathbf{x}}) + \nabla f(\bar{\mathbf{x}})^\top (\mathbf{x} - \bar{\mathbf{x}}) + \alpha(\bar{\mathbf{x}}, \mathbf{x} - \bar{\mathbf{x}})\|\mathbf{x} - \bar{\mathbf{x}}\|
			 	\end{equation}
			 	for all $\mathbf{x} \in S$ where $\lim_{\mathbf{x} - \bar{\mathbf{x}}}\alpha (\bar{\mathbf{x}}, \mathbf{x} - \bar{\mathbf{x}}) = 0$
			\end{definition}

			\begin{remark}
			 	If function $f$ is differentiable, then $\nabla f(\bar{\mathbf{x}}) = (\frac{\partial f(\bar{\mathbf{x}})}{\partial \mathbf{x}_1}, \frac{\partial f(\bar{\mathbf{x}})}{\partial \mathbf{x}_2}, \cdots, \frac{\partial f(\bar{\mathbf{x}})}{\partial \mathbf{x}_n})$, and the gradient is unique.
			\end{remark}

			\begin{lemma}
				Let $S\neq \emptyset$ be a convex set of $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be convex. If $f$ is differentiable at $\bar{\mathbf{x}} \in int(S)$, then the subdifferential of $f$ at $\bar{\mathbf{x}}$ is the singleton, $\{\nabla f(\bar{\mathbf{x}})\}$
			\end{lemma}

			\begin{theorem}
				Let $S$ be a nonempty subset of $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be differentiable on $S$. Then $f$ is (resp. strictly) convex on $S$ iff $\forall \bar{\mathbf{x}} \in S$
				\begin{equation}
					f(\mathbf{x}) \ge f(\bar{\mathbf{x}}) + \nabla f(\bar{\mathbf{x}}) (\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x} \in S
				\end{equation}
				(resp.)
				\begin{equation}
					f(\mathbf{x}) > f(\bar{\mathbf{x}}) + \nabla f(\bar{\mathbf{x}}) (\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x}\neq \bar{\mathbf{x}} \in S
				\end{equation}
			\end{theorem}

			\begin{theorem}[Mean-value Theorem]
				Let $S$ be a nonempty subset of $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be differentiable on $S$. Then for all $\mathbf{x}_1, \mathbf{x}_2 \in S$, there exists $\lambda \in (0, 1)$ such that
				\begin{equation}
					f(\mathbf{x}_2) = f(\mathbf{x}_2) + \nabla f(\hat{\mathbf{x}})(\mathbf{x}_2 - \mathbf{x}_1)
				\end{equation}
				where
				\begin{equation}
					\hat{\mathbf{x}} = \lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2
				\end{equation}
			\end{theorem}

	\chapter{Optimality Conditions and Duality}
		\section{The Fritz John Optimality Conditions}

		\section{The Karush-Kuhn-Tucker Optimality Conditions}

		\section{Constraint Qualification}

		\section{Lagrangian Duality and Saddle Point Optimality Condition}

	\chapter{Unconstrained Optimization}

	\chapter{Quadratic Programming}

	\chapter{Penalty and Barrier Functions}