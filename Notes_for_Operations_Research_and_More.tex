\include{templete}
\begin{document}
	\maketitle
	\today

	\clearpage
	\thispagestyle{plain}
	\par\vspace*{.35\textheight}{\centering \textit{To My Beloved Motherland China}\par}

	\tableofcontents

	\part{Preliminary Topics}\label{PT}
		\chapter{Introduction to Optimization}
			\section{Optimization Model}
				The following is the basic forms of terminology:
				\begin{align}
					\text{(P)} \quad \min \quad & f(x)  \\
								\text{s.t.} \quad & g_i(x)\le 0, \quad i=1,2,...,m \\
											& h_j(x)=0, \quad j=1,2,...,l \\
											& x \in X 
				\end{align}
				We have
				\begin{itemize}
					\item - $x\in R^n \rightarrow X \subseteq R^m$
					\item $g_i(x)$ are called inequality constraints
					\item $h_j(x)$ are called equality constraints
					\item $X$ is the domain of the variables (e.g. cone, polygon, $\{0, 1\}^n$, etc.)
					\item Let $F$ be the feasible region of $(P)$:
					\begin{itemize}
						\item $x^0$ is a feasible solution iff $x^0\in F$
						\item $x^*$ is an optimized solution iff $x^* \in F$ and $f(x^*)\le f(x^0), \forall x^0 \in F$ (for minimized problem)
					\end{itemize}
				\end{itemize}

				\notice{Not every $(P)$ has a feasible region, we can have $F = \emptyset$. Even if $F \neq \emptyset$, there might not be an solution to $P$, e.g. unbounded. If $(P)$ has optimized solution(s), it could be 1) Unique, or 2) Infinite number of solution, or 3) Finite number of solution}

				Types of Optimization Problem
				\begin{itemize}
					\item $m = l = 0$, $x \in R^n$, unconstrained problem
					\item $m + l > 0$, constrained problem
					\item $f(x), g_i(x), h_j(x)$ are linear, Linear Optimization
					\begin{itemize}
						\item If $X=R^n$, Linear Programming
						\item If $X$ is discrete,  Discrete Optimization
						\item If $X \subseteq Z^n$, Integer Programming
						\item If $X\in \{0,1\}^n$, Binary Programming
						\item If $X\in Z^n \times R^m$, Mixed Integer Programming
					\end{itemize}
				\end{itemize}

			\section{Linear Programming Formulation Skills}
				\subsection{Absolute Value}
					Consider the following model statement:
					\begin{align}
						\min \quad & \sum_{j\in J}c_j|x_j|, \quad c_j > 0 \\
						\text{s.t.} \quad & \sum_{j\in J}a_{ij}x_j \gtreqless b_i, \quad \forall i\in I \\
						                  & x_j \quad \text{unrestricted}, \quad \forall j\in J 
					\end{align}
					Modeling:
					\begin{align}
						\min \quad & \sum_{j\in J}c_j(x_j^+ + x_j^-), \quad c_j > 0 \\
						\text{s.t.} \quad & \sum_{j\in J}a_{ij}(x_j^+ - x_j^-) \gtreqless b_i, \quad \forall i\in I \\
						                  & x_j^+, x_j^- \ge 0, \quad \forall j\in J 
					\end{align}

				\subsection{A Minimax Objective}
					Consider the following model statement:
					\begin{align}
						\min \quad & \max_{k\in K}\sum_{j\in J}c_{kj}x_j \\
						\text{s.t.} \quad & \sum_{j\in J}a_{ij}x_j \gtreqless b_i, \quad \forall i\in I \\
						                  & x_j \ge 0, \quad \forall j\in J 
					\end{align}
					Modeling:
					\begin{align}
						\min \quad & z \\
						\text{s.t.} \quad & \sum_{j\in J}a_{ij}x_j \gtreqless b_i, \quad \forall i\in I \\
										  & \sum_{j\in J}c_{kj}x_j \le z, \quad \forall k\in K \\
						                  & x_j \ge 0, \quad \forall j\in J 
					\end{align}
				\subsection{A Fractional Objective}
					Consider the following model statement:
					\begin{align}
						\min \quad & \frac{\sum_{j\in J}c_{j}x_j + \alpha}{\sum_{j\in J}d_{j}x_j + \beta} \\
						\text{s.t.} \quad & \sum_{j\in J}a_{ij}x_j \gtreqless b_i, \quad \forall i\in I \\
						                  & x_j \ge 0, \quad \forall j\in J 
					\end{align}
					Modeling:
					\begin{align}
						\min \quad & \sum_{j\in J}c_{j}x_jt + \alpha t \\
						\text{s.t.} \quad & \sum_{j\in J}a_{ij}x_j \gtreqless b_i, \quad \forall i\in J \\
										  & \sum_{j\in J}d_jx_jt + \beta t = 1\\
										  & t > 0 \\
						                  & x_j \ge 0, \quad \forall j\in J \\
						                  & (t = \frac1{\sum_{j\in J}d_jx_j + \beta}) 
					\end{align}

					For the following statement:
					\begin{align}
						\min \quad & z^P = \frac{\mathbf{c^\top x} + d}{\mathbf{e^\top x} + f}\\
						\text{s.t.} \quad & \mathbf{Gx} \le \mathbf{h}\\
						                  & \mathbf{Ax} = \mathbf{b}
					\end{align}

					Modeling
					\begin{align}
						\min \quad & z^R = \mathbf{c^\top y} + dz\\
						\text{s.t.} \quad & \mathbf{Gy} - \mathbf{h}z \le 0\\
										  & \mathbf{Ay} - \mathbf{b}z = 0\\
										  & \mathbf{e^\top y} + fz = 1\\
										  & z \ge 0
					\end{align}

				\subsection{A Range Constraint}
					Consider the following model statement:
					\begin{align}
						\min \quad & \sum_{j\in J}c_jx_j \\
						\text{s.t.} \quad & d_i\le \sum_{j\in J}a_{ij}x_j \le e_i, \quad \forall i\in I \\
						                  & x_j \ge 0, \quad \forall j\in J 
					\end{align}
					Modeling:
					\begin{align}
						\min \quad & \sum_{j\in J}c_jx_j, \quad c_j > 0 \\
						\text{s.t.} \quad & u_i + \sum_{j\in J}a_{ij}x_j = e_i, \quad \forall i\in I \\
						                  & x_j \ge 0, \quad \forall j\in J \\
						                  & 0\le u_i \le e_i-d_i, \quad \forall i\in I 
					\end{align}

			\section{Integer Programming Formulation Skills}
				\subsection{A Variable Taking Discontinuous Values}
					In algebraic notation: 
					\begin{equation}
						x = 0,\quad \text{or} \quad l\le x \le u 
					\end{equation}
					Modeling:
					\begin{align}
						& x \le uy \\
						& x \ge ly  \\
						& y \in \{0, 1\} 
					\end{align}
					where
					\begin{equation}y=\begin{cases}0, & \text{if }x=0 \\ 1, & \text{if } l\le x \le u\end{cases} \end{equation}

				\subsection{Fixed Costs}
					In algebraic notation: 
					\begin{equation}
						C(x) = \begin{cases} 0 & \text{for } x=0 \\ k + cx & \text{for } x > 0 \end{cases} 
					\end{equation}
					Modeling:
					\begin{align}
						& C^*(x, y) = ky+cx\\
						& x \le My  \\
						& x \ge 0 \\
						& y \in \{0, 1\} 
					\end{align}
					where
					\begin{equation}y=\begin{cases}0, & \text{if }x=0 \\ 1, & \text{if }x\ge 0\end{cases} \end{equation}

				\subsection{Either-or Constraints}
					In algebraic notation: 
					\begin{equation}
						\sum_{j\in J} a_{1j} x_j \le b_1 \text{ or } \sum_{j\in J} a_{2j} x_j \le b_2 
					\end{equation}
					Modeling:
					\begin{align}
						& \sum_{j\in J} a_{1j} x_j \le b_1 + M_1y  \\
						& \sum_{j\in J} a_{2j} x_j \le b_2 + M_1(1-y)  \\
						& y \in \{0, 1\} 
					\end{align}
					where
					\begin{equation}y=\begin{cases}0, & \text{if }\sum_{j\in J} a_{1j} x_j \le b_1 \\ 1, & \text{if } \sum_{j\in J} a_{2j} x_j \le b_2\end{cases} \end{equation}
					Notice that the sign before $M$ is determined by the inequality $\ge$ or $\le$, if it is \lq\lq{}$\ge$\rq\rq{}, use \lq\lq{}$-$\rq\rq{}, if it \lq\lq{}$\le$\rq\rq{}, use \lq\lq{}+\rq\rq{}.

				\subsection{Conditional Constraints}
					If constraint A is satisfied, then constraint B must also be satisfied
					\begin{equation}
						\text{If} \quad \sum_{j\in J} a_{1j} x_j \le b_1 \text{ then } \sum_{j\in J} a_{2j} x_j \le b_2 
					\end{equation}
					The key part is to find the opposite of the first condition. We are using $A\Rightarrow B \Leftrightarrow \neg B \Rightarrow \neg A$\\
					Therefore it is equivalent to
					\begin{equation}
						\sum_{j\in J} a_{1j} x_j > b_1 \text{ or } \sum_{j\in J} a_{2j} x_j \le b_2 
					\end{equation}
					Furthermore, it is equivalent to
					\begin{equation}
						\sum_{j\in J} a_{1j} x_j \ge b_1 + \epsilon \text{ or } \sum_{j\in J} a_{2j} x_j \le b_2 
					\end{equation}
					Where $\epsilon$ is a very small positive number.\\
					Modeling:
					\begin{align}
						& \sum_{j\in J} a_{1j} x_j \ge b_1 + \epsilon -  M_2y  \\
						& \sum_{j\in J} a_{2j} x_j \le b_2 + M_2(1-y)  \\
						& y \in \{0, 1\} 
					\end{align}

				\subsection{Special Ordered Sets}
					Out of a set of yes-no decisions, at most one decision variable can be yes. Also known as SOS1.
					\begin{align}
						x_1=1,x_2=x_3&=\dots=x_n=0  \\
						&\text{or}  \\
						x_2=1, x_1=x_3&=\dots=x_n=0  \\
						&\text{or ...} 
					\end{align} 
					Modeling:
					\begin{equation} \sum_{i} x_i = 1, \quad i \in N \end{equation}
					Out of a set of binary variables, at most two variables can be nonzero. In addition, the two variables must be adjacent to each other in a fixed order list. Also known as SOS2.
					Modeling:
					If $x_1, x_2, ... , x_n$ is a SOS2, then
					\begin{align}
						& \sum_{i=1}^{n} x_i \le 2  \\
						& x_i + x_j \le 1, \forall i \in \{1, 2,..., n\}, j \in \{i+2, i+3, ..., n\}  \\
						&x_i \in \{0, 1\}
					\end{align}
					There is another type of definition, that is out of a set of nonnegative variables \textbf{not binary here}, at most two variables can be nonzero. In addition, the two variables must be adjacent to each other in a fixed order list. All variables summing to 1.\\
					This definition of SOS2 is used in Piecewise Linear Formulations.

				\subsection{Piecewise Linear Formulations}
					The objective function is a sequence of line segments, e.g. $y=f(x), $ consists $k-1$ linear segments going through $k$ given points $(x_1, y_1), (x_2, y_2), ... ,(x_k, y_k)$.\\
					Denote 
					\begin{equation}d_i=\begin{cases}1, & x\in (x_i, x_{i+1})\\0, & \text{otherwise} \end{cases}\end{equation}
					Then the objective function is
					\begin{equation}\sum_{i \in \{1, 2, ..., k-1\}} y = d_if_i(x) \end{equation} 
					Modeling: Given that objective function as a piecewise linear formulation, we can have these constraints\\
					\begin{align}
						&\sum_{i \in \{1, 2, ..., k-1\}} d_i =1  \\
						&d_i \in \{0, 1\}, i \in \{1, 2, ..., k-1\}  \\
						& x = \sum_{i \in \{1, 2, ..., k\}} w_i x_i  \\
						& y = \sum_{i \in \{1, 2, ..., k\}} w_i y_i  \\
						& w_1 \le d_1  \\
						& w_i \le d_{i-1} + d{i}, i \in \{2, 3, ..., k-1\}  \\
						& w_k \le d_{k-1} 
					\end{align}
					In this case, $ w_i \in SOS2$ (second definition)

				\subsection{Conditional Binary Variables}
					Choose at most $n$ binary variable to be 1 out of  $x_1, x_2, ... x_m, m\ge n$. If $n=1$ then it is SOS1.\\
					Modeling:
					\begin{equation}
						\sum_{k\in \{1,2,...,m\}} x_k \le n
					\end{equation}
					Choose exactly $n$ binary variable to be 1 out of  $x_1, x_2, ... x_m, m\ge n$\\
					Modeling:
					\begin{equation}
						\sum_{k\in \{1,2,...,m\}} x_k = n
					\end{equation}
					Choose $x_j$ only if $x_k = 1$\\
					Modeling:
					\begin{equation}x_j = x_k  \end{equation}
					\lq\lq{}and\rq\rq{} condition, iff $x_1, x_2, ... , x_m =1$ then $y=1$\\
					Modeling:
					\begin{align}
						& y \le x_i, i\in \{1, 2, ..., m\}  \\
						& y \ge \sum_{i \in \{1, 2, ..., m\}} x_i - (m - 1) 
					\end{align}

				\subsection{Elimination of Products of Variables}
					For variables $x_1$ and $x_2$,
					\begin{equation}y = x_1 x_2\end{equation}
					Modeling: If $x_1, x_2$ are binary, it is the same as \lq\lq{}and\rq\rq{} condition of binary variables.\\
					If $x_1$ is binary, while $x_2$ is continuous and $0 \le x_2 \le u$, then
					\begin{align}
						y &\le ux_1  \\
						y &\le x_2  \\
						y &\ge x_2 - u(1- x_1)  \\
						y &\ge 0 
					\end{align}
					If both $x_1$ and $x_2$ are continuous, it is non-linear, we can use Piecewise linear formulation to simulate.

		\chapter{Basic Concepts of Linear Algebra}
			\section{Vector Spaces}
				\subsection{Field}
					\begin{definition}[Field]
						Let $F$ denote either the set  of real numbers or the set of complex numbers.
						\begin{itemize}
							\item Addition is commutative: $x + y = y + x, \forall x, y \in F$
							\item Addition is associative: $x + (y + z) = (x + y) + z, \forall x, y, z \in F$
							\item Element 0 exists and unique: $\exists 0, x + 0 = x, \forall x \in F$
							\item To each $x \in F$ there corresponds a unique element $(-x) \in F$ such that $x + (-x) = 0$
							\item Multiplication is commutative: $xy = yx, \forall x, y \in F$
							\item Multiplication is associative: $x(yz) = (xy)z, \forall x, y, z \in F$
							\item Element 1 exists and unique: $\exists 1, x1=x, \forall x \in F$
							\item To each $x\neq 0 \in F$ there corresponds a unique element $x^{-1} \in F$ that $xx^{-1} = 1$
							\item Multiplication distributes over addition: $x(y + z) = xy + xz, \forall x, y, z \in F$
						\end{itemize}
						Suppose one has a set $F$ of objects $x, y, z, \cdots$ and two operations on the elements of $F$ as following:
						\begin{itemize}
						 	\item (Addition) associates with each pair of elements $x, y \in F$ an element $(x + y)\in F$,
						 	\item (Multiplication) associates with each pair $x, y$ an element $xy \in F$,
						\end{itemize}
						and these two operations satisfy all conditions above. The set $F$, together with these two operations, is then called a \textbf{field}.
					\end{definition}

					\begin{definition}[Subfield]
						A \textbf{subfield} of the field $C$ is a set $F$ of complex numbers which itself is a field.
					\end{definition}

					\begin{example}
						The set of rational numbers is a field.
					\end{example}

					\begin{example}
						The set of integers is \textbf{not} a field.
					\end{example}

					\begin{example}
						The set of all complex numbers of the form $x + y\sqrt{2}$ where $x$ and $y$ are rational, is a subfield of $\mathbb{C}$.
					\end{example}

				\subsection{Vector Space and Subspace}
					\begin{definition}[Vector space]
						A \textbf{vector space} consists of the following:
						\begin{itemize}
							\item A field $F$ of scalars;
							\item A set $V$ of vectors;
							\item An addition operation, which associated with each pair of vectors $\alpha, \beta \in V$ a vector $\alpha + \beta$ in $V$ called the \textbf{sum} of $\alpha$ and $\beta$, in such a way that
							\begin{itemize}
								\item $\alpha + \beta = \beta + \alpha$;
								\item $\alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma$;
								\item $\forall \alpha \in V, \alpha + 0 = \alpha$
								\item $\forall \alpha \in V, \alpha + (-\alpha) = 0$
							\end{itemize}
							\item A multiplication operation, which associated with each scalar $c \in F$ and a vector $\alpha \in V$ a vector $c\alpha \in V$ called the \textbf{product} of $c$ and $\alpha$, in such a way that
							\begin{itemize}
								\item $1\alpha = \alpha, \forall \alpha \in V$
								\item $(c_1c_2)\alpha = c_1(c_2\alpha)$
								\item $c(\alpha + \beta) = c\alpha + c\beta$
								\item $(c_1 + c_2) \alpha = c_1\alpha + c_2\alpha$
							\end{itemize}
						\end{itemize}
						we may simply denote the vector space as $V$, which is the same notation as the set of vectors, if the field $F$ of scalars needs to be specified, we shall say $V$ is a vector space over the field $F$.
					\end{definition}

					\begin{definition}[Subspace]
						Let $V$ be a vector space over the field $F$. A \textbf{subspace} of $V$ is a subset $H$ of $V$ which is itself a vector space over $F$ with the operations of vector addition and scalar multiplication on $H$
					\end{definition}

					The definition of subspace implies $\lambda x \in H, \forall \lambda \in F$ and if $x, y \in V, x + y \in H$.

					The following is equivalent:
					\begin{itemize}
						\item $H\subseteq \mathbb{R}^n$ is a subspace
						\item There is an $m \times n$ matrix $A$ such that $H = \{x \in \mathbb{R}^n | Ax = 0\}$
						\item There is a $k \times n$ matrix $B$ such that $H = \{x \in \mathbb{R}^n | x = uB, u \in \mathbb{R}^k\}$
					\end{itemize}

					Particularly, 
					\begin{definition}[Orthogonal subspace]
						For a subspace $H$, then $\{x \in \mathbb{R}^n | xy = 0, y \in H\}$ is a \textbf{orthogonal subspace} and denoted by $H^{\perp}$
					\end{definition}
					
					\begin{proposition}
						If $H = \{x \in \mathbb{R}^n | Ax = 0\}$, with $A$ being an $m \times n$ matrix, then $H^{\perp} = \{x \in \mathbb{R}^n | x = A^{\top}u, u \in \mathbb{R}^m\}$
					\end{proposition}
		
				\subsection{Linear, Conic, Affine, and Convex Combinations}
					\begin{proposition}
						The following statements are equivalent:
						\begin{itemize}
							\item $x^1, x^2, \cdots, x^k \in \mathbb{R}^n$ are affinely independent
							\item $x^2 - x^1, x^3 - x^1, \cdots, x^k - x^1$ are linearly independent
							\item $\begin{bmatrix}x^1 \\ 1\end{bmatrix}, \begin{bmatrix}x^2 \\ 1\end{bmatrix}, \cdots, \begin{bmatrix}x^k \\ 1\end{bmatrix}$ are linearly independent
						\end{itemize}
					\end{proposition}				

			\section{Determinants}
				\subsection{}

			\section{Inner Products}
				\begin{definition}[Inner Product]
					Let $F$ be the field of real numbers or the field of complex numbers, and $V$ a vector space over $F$. An \textbf{inner product} on $V$ is a function which assigns to each ordered pair of vectors $\mathbf{\alpha}$, $\mathbf{\beta}$ in $V$ a scalar $<\mathbf{\alpha}|\mathbf{\beta}>$ in $F$ in such a way that $\forall \mathbf{\alpha}, \mathbf{\beta}, \mathbf{\gamma} \in V, c \in \mathbb{R}$ that
					\begin{itemize}
						\item $<\mathbf{\alpha}+\mathbf{\beta}|\mathbf{\gamma}> = <\mathbf{\alpha}|\mathbf{\gamma}> + <\mathbf{\beta}|\mathbf{\gamma}>$
						\item $<c\mathbf{\alpha}|\mathbf{\beta}> = c<\mathbf{\alpha}|\mathbf{\beta}>$
						\item $<\mathbf{\alpha}|\mathbf{\beta}> = \overline{<\mathbf{\beta}|\mathbf{\alpha}>}$
						\item $<\mathbf{\alpha}|\mathbf{\alpha}> \ge 0$, $<\mathbf{\alpha}|\mathbf{\alpha}> = 0$ iff $\mathbf{\alpha} = \mathbf{0}$
					\end{itemize}
					Furthermore, the above properties imply that
					\begin{itemize}
						\item $<\mathbf{\alpha}|c\mathbf{\beta}+\mathbf{\gamma}> = \bar{c}<\mathbf{\alpha}|\mathbf{\beta}> + <\mathbf{\alpha}|\mathbf{\gamma}>$
					\end{itemize}
				\end{definition}

				\begin{definition}
					On $F^n$ there is an inner product which we call the \textbf{standard inner product}. It is defined on $\mathbf{\alpha} = (x_1, x_2, ..., x_n)$ and $\mathbf{\beta} = (y_1, y_2, ..., y_n)$ by
					\begin{equation}
						<\mathbf{\alpha}|\mathbf{\beta}> = \sum_j x_j \bar{y_j}
					\end{equation}
					For $F = \mathbb{R}^n$
					\begin{equation}
						<\mathbf{\alpha}|\mathbf{\beta}> = \sum_j x_j y_j
					\end{equation}
					In the real case, the standard inner product is often called the dot product and denoted by $\mathbf{\alpha} \cdot \mathbf{\beta}$
				\end{definition}

				\begin{example}
					For $\mathbf{\alpha} = (x_1, x_2)$ and $\mathbf{\beta} = (y_1, y_2)$ in $\mathbb{R}^2$, the following is an inner product.
					\begin{equation}
						<\mathbf{\alpha}|\mathbf{\beta}> = x_1y_1 - x_2y_1 - x_1y_2 + 4x_2y_2
					\end{equation}
				\end{example}

				\begin{example}
					For $\mathbb{C}^{n\times n}$, 
					\begin{equation}
						<\mathbf{A}|\mathbf{B}> = trace(\mathbf{B}^* \mathbf{A})
					\end{equation}
					is an inner product, where
					\begin{equation}
						\mathbf{A}^*_{ij} = \bar{\mathbf{A}}_{ji} \quad (\textbf{conjugate transpose})
					\end{equation}
					For $\mathbb{R}^{n\times n}$,
					\begin{equation}
						<\mathbf{A}|\mathbf{B}> = trace(\mathbf{B}^T \mathbf{A}) = \sum_j (AB^T)_{jj} = \sum_j\sum_k A_{jk}B_{jk}
					\end{equation}
				\end{example}

			\section{Norms}
				\begin{definition}[Norms]
					A \textbf{norm} on a vector space $\mathcal{V}$ is a function $\|\cdot\|:\mathcal{V} \rightarrow \mathbb{R}$ for which the following three properties hold for all point $\mathbf{x}, \mathbf{y} \in \mathcal{V}$ and scalars $\lambda \in \mathbb{R}$
					\begin{itemize}
						\item (Absolute homogeneity) $\|\lambda \mathbf{x} \| = |\lambda| \|\mathbf{x}\|$
						\item (Triangle inequality) $\|\mathbf{x} + \mathbf{y}\| \le \|\mathbf{x}\| + \|\mathbf{y}\|$
						\item (Positivity) Equality $\|\mathbf{x}\| = 0$ holds iff $\mathbf{x} = 0$
					\end{itemize}
				\end{definition}

				\begin{definition}[$L_p$-norms]
					Let $p \ge 1$ be a real number. We define the $p$-norm of vector $\mathbf{v}\in \mathbb{R}^n$ as:
					\begin{equation}
						\|\mathbf{x}\|_p = (\sum_{i = 1}^n|v_i|^p)^{\frac{1}{p}}
					\end{equation}
					Particularly
					\begin{align}
						\|\mathbf{v}\|_1 &= \sum_{i = 1}^n|v_i|\\
						\|\mathbf{v}\|_2 &= \sqrt{\sum_{i = 1}^n v_i^2}\\
						\|\mathbf{v}\|_\infty &= \max_{i=1}^n |v_i|
					\end{align}
				\end{definition}

				\begin{definition}[Frobenius norm]
					$\mathbf{X} \in \mathbb{R}^{m \times n}$, the \textbf{Frobenius norm} is defined as
					\begin{equation}
						\|\mathbf{X}\|_F = \sqrt{trace(\mathbf{X}^\top \mathbf{X})}
					\end{equation}
				\end{definition}

				\begin{definition}[Dual norm]
					For an arbitrary norm $\|\cdot\|$ on Euclidean space $\mathbf{E}$, the \textbf{dual norm} $\|\cdot\|^*$ on $\mathbf{E}$ is defined by
					\begin{equation}
						\|\mathbf{v}\|^* = \max \{<\mathbf{v}|\mathbf{x}>|\|\mathbf{x}\| \le 1\}
					\end{equation}
				\end{definition}

				For $p, q \in [1, \infty]$, the $l_p$ and $l_q$ norms on $\mathbb{R}^n$ are dual to each other whenever $\frac1p + \frac1q = 1$.

			\section{Eigenvectors and Eigenvalues}
				\begin{definition}
					If $\mathbf{A}$ is an $n \times n$ matrix, then a nonzero vector $\mathbf{x} \in \mathbb{R}^n$ is called an \textbf{eigenvector} of $\mathbf{A}$ if $\mathbf{Ax}$ is a scalar multiple of $\mathbf{x}$, i.e.
					\begin{equation}
						\mathbf{Ax} = \lambda \mathbf{x}
					\end{equation}
					for some scalar $\lambda$. The scalar $\lambda$ is called \textbf{eigenvalue} of $\mathbf{A}$ and the vector $\mathbf{x}$ is said to be an \textbf{eigenvector corresponding to $\lambda$}
				\end{definition}

				\begin{theorem}[Characteristic Equation]
					If $\mathbf{A}$ is an $n \times n$ matrix, then $\lambda$ is an eigenvalue of $\mathbf{A}$ iff
					\begin{equation}
						\det(\lambda I - A) = 0
					\end{equation}
				\end{theorem}

				\begin{corollary}
					\begin{equation}
						\sum \lambda_A = tr(\mathbf{A})
					\end{equation}
				\end{corollary}

				\begin{corollary}
					\begin{equation}
						\prod \lambda_A = \det(\mathbf{A})
					\end{equation}
				\end{corollary}

				\notice{Gaussian elimination changes the eigenvalues.}

			\section{Decompositions}

		\chapter{Basic Concepts of Convex Analysis}
			\section{Convex Sets}
				\begin{definition}[convex set, convex combination]
					A set $X$ in $\mathbb{R}^n$ is called a \textbf{convex set} if given any two points $\mathbf{x}_1 \in X$ and $\mathbf{x}_2 \in X$, then $\lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2 \in X, \forall \lambda \in [0, 1]$. Any point of the form $\lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2$ where $0 \le \lambda \le 1$ is called a \textbf{convex combination} of $\mathbf{x}_1$ and $\mathbf{x}_2$. If $\lambda \in (0, 1)$, then the convex combination is called \textbf{strict}.
				\end{definition}

				\begin{example}
					$S = \{(x_1, x_2) | 3x_1^2 + 4x_2^2 \le 1\}$
				\end{example}

				\begin{example}
					$X = \{\mathbf{x} | \mathbf{A}_{m\times n} \mathbf{x} = \mathbf{b}_m\}$
				\end{example}

				The following are some families of convex sets.

				\begin{example}
					Empty set is by convention considered as convex.
				\end{example}

				\begin{example}
					Polyhedrons are convex sets.
				\end{example}

				\begin{example}
					Let $P = \{\mathbf{x}\in \mathbb{R}^n | \mathbf{x}^\top \mathbf{Ax} \le \mathbf{b}\}$ where $\mathbf{A} \in \mathbb{S}_+^{n\times n}$ and $\mathbf{b} \in \mathbb{R}_+$. The set $P$ is a convex subset of $\mathbb{R}^n$.
				\end{example}

				\begin{example}
					Let $\|.\|$ be any norm in $\mathbb{R}^n$. Then, the unit ball $B = \{\mathbf{x} \in \mathbb{R}^n | \|\mathbf{x}\| \le b, b > 0\}$ is convex.
				\end{example}

				Let	$S_1, S_2$ be convex set, then:
				\begin{itemize}
					\item $S_1 \cap S_2$ is convex set
					\item $S_1 \oplus S_2$ (Minkowski addition) is convex set, where
						\begin{equation}
							S_1 \oplus S_2 = \{\mathbf{x} \in \mathbb{R}^n| \mathbf{x} = \mathbf{x_1} + \mathbf{x_2}, \mathbf{x_1} \in S_1, \mathbf{x_2} \in S_2\}
						\end{equation}
					\item $S_1 \ominus S_2$ is convex set, where
						\begin{equation}
							S_1 \oplus S_2 = \{\mathbf{x} \in \mathbb{R}^n| \mathbf{x} = \mathbf{x_1} - \mathbf{x_2}, \mathbf{x_1} \in S_1, \mathbf{x_2} \in S_2\}
						\end{equation}
					\item $f(S_1)$ is convex iff $f(\mathbf{x}) = \mathbf{Ax} + \mathbf{b}, \mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{b} \in \mathbb{R}^m$
				\end{itemize}

				\begin{definition}[convex hull]
					Let $S$ be an arbitrary set in $\mathbb{R}^n$. The \textbf{convex hull} of $S$, denoted by $conv(S)$ is the collection of all convex combinations of elements in $S$\
					\begin{align}
						\mathbf{x} \in conv(S) \iff & \mathbf{x} = \sum_{j = 1}^k \lambda_j \mathbf{x}_j \\
						& \sum_{j = 1}^k \lambda_j = 1\\
						& \lambda_j \le 0, \quad j = 1, \cdots, k \\
						& \mathbf{x}_1, \mathbf{x_2}, \cdots, \mathbf{x_k} \in S
					\end{align}
					
				\end{definition}

				\begin{lemma}
					Let $S$ be arbitrary set in $\mathbb{R}^n$. Then $conv(S)$ is the smallest convex set containing $S$, which means $conv(S)$ is the intersection of all convex sets containing $S$.
				\end{lemma}

				\begin{theorem}[Carath\'{e}odory's Theorem]
					Let $S \subseteq \mathbb{R}^n$. Then $\forall \mathbf{x} \in conv(S)$, there exists $\mathbf{x}^1, \mathbf{x}^2, ..., \mathbf{x}^p \in S$, where $p\le n+1$ such that $\mathbf{x} \in conv\{\mathbf{x}^1, \mathbf{x}^2, ... \mathbf{x}^p\}$.
				\end{theorem}

				\notice{This theorem means, any point $\mathbf{x} \in \mathbb{R}^n$ in a convex hull of $S$, i.e., $conv(S)$, can be included in a convex subset $S^\prime \subseteq conv(S)$ that has $n+1$ extreme points.}

				\begin{theorem}
					Let $S$ be a convex set with nonempty interior. Let $\mathbf{x}_1 \in cl(S)$ and $\mathbf{x}_2 \in int(S)$, then $\mathbf{y} = \lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2 \in int(S), \forall \lambda \in (0, 1)$
				\end{theorem}

			\section{Convex Functions}
				\begin{definition}
					Let $C \in \mathbb{R}^n$ be a convex set. A function $f: C \rightarrow \mathbf{R}$ is (resp. strictly) convex if
					\begin{align}
						f(\lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2) &\le \lambda f(\mathbf{x}_1) + (1 - \lambda) f(\mathbf{x}_2) \\ \forall \mathbf{x}_1, \mathbf{x}_2 &\in C, \forall \lambda \in (0, 1)
					\end{align}
					(resp.)
					\begin{align}
						f(\lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2) &< \lambda f(\mathbf{x}_1) + (1 - \lambda) f(\mathbf{x}_2) \\ \forall \mathbf{x_1} \neq \mathbf{x}_2 &\in C, \forall \lambda \in (0, 1)
					\end{align}
				\end{definition}

				\notice{When calling a function convex, we imply that its domain is convex.}

				\begin{example}
					Given any norm $\|.\|$ on $\mathbb{R}^n$, the function $f(x) = \|x\|$ is convex over $\mathbb{R}^n$.
				\end{example}

				\begin{definition}
					Let $S$ be a nonempty convex subset of $\mathbb{R}^n$, $f: S \rightarrow \mathbb{R}$ is (resp. strictly) \textbf{concave} if $-f(x)$ is (resp. strictly) convex.
				\end{definition}

				\notice{A function may be neither convex nor concave.}

				\begin{theorem}
					Consider $f: \mathbb{R}^n\rightarrow \mathbb{R}$. $\forall \bar{\mathbf{x}} \in \mathbb{R}^n$ and a nonzero direction $\mathbf{d} \in \mathbb{R}^n$. Define $F_{\bar{\mathbf{x}}, d}(\lambda) = f(\bar{\mathbf{x}} + \lambda \mathbf{d}$. Then $f$ is (resp. strictly) convex iff $F_{\bar{\mathbf{x}}, d}(\lambda)$ is (resp. strictly) convex for all $\bar{\mathbf{x}} \in \mathbb{R}^n, \forall \mathbf{d} \in \mathbb{R}^n \setminus \{0\}$.
				\end{theorem}

				\begin{definition}[Level-set]
					Given a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ and a scalar $\alpha \in \mathbb{R}$, we refer to the set $S_\alpha = \{\mathbf{x} \in S|f(\mathbf{x}) \le \alpha\} \subseteq \mathbb{R}^n$ as the \textbf{$\alpha$-level-set} of $f$.
				\end{definition}

				\begin{lemma}
					Let $S$ be a nonempty convex set in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}^n$ be a convex function, then the \textbf{$\alpha$-level-set} of $f$ is a convex set for each value of $\alpha \in \mathbb{R}$.
				\end{lemma}

				\notice{The converse is not necessarily true.}

				\begin{definition}[Epigraphs, Hypographs]
					Let $S \in \mathbb{R}^n$ be such that $S \neq \emptyset$. The \textbf{epigraph} of $f$, denoted by $epi(f)$ is
					\begin{equation}
						epi(f) = \{(\mathbf{x}, y) \in S|\mathbf{x} \in S, y \in \mathbb{R}, y \ge f(x)\} \in \mathbb{R}^{n+1}
					\end{equation}
					The \textbf{hypograph} of $f$, denoted by $hypo(f)$ is
					\begin{equation}
						hypo(f) = \{(\mathbf{x}, y) \in S|\mathbf{x} \in S, y \in \mathbb{R}, y \le f(x)\} \in \mathbb{R}^{n+1}
					\end{equation}
				\end{definition}

				\begin{theorem}
					Let $S$ be a nonempty convex subset in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$. Then $f$ is convex iff $epi(f)$ is convex.
				\end{theorem}

				\begin{theorem}
					Let $S$ be a nonempty convex subset in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be a convex function on $S$. Then $f$ is continuous in $int(S)$.
				\end{theorem}

			\section{Subgradients and Subdifferentials}
				\begin{definition}[Subgradient]
					Let $S$ be a nonempty convex set in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be a convex function, then $\xi$ is a \textbf{subgradient} of $f$ at $\bar{\mathbf{x}}$ if
					\begin{equation}
						f(\mathbf{x}) \ge f(\bar{\mathbf{x}}) + \xi^\top(\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x} \in S
					\end{equation}
				\end{definition}

				\begin{definition}[Subdifferential]
					The set of all subgradients of $f$ at $\bar{\mathbf{x}}$ is called \textbf{subdifferential} of $f$ at $\bar{\mathbf{x}}$, denoted as $\partial f(\bar{\mathbf{x}})$
				\end{definition}

				\begin{theorem}
					Let $S$ be a nonempty convex set in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be a convex function. Then for $\bar{\mathbf{x}} \in int(S)$, there exists a vector $\xi$ such that
					\begin{equation}
						f(\mathbf{x}) \ge f(\bar{\mathbf{x}}) + \xi^\top(\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x} \in S
					\end{equation}
					In particular, the hyperplane
					\begin{equation}
						\mathcal{H} = \{(\mathbf{x}, y)|y = f(\bar{\mathbf{x}}) + \xi^\top(\mathbf{x} - \bar{\mathbf{x}})\}
					\end{equation}
					is a supporting plane of $epi(f)$ at $(\bar{\mathbf{x}}, f(\bar{\mathbf{x}}))$
				\end{theorem}

				\begin{theorem}
					Let $S$ be a nonempty convex set in $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be a convex function. Suppose that for each $\bar{\mathbf{x}} \in S$, there exists $\xi$ such that
					\begin{equation}
					 	f(\mathbf{x}) \ge f(\bar{\mathbf{x}}) + \xi^\top(\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x} \in S
					\end{equation} 
					Then $f$ is convex on $int(S)$
				\end{theorem}

				\notice{Not all convex functions are continuous, it has to be continuous in its interior, but it may not be continuous at the boundary.}

			\section{Differentiable Functions}
				\begin{definition}[Differentiable Functions]
				 	Let $S$ be a nonempty subset of $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$.Then $f$ is said to be \textbf{differentiable} at $\bar{\mathbf{x}} \in int(S)$ if there exists a vector $\nabla f(\bar{\mathbf{x}})$ and a function $\alpha: \mathbb{R}^n \rightarrow \mathbb{R}$ such that
				 	\begin{equation}
				 		f(\mathbf{x}) = f(\bar{\mathbf{x}}) + \nabla f(\bar{\mathbf{x}})^\top (\mathbf{x} - \bar{\mathbf{x}}) + \alpha(\bar{\mathbf{x}}, \mathbf{x} - \bar{\mathbf{x}})\|\mathbf{x} - \bar{\mathbf{x}}\|
				 	\end{equation}
				 	for all $\mathbf{x} \in S$ where $\lim_{\mathbf{x} - \bar{\mathbf{x}}}\alpha (\bar{\mathbf{x}}, \mathbf{x} - \bar{\mathbf{x}}) = 0$
				\end{definition}

				\begin{remark}
				 	If function $f$ is differentiable, then $\nabla f(\bar{\mathbf{x}}) = (\frac{\partial f(\bar{\mathbf{x}})}{\partial \mathbf{x}_1}, \frac{\partial f(\bar{\mathbf{x}})}{\partial \mathbf{x}_2}, \cdots, \frac{\partial f(\bar{\mathbf{x}})}{\partial \mathbf{x}_n})$, and the gradient is unique.
				\end{remark}

				\begin{lemma}
					Let $S\neq \emptyset$ be a convex set of $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be convex. If $f$ is differentiable at $\bar{\mathbf{x}} \in int(S)$, then the subdifferential of $f$ at $\bar{\mathbf{x}}$ is the singleton, $\{\nabla f(\bar{\mathbf{x}})\}$
				\end{lemma}

				\begin{theorem}
					Let $S$ be a nonempty subset of $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be differentiable on $S$. Then $f$ is (resp. strictly) convex on $S$ iff $\forall \bar{\mathbf{x}} \in S$
					\begin{equation}
						f(\mathbf{x}) \ge f(\bar{\mathbf{x}}) + \nabla f(\bar{\mathbf{x}}) (\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x} \in S
					\end{equation}
					(resp.)
					\begin{equation}
						f(\mathbf{x}) > f(\bar{\mathbf{x}}) + \nabla f(\bar{\mathbf{x}}) (\mathbf{x} - \bar{\mathbf{x}}), \forall \mathbf{x}\neq \bar{\mathbf{x}} \in S
					\end{equation}
				\end{theorem}

				\begin{theorem}[Mean-value Theorem]
					Let $S$ be a nonempty subset of $\mathbb{R}^n$. Let $f: S\rightarrow \mathbb{R}$ be differentiable on $S$. Then for all $\mathbf{x}_1, \mathbf{x}_2 \in S$, there exists $\lambda \in (0, 1)$ such that
					\begin{equation}
						f(\mathbf{x}_2) = f(\mathbf{x}_2) + \nabla f(\hat{\mathbf{x}})(\mathbf{x}_2 - \mathbf{x}_1)
					\end{equation}
					where
					\begin{equation}
						\hat{\mathbf{x}} = \lambda \mathbf{x}_1 + (1 - \lambda) \mathbf{x}_2
					\end{equation}
				\end{theorem}

			\section{Convex and Affine Hulls}

			\section{Interior and Closure}

			\section{Cones}

			\section{Hyperplanes}

		\chapter{Basic Concepts of Polyhedral Theory}
			\section{Extreme Points}
				\begin{definition}[extreme points]
					Let $S \subseteq \mathbb{R}^n$, an \textbf{extreme point} of $S$ is a point $x \in S$ such that
					\begin{align}
						\begin{cases}
						 	x = \sum_{i = 1}^m \lambda_ix^i &\\
							\sum_{i = 1}^m \lambda_i = 1&\\
							\lambda_i > 0, i = 1, \cdots, m&\\
							x^i \in S, i = 1, \cdots, m&
						\end{cases} \quad & \quad \text{implies} \quad  x^i = x, i = 1, \cdots, m
					\end{align}
					which means, if $x$ is an extreme point, it cannot be written as a convex combination of points in $S$ except the copies of $x$ itself.
				\end{definition}

				\begin{theorem}
					Let $P$ be a polyhedron in $\mathbb{R}^n$ and $v \in P$. Then $v$ is an extreme point of $P$ iff $dim(span\{a^i | (a^i)^top v = b_i\}) = n$
				\end{theorem}

				This theorem means, at an extreme point of a $n$-dimensional polyhedron, there will be $n$ active inequalities. A direct result will be the equivalence between basic feasible solutions and extreme points in Linear Programming.

			\section{Polar Cones}

			\section{Polyhedral and Dimension}
				\subsection{Polyhedral}
					\begin{definition}[polyhedron]
						A \textbf{polyhedron} is a set of the form $\{x\in \mathbb{R}^n | Ax \le b\}=\{x \in \mathbb{R}^n | a^ix\le b^i, \forall i \in M\}$, where $A \in \mathbb{R}^{m\times n}$ and $b \in \mathbb{R}^m$
					\end{definition}

					\begin{proposition}
						A polyhedron is a convex set.
					\end{proposition}

					\begin{definition}[polytope]
						A polyhedron $P \subset \mathbb{R}^n$ is \textbf{bounded} if there exists a constant $K$ such that $|x_i|<K, \forall x \in P, \forall i \in [1, n]$, in this case the polyhedron is call \textbf{polytope}. The lower-bound of $K$ is called \textbf{diagonal} denoted by $d$
					\end{definition}

					\begin{definition}[cone]
						$C \subseteq \mathbb{R}^n$ is a \textbf{cone} if $x \in C$ implies $\lambda x\in C, \forall \lambda \in \mathbf{R}_{+}$
					\end{definition}

				\subsection{Dimension of Polyhedral}
					\begin{definition}[dimension]
						A polyhedron $P$ is \textbf{dimension} $k$, denoted $dim(P)=k$, if the maximum number of affinely independent points in $P$ is $k+1$
					\end{definition}

					\begin{definition}[full-dimensional]
						A polyhedron $P\subseteq \mathbb{R}^n$ is \textbf{full-dimensional} if $dim(P) = n$
					\end{definition}

					\begin{proposition}
						If $P\subseteq \mathbb{R}^n$, then $dim(P) = n - rank(A^=, b^=)$
					\end{proposition}

					To proof a constraint $(A^=, b^=)$ is an equality constraint, we need to proof all point in the closure of $P$ satisfied the constraint, to proof it is not an equality constraint, we need to find one point that is not in the hyperplane.

					\begin{definition}[inner point, interior point]
						$x\in P$ is called an \textbf{inner point} of $P$ if $a^ix < b_i, \forall i \in M^\le$, $x\in P$ is called an \textbf{interior point} of $P$ if $a^ix<b_i, \forall i \in M$
					\end{definition}

					\begin{corollary}
						Every nonempty polyhedron has at least one inner point.
					\end{corollary}

					\begin{corollary}
						A polyhedron has an interior point iff $P$ is full-dimensional, i.e., there is no equality constraint.
					\end{corollary}

			\section{Face and Facet}
				\subsection{Valid Inequalities and Faces}
					The inequality denoted by $(\pi, \pi_o)$ is called a \textbf{valid inequality} for $P$ if $\pi x \le \pi_0, \forall x \in P$. Note that $(\pi, \pi_0)$ is a valid inequality iff $P$ lies in the half-space $\{x\in \mathbb{R}^n|Ax\le b\}$\\
					\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[scale=0.4]
						\draw (0,0) -- (3, -0.4) -- (3.3, 4.1) -- (-0.2, 4.1) -- (0, 0);
						\draw (-2, 2) -- (2, -2);
						\draw (4.9, 4.1) -- (-1.8, 4.1);
						\draw (4, 1) -- (2, 6);
						\draw [arrow] (-1.8, 4.1) -> (-1.8, 3.3);
						\draw [arrow] (4.9, 4.1) -> (4.9, 3.3);
						\draw [arrow] (-2, 2) -> (-1, 3);
						\draw [arrow] (2, -2) -> (3, -1);
						\draw [arrow] (4, 1) -> (3, 0.6);
						\draw [arrow] (2, 6) -> (1, 5.6);
						\node at (1, -1) [left] {valid inequality};
						\node at (4.9, 4) [right] {valid inequality (induce a facet)};
						\node at (2, 6) [right] {invalid inequality};
					\end{tikzpicture}
					\caption{Example of valid/invalid inequality}
					\end{figure}
					- If $(\pi, \pi_0)$ is a valid inequality for $P$ and $F=\{x\in P|\pi x=x_0\}$, $F$ is called a \textbf{facet} of $P$ and we say that $(\pi, \pi_0)$ \textbf{represents} or \textbf{defines} $F$\\
					- A face is said to be \textbf{proper} if $F\ne \emptyset$ and $F\ne P$\\
					- The face represented by $(\pi, \pi_0)$ is nonempty iff $\max \{\pi x |x\in P\}=\pi_0\}$\\
					- If the face $F$ is nonempty, we say it \textbf{supports} $P$\\
					- Let $P$ be a polyhedron with equality set $M^=$. If 
					\begin{equation}F=\{x\in P | \pi^T x = \pi_0\}  \end{equation}
					is not empty, then $F$ is a polyhedron. Let 
					\begin{equation}M^= \subseteq M_F^=, M_F^{\le}=M \setminus M_F^= \end{equation}
					then 
					\begin{equation}F=\{x | a_i^T x=b_i, \forall i \in M_F^=, a_i^T x \le b_i, \forall i \in M_f^{\le}\} \end{equation}

				\subsection{Facet}
					- A face $F$ is said to be a \textbf{facet} of $P$ if $dim(F) = dim(P)-1$\\
					- Facets are all we need to describe polyhedral\\
					- If $F$ is a facet of $P$, then in any description of $P$, there exists some inequality representing $F$\\
					- Every inequality that represents a face that is not a facet is unnecessary in the description of $P$
					- Every full-dimensional polyhedron $P$ has a unique (up to scalar multiplication) representation that consists of one inequality representing each facet of $P$\\
					- If $dim(P) = n-k$ with $k>0$, then $P$ is described by a maximal set of linearly independent rows of $(A^=, b^=)$, as well as one inequality representing each facet of $P$

				\subsection{Proving Facet}
					To prove an inequality $\sum_i a_i x_i \le b_i$ is facet inducing for a $D$ dimensional polyhedral, we need to prove there are $D$ affinely independent vectors in $\sum_i a_i x_i = b_i$

				\subsection{Domination}
					$\Pi x\le \Pi_0$ dominates $Mx\le M_0$ if
					\begin{equation}
						\begin{cases}
							\Pi \ge \mu M, \mu > 0\\
							\Pi_0 \le \mu M_0, \mu > 0\\
							(\Pi, \Pi_0) \ne (M, M_0)
						\end{cases}
					\end{equation}

		\chapter{Basic Concepts of Real Analysis}
			\section{The Real Number System}

			\section{Open Sets and Closed Sets}
				\begin{definition}[neighborhood]
					$N_\epsilon = \{y\in \mathbb{R}^n|\lVert y-x\rVert < \epsilon \}$ as the \textbf{neighborhood} of $x\in \mathbb{R}^n$
				\end{definition}

				\begin{definition}[interior]
					Given $S\subseteq \mathbb{R}^n$, x belongs to the \textbf{interior} of $S$, denoted by $int(S)$ if there is $\epsilon > 0$ such that $N_\epsilon(x) \subseteq S$
				\end{definition}

				\begin{definition}[boundary]
					$x$ belongs to the \textbf{boundary} $\partial S$ if $\forall \epsilon >0$, $N_\epsilon(x)$ contains at least one point in $S$ and a point not in $S$
				\end{definition}

				\begin{definition}[closure]
					$x\in S$ belongs to the \textbf{closure} of $S$, denoted $cl(s)$ if $\forall \epsilon > 0$, $N_\epsilon(x) \cap S = \emptyset$
				\end{definition}

				\begin{definition}[metric space]
					A \textbf{metric space} is a set $X$ where we have a notion of distance. That is, if $x, y \in X$, then $d(x, y)$ is the distance between $x$ and $y$. The particular distance function must satisfy the following conditions:
					\begin{itemize}
						\item $d(x, y) > 0, \forall x, y \in X$
						\item $d(x, y) = 0 \iff x=y$
						\item $d(x, y) = d(y, x)$
						\item $d(x, z) \le d(x, y) + d(y, z)$
					\end{itemize}
				\end{definition}

				\begin{definition}[ball]
					Let $X$ be a metric space. A \textbf{ball} $B$ of radius $r$ around a point $x \in X$ is
					\begin{equation}
						B = \{y \in X|d(x, y) < r\}
					\end{equation}
				\end{definition}

				\begin{definition}[open set]
					A subset $O \subseteq X$ is \textbf{open} if $\forall x \in O, \exists r, B=\{x\in X|d(x, y) < r\} \subseteq O$
				\end{definition}

				$S$ is said to be an \textbf{open set} iff $S=int(S)$

				\begin{theorem}
					The union of any collection if open sets is open.
				\end{theorem}

				\begin{proof}
					Sets $S_1, S_2, ..., S_n$ are open sets, let $S = \cup_{i=1}^n S_i$, then $\forall i, S_i \subseteq S$. $\forall x\in S, \exists i, x\in S_i$. Given that $S_i$ is an open set, then for $x$, $\exists r$ that $B = \{x \in S_i | d(x, y) < r\} \subseteq S_i \subseteq S$, therefore $S$ is an open set.
				\end{proof}

				\begin{theorem}
					The intersection of any finite number of open sets is open.
				\end{theorem}

				\begin{proof}
					Sets $S_1, S_2, ..., S_n$ are open sets, let $S = \cap_{i=1}^n S_i$, then $\forall i, S \subseteq S_i$. $\forall x\in S, x\in S_i$. For any $i$, we can define an $r_i$, such that $B_i = \{x\in S_i|d(x, y) < r_i\} \subseteq S_i$. Let $r = \min_i\{r_i\}$. Noticed that $\forall i, B^\prime = \{x \in S_i|d(x, y) < r\} \subseteq B_i \subseteq S_i$. Therefore $S$ is an open set.
				\end{proof}

				\begin{remark}
					The intersection of infinite number of open sets is not necessarily open.
				\end{remark}

				Here we find an example that the intersection of infinite number of open sets can be closed.
				\begin{example}
					Let $A_n \in \mathbb{R}$ and $B_n \in \mathbb{R}$ be two infinite series, with the following properties. First, $\forall n, A_n < a, \lim A_n = a$, second, $\forall n, B_n > b, \lim B_n = b$, third $a < b$. Then we define infinite number of sets $S_i$, the $i$th set is defined as
					\begin{equation}
						S_i = (A_i, B_i) \subset \mathbb{R}
					\end{equation}
					Then
					\begin{equation}
						S = \cap_{i=1}^\infty S_i = [a, b] \subset \mathbb{R}
					\end{equation}
					and $S$ is a closed set.
				\end{example}

				\begin{definition}[limit point]
					A point $z$ is a \textbf{limit point} for a set $A$ if every open set $U$ that $z\in U$ intersects $A$ in a point other than $z$.
				\end{definition}

				\notice{$z$ is not necessarily in $A$.}

				\begin{definition}[closed set]
					A set $C$ is \textbf{closed} iff it contains all of its limit points.
				\end{definition}

				$S$ is called \textbf{closed} iff $S=cl(S)$

				\begin{theorem}
					$S\in \mathbb{R}^n$ is closed $\iff \forall \{x_k\}_{k=1}^\infty \in S, \lim_{k \rightarrow \infty} \{x_k\}_{k=1}^\infty \in S$
				\end{theorem}

				\begin{theorem}
					Every intersection of closed sets is closed.
				\end{theorem}

				\begin{theorem}
					Every finite union of closed sets is closed.
				\end{theorem}

				\begin{remark}
					The union of infinite number of closed sets is not necessarily closed.
				\end{remark}

				\begin{theorem}
					A set $C$ is a closed set if $X \setminus C$ is open
				\end{theorem}

				\begin{proof}
					Let $S$ be an open set, $x \notin S$, for any open set $S_i$ that $x\in S_i$, we can find a correspond $r_i > 0$, such that $B_i = \{x \in S_i | d(x, y) < r_i\}$. Take $r = \min_{\forall i}\{r_i\}$, set $B = \{x \notin S|d(x, y) < r\} \neq \emptyset$. Which means for any $x\notin S$, we can find at least one point $x^\prime \in B$ that for all open set $S_i$, $x^\prime \in S_i$, which makes $x$ a limit point of the complement of the open set. Notice that $x$ is arbitrary, then the collection of $x$, i.e., the complement of $S$ is a closed set.
				\end{proof}

				\begin{remark}
					The empty set is open and closed, the whole space $X$ is open and closed.
				\end{remark}

			\section{Functions, Sequences, Limits and Continuity}

			\section{Differentiation}

			\section{Integration}

			\section{Infinite Series of Constants}

			\section{Power Series}

			\section{Uniform Convergence}

			\section{Arcs and Curves}

			\section{Partial Differentiation}

			\section{Multiple Integrals}

			\section{Improper Integrals}

			\section{Fourier Series}

		\chapter{Basic Concepts of Probability Theory}
			\section{Relationship between Some Random Variables}
				\begin{figure}[h]
					\centering
					\begin{tikzpicture}[node distance = 1.8 cm]
						\node (Pascal) [roundedRectangle] {Pascal($n$, $p$)};
						\node (Poisson) [roundedRectangle, xshift = 6 cm] {Poisson($\lambda$)};
						\node (Geometric) [roundedRectangle, below of = Pascal] {Geometric($p$)};
						\node (Binomial) [roundedRectangle, below of = Poisson, xshift = 2 cm] {Binomial($n$, $p$)};
						\node (Bernoulli) [roundedRectangle, below of = Poisson, xshift = 7 cm] {Bernoulli($p$)};
						\node (Lognormal) [roundedRectangle, below of = Geometric, xshift = 2.5 cm] {Lognormal};
						\node (Normal) [roundedRectangle, below of = Geometric, xshift = 7.5 cm] {Normal($\mu$, $\sigma^2$)};
						\node (Hypergeometric) [roundedRectangle, below of = Geometric, xshift = 12.5 cm] {Hypergeometric($M$, $N$, $K$)};
						\node (SNormal) [roundedRectangle, below of = Lognormal, xshift=2 cm] {Normal(0, 1)};
						\node (Beta) [roundedRectangle, below of = Lognormal, xshift = 7 cm] {Beta($\alpha$, $\beta$)};
						\node (DExponential) [roundedRectangle, below of = Beta, xshift = 3.8 cm] {Double-Exponential($0$, $\lambda$, $\lambda$)};
						\node (Cauchy) [roundedRectangle, below of = SNormal, xshift = -6 cm] {Cauchy};
						\node (Gamma) [roundedRectangle, below of = SNormal, xshift = 2 cm] {Gamma($r$,$\lambda$)};
						\node (tv) [roundedRectangle, below of = Cauchy, xshift = 1 cm] {$t_{(v)}$};
						\node (Chi-squared) [roundedRectangle, below of = Gamma, xshift = -2 cm] {Chi-squared($n$)};
						\node (Exponential) [roundedRectangle, below of = Gamma, xshift = 3 cm] {Exponential($\lambda$)};
						\node (Fvv) [roundedRectangle, below of = tv, xshift = 1 cm] {$F_{(V_{1},V_{2})}$};
						\node (Weibull) [roundedRectangle, below of = Exponential, xshift = -3 cm] {Weibull($a$, $b$)};
						\node (SUniform) [roundedRectangle, below of = Exponential, xshift = 3 cm] {Uniform($0$, $1$)};
						\node (Uniform) [roundedRectangle, below of = SUniform, xshift = -2 cm] {Uniform($a$, $b$)};
						\draw [arrow] (Pascal) to [out = -30, in = 30] node [right] {$n=1$} (Geometric);
						\draw [arrow] (Pascal) -- node [above, align=center, text width=2.5 cm] {$\lambda=\frac{n}{p}$ \\ $n\rightarrow \infty$} (Poisson);
						\draw [arrow] (Pascal) to [out = -15, in = 170] node [below, align=center, text width=2.5 cm, yshift=0.1 cm, xshift=-1.1 cm] {$\mu = n(1-p)$ \\ $n\rightarrow \infty$} (Normal);
						\draw [arrow] (Geometric) to [out = 150, in = -150] node [left] {$\sum X_i$} (Pascal);
						\draw [arrow] (Geometric) to [out = -160, in = 180, looseness=6] node [below, yshift = -0.2 cm] {$\min(X_i)$} (Geometric);
						\draw [arrow] (Poisson) to [out = 70, in = 110, looseness=4] node [above] {$\sum X_i$} (Poisson);
						\draw [arrow] (Poisson) to [out = -90, in = 145] node [left] {$X|\sum X_i$} (Binomial);
						\draw [arrow] (Poisson) to [out = -150,in = 160] node [left, align=center, yshift=0.6 cm] {$\lambda=\sigma^2$ \\ $n\rightarrow \infty$} (Normal);
						\draw [arrow] (Binomial) to [out = 75, in = -40] node [right, align=center] {$\lambda=np$ \\ $n\rightarrow \infty$} (Poisson);
						\draw [arrow] (Binomial) -- node [below] {$n=1$} (Bernoulli);
						\draw [arrow] (Bernoulli) to [out = 165, in = 15] node [above] {$\sum X_i$} (Binomial);
						\draw [arrow] (Binomial) to [out = -70, in = 30, align=center] node [left] {$\sigma^2=np(1-p)$ \\ $\mu=np, n\rightarrow \infty$} (Normal);
						\draw [arrow] (Hypergeometric) to [out = 170, in = -45] node [right] {$p=\frac{M}{N}, n=k, N\rightarrow \infty$} (Binomial);
						\draw [arrow] (Normal) to [out = 0, in = -45, looseness=3] node [right] {$\sum X_i$} (Normal);
						\draw [arrow] (Normal) to [out = 175, in = 5] node [above, xshift=-0.3 cm] {$e^{X}$} (Lognormal);
						\draw [arrow] (Normal) to [out = -155, in = 30] node [right, xshift=0.2 cm] {$\frac{X-\mu}{\sigma^2}$} (SNormal); 
						\draw [arrow] (Lognormal) to [out = -5, in = -175] node [below] {$\ln X$} (Normal);
						\draw [arrow] (Lognormal) to [out = 160, in = -160, looseness = 3] node [left] {$\prod X_i$} (Lognormal);
						\draw [arrow] (SNormal) to [out = 90, in = -165] node [left, yshift = -0.4 cm, xshift = -0.3 cm] {$\mu + \sigma X$} (Normal);
						\draw [arrow] (Beta) to [out = 120, in = -90] node [right, yshift=-0.3 cm, xshift=0.9 cm] {$\alpha, \beta \rightarrow \infty$} (Normal);
						\draw [arrow] (Beta) to [out = 0, in = 30, looseness=2.4] node [right, yshift=1 cm, xshift=-3 cm] {$\alpha=\beta=1$} (SUniform);
						\draw [arrow] (Gamma) to [out = 100, in = -110] node [right, align = center, yshift=-0.8 cm, xshift = -0.1 cm] {$\mu=r\lambda$\\$\sigma^2=r\lambda^2$\\$r\rightarrow \infty$} (Normal); 
						\draw [arrow] (Gamma) to [out = 15, in = -90] node [right] {$\frac{X_1}{X_1 + X_2}$} (Beta);
						\draw [arrow] (Gamma) to [out = -165, in = 75] node [right] {$r=\frac{n}{2}, \lambda=2$} (Chi-squared);
						\draw [arrow] (Gamma) to [out = -40, in = 160] node [left] {$r=1$} (Exponential);
						\draw [arrow] (SNormal) to [out = 180, in = 30] node [right] {$\frac{X_1}{X_2}$} (Cauchy);
						\draw [arrow] (SNormal) to [out = -120, in =90] node [left] {$\sum X_i^2$} (Chi-squared);
						\draw [arrow] (Cauchy) to [out = 80, in = 120, looseness = 4] node [above] {$\frac{1}{X}$} (Cauchy);
						\draw [arrow] (Cauchy) to [out = -120, in = -140, looseness = 3] node [below] {$\sum X_i$} (Cauchy);
						\draw [arrow] (tv) to [out = 90, in = -90] node [right, yshift = 0.2 cm] {$v=1$} (Cauchy);
						\draw [arrow] (tv) to [out = 60, in = -130] node [left] {$v\rightarrow \infty$} (SNormal);
						\draw [arrow] (tv) to [out = -90, in = 120] node [left] {$X^2$} (Fvv);
						\draw [arrow] (Fvv) to [out = 30, in = -120] node [below] {$V_1X, V_2\rightarrow \infty$} (Chi-squared);
						\draw [arrow] (Chi-squared) to [out = -165, in = 60] node [above] {$\frac{X_1V_2}{X_2V_1}$} (Fvv);
						\draw [arrow] (Chi-squared) to [out = 125, in = 175, looseness = 3] node [above] {$\sum X_i$} (Chi-squared);
						\draw [arrow] (Chi-squared) -- node [below] {$n=2$} (Exponential);
						\draw [arrow] (Exponential) to [out = 140, in = 0] node [right] {$\sum X_i$} (Gamma);
						\draw [arrow] (Exponential) to [out = 50, in = -120] node [below] {$X_1 - X_2$} (DExponential);
						\draw [arrow] (Exponential) to [out = 10, in = -20, looseness = 3] node [right] {$\min X_i$} (Exponential);
						\draw [arrow] (Exponential) to [out = -40, in = 180] node [right] {$e^{-\frac{X}{\lambda}}$} (SUniform);
						\draw [arrow] (Exponential) to [out = -100, in = 0] node[right] {$X^{\frac{1}{b}}$} (Weibull);
						\draw [arrow] (DExponential) to [out = -170, in = 80] node [above] {$|X|$} (Exponential);
						\draw [arrow] (Uniform) to [out = 0, in = -80] node [right] {$a=0, b=1$} (SUniform);
						\draw [arrow] (SUniform) to [out = 50, in = -30] node [below] {$-\lambda \ln X$} (Exponential);
						\draw [arrow] (SUniform) to [out = -110, in = 30] node [left] {$a+(b-a)X$} (Uniform);
						\draw [arrow] (Weibull) to [out = 70, in = -160] node [left] {$b=1$} (Exponential);
					\end{tikzpicture}
					\caption{Relationship between Some Random Variables}
				\end{figure}

			\section{Discrete Random Variables}
				\begin{align}
					\centering
					\begin{tabular}[c]{|c|c|c|c|c|c|}
						\hline
						Distribution & PMF & CDF & Exp. & Var. & MGF \\
						\hline
						Uniform$(a, b)$ & 
						\makecell{$\frac1{b-a+1}$ \\ $x=a, a+1, ..., b$} & 
						\makecell{$\frac{x-a+1}{b-a+1}$ \\ $x=a, a+1, ..., b$} & 
						$\frac{b-a}{2}$ & 
						$\frac{(b-a+1)^2-1}{12}$ & 
						\makecell{$\frac{e^{at}-e^{(b+1)t}}{(b-a+1)(1-e^t)}$ \\ $t \in \mathbb{R}$} \\
						\hline
						Bernoulli$(p)$ & 
						\makecell{$p^x(1-p)^{1-x}$ \\ $x \in \{0,1\}$} &
						$\begin{cases}0, \quad x<0 \\ 
											1-p, \quad 0\le x \le 1 \\
											1, \quad x>1\end{cases}$ & 
						$p$ & 
						$p(1-p)$ & 
						\makecell{$1-p+pe^{t}$ \\ $t\in \mathbb{R}$}\\
						\hline
						Binomial$(n, p)$ &
						\makecell{$\left(\begin{matrix}n\\x\end{matrix}\right)p^x(1-p)^{n-x}$ \\ $x=0,1,...,n$} &
						\makecell{$\sum_{k=0}^{x}\left(\begin{matrix}n\\k\end{matrix}\right)p^k(1-p)^{n-k}$ \\ $x=0,1,...,n$} &
						$np$ &
						$np(1-p)$ &
						\makecell{$(1-p+pe^{t})^n$ \\ $t\in \mathbb{R}$}\\
						\hline
						Poisson$(\mu)$ &
						\makecell{$\frac{\mu^xe^\mu}{x!}$ \\ $x = 0,1,...,n,...$} &
						\makecell{$\frac{\Gamma(x+1, \mu)}{\Gamma(x+1)}$ \\ $x=0,1,...,n,...$} &
						$\mu$ &
						$\mu$ &
						\makecell{$e^{\mu(e^t-1)}$ \\ $t\in \mathbb{R}$} \\
						\hline
						Geometric$(p)$ &
						\makecell{$p(1-p)^x$ \\ $x=0,1,...,n,...$} &
						\makecell{$1-(1-p)^{x+1}$ \\ $x=0,1,...,n,...$} &
						$\frac{1-p}p$ &
						$\frac{1-p}{p^2}$ &
						\makecell{$\frac{p}{1-(1-p)e^t}$ \\ $t < -\ln(1-p)$}\\
						\hline
						Pascal$(n, p)$ &
						\makecell{$\left(\begin{matrix}n-1+x\\x\end{matrix}\right)p^n(1-p)^x$ \\ $x=0,1,2,...,n,...$} &
						\makecell{$1-I_p(k+1,n)$ \\ $x=0,1,2,...,n,...$} &
						$\frac{n(1-p)}{p}$&
						$\frac{n(1-p)}{p^2}$ &
						\makecell{$(\frac{p}{1-(1-p)e^t})^n$ \\ $t < -\ln(1-p)$}\\
						\hline
					\end{tabular}
				\end{align}

			\section{Continuous Random Variables}
				\begin{align}
					\centering
					\begin{tabular}[c]{|c|c|c|c|c|c|}
						\hline
						Distribution & PDF & CDF & Exp. & Var. & MGF \\
						\hline
						Uniform$(a, b)$ & 
						\makecell{$\frac1{b-a}$ \\ $x=[a,b]$} & 
						\makecell{$\frac{x-a}{b-a}$ \\ $x=[a,b]$} & 
						$\frac{b-a}{2}$ & 
						$\frac{(b-a)^2}{12}$ & 
						$\begin{cases}1, \quad t=0 \\ \frac{e^{bt}-e^{at}}{t(b-a)}, \quad t\ne 0\end{cases}$\\
						\hline
						Normal$(\mu, \sigma)$ &
						\makecell{$\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ \\ $x\in \mathbb{R}$} &
						\makecell{$\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ \\ $x\in \mathbb{R}$} &
						$\mu$ &
						$\sigma^2$ &
						\makecell{$e^{\frac{t(t\sigma^2+2\mu)}{2}}$ \\ $t\in \mathbb{R}$}\\
						\hline
						Exponential$(\lambda)$ &
						\makecell{$\lambda e^{-\lambda x}$ \\ $x>0$} &
						\makecell{$1-e^{\lambda x}$ \\ $x>0$} &
						$\frac{1}{\lambda}$ &
						$\frac{1}{\lambda^2}$ &
						\makecell{$\frac{1}{1-\frac{t}{\lambda}}$ \\ $t<\lambda$}\\
						\hline
						Erlang$(n, \lambda)$ &
						\makecell{$\frac{\lambda^n x^{n-1} e^{-\lambda x}}{(n-1)!}$ \\ $x>0$} &
						\makecell{$1-\sum_{i=0}^{n-1}\frac{\lambda^n x^n e^{-\lambda x}}{n!}$ \\ $x>0$} &
						$\frac{n}{\lambda}$ &
						$\frac{n}{\lambda^2}$ &
						\makecell{$\frac{1}{(1-\frac{t}{\lambda})^n}$ \\ $t<\lambda$} \\
						\hline
					\end{tabular}
				\end{align}

	\part{Linear Programming}\label{LP}
		\chapter{The Simplex Method - Basic}
			\section{Basic Feasible Solutions and Extreme Points}
				\begin{definition}[Basic Feasible Solutions]
					Consider the system $\{\mathbf{A_{m\times n}x} = \mathbf{b_m}, \mathbf{b_m}\ge \mathbf{0}\}$, suppose $rank(\mathbf{A}, \mathbf{b}) = rank(\mathbf{A}) = m$, we can rearrange the columns of $\mathbf{A}$ so that we have a partition of $\mathbf{A}$. Let $\mathbf{A} = \left[\begin{matrix}\mathbf{B} & \mathbf{N}\end{matrix}\right]$ where $\mathbf{B}$ is an $m\times m$ invertible matrix, and $\mathbf{N}$ is an $m\times (n-m)$ matrix. The solution $\mathbf{x}=\left[\begin{matrix}\mathbf{x_B}\\\mathbf{x_N}\end{matrix}\right]$ to the equation $\mathbf{Ax} = \mathbf{b}$, where $\mathbf{x_B} = \mathbf{B}^{-1} \mathbf{b}$ and $\mathbf{x_N} = \mathbf{0}$ is called \textbf{basic solution} of system. If $\mathbf{x_B} \ge \mathbf{0}$, it is called \textbf{basic feasible solution} or \textbf{B.F.S.}. If $\mathbf{x_B} > \mathbf{0}$ it is called \textbf{non-degenerate basic feasible solution}. For $\mathbf{x_B} \ge \mathbf{0}$, if some $x_j = 0$, those components are called \textbf{degenerated basic feasible solution}. $\mathbf{B}$ is called the \textbf{basic matrix}, $\mathbf{N}$ is called \textbf{nonbasic matrix}
				\end{definition}

				\begin{theorem}
					$\mathbf{x}$ is an extreme point $\iff$ $\mathbf{x}$ is a basic feasible solution.
				\end{theorem}

				\begin{proof}
					\fixme{This proof is lack of details.}
					Denote $\mathcal{S}$ as feasible region. 

					($\Rightarrow$) First, Let $\mathbf{x}$ be a B.F.S., Suppose $\mathbf{x} =\lambda \mathbf{u} + (1 - \lambda) \mathbf{v}$, for $\mathbf{u}, \mathbf{v} \in \mathcal{S}, \lambda \in (0, 1)$. Let $I = \{i: x_i > 0\}$ be the set of index where the inequality constraint are not tight. Then for $i \notin I$, $x_i = 0$, which implies $u_i = v_i = 0$. $\mathbf{u}, \mathbf{v} \in \mathcal{S} \Rightarrow \mathbf{Au} = \mathbf{Av} = \mathbf{b} \Rightarrow \mathbf{A (u - v)} = \mathbf{0} \Rightarrow \sum_{i = 1}^n (u_i - v_i)a_i = 0$.
					\begin{itemize}
						\item 
					\end{itemize}
					- if $i \notin I$ then $x_i = 0$, which implies $u_i = v_i = 0$
					- $\because \mathbf{Au} = \mathbf{Av} = \mathbf{b}$, $\therefore \mathbf{A(u-v)} = \textbf{0} \Rightarrow \sum_{i=1}^n(u_i - v_i)a_i = 0$, $\because u_i = v_i = 0$, for $i\notin I$, it implies $u_i = v_i$ for $i\in I$, Hence $u=v$, $x$ is E.P.\\

					($\Leftarrow$) Second, suppose $\mathbf{x}$ is not B.F.S., i.e. $\{a_i: i \in I\}$ are linearly dependent.\\
					Then there $\exists \mathbf{u}\ne \mathbf{0}, u_i =0 , i\notin I$ such that $\mathbf{Au}=\mathbf{0}$.\\
					Hence, for a small $\epsilon$, $\mathbf{x}=\frac12(\mathbf{x} + \epsilon \mathbf{u}) + \frac12(\mathbf{x} - \epsilon \mathbf{u})$, $\mathbf{x}$ is not E.P.
				\end{proof}

			\section{The Simplex Method}
				\subsection{Key to Simplex Method}
					\subsubsection{Cost Coefficient}
						The cost coefficient can be derived from the following
						\begin{align}
							z &= cx \\
							  &= c_Bx_B + c_Nx_N \\
							  &= c_B(B^{-1}b - B^{-1}Nx_N) + c_Nx_N\\
							  &= c_BB^{-1}b - \sum_{j\in N}(c_BB^{-1}a_j - c_j)x_j \\
							  &= c_BB^{-1}b - \sum_{j\in N}(z_j-c_j)x_j 
	 					\end{align}
	 					We denote $z_0 = c_BB^{-1}b$, $z_j = c_B^{-1}a_j$, $\bar{b} = B^{-1}b$ and $y_j = B^{-1}a_j$ for all nonbasic variables.\\
	 					The formulation can be transformed into
	 					\begin{align}
	 						\min \quad & z = z_0 - \sum_{j\in N}(z_j - c_j)x_j\\
	 						\text{s.t.} \quad & \sum_{j\in N}y_jx_j + x_B = \bar{b} \\
	 										  & x_j \ge 0, j\in N \\
	 										  & x_B \ge 0 
	 					\end{align}
	 					In the above formulation, $z_j - c_j$ is the cost coefficient. If $\exists j$ and $z_j - c_j > 0$, it means the objective function can still be optimized. (If $\forall j$, $z_j - c_j \le 0$, then $z \ge z_0$ for any feasible solution, $z$ is the optimal solution)

	 				\subsubsection{Pivot}
	 					After finding the most violated $z_j - c_j$, we find a variable, say $x_k$, where $z_k - c_k = \min \{z_j - c_j\}$ to be the variable leaving the basis. \\
	 					If there are degenerated variables, we can perform different method to choose variable to enter basis.

	 				\subsubsection{Minimum Ratio}
	 					\begin{equation}
	 						x_{B_i} = \bar{b}_i - y_{ik}x_k \ge 0 
	 					\end{equation}
	 					Therefore we have the minimum ratio rule
	 					\begin{equation}
	 						x_k = \min_{i \in B} \{\frac{\bar{b}_i}{y_{ik}}, y_{ik} > 0\} 
	 					\end{equation}
	 					If for the that column all $y_{ik} \le 0$, unbounded.

				\subsection{Simplex Method Algorithm}
					The pseudo-code of Simplex Method is given as following:
					\begin{algorithm}[h!]
						\caption{Simplex Method}
						\begin{algorithmic}[1]
							\Require Given a basic feasible solution with basis $B$
							\Ensure Optimal objective value $\min z= cx$
							\State Set $\mathbf{B}$ for basic variables, $\mathbf{N}$ for nonbasic variables
							\State $\mathbf{B} \gets \text{all slack variables}$
							\State $\mathbf{N} \gets \text{all variables excepts slack variables}$
							\For{$\forall j$}
								\State $z_j=c_BB^{-1}a_j=0$
							\EndFor
							\While{$\exists z_j-c_j > 0$}
								\State $z_j=wa_j-c_j=c_BB^{-1}a_j-c_j$
								\State $z_k-c_k=\max\limits_{j \in \mathbf{N}}\{z_j-c_j\}$
								\State $y_k=B^{-1}a_k$
								\If{$\exists y_{ik} >0$}
									\State $\theta_r=\min\limits_{i \in \mathbf{B}}\{\theta_i=\frac{\bar{b}_i}{y_{ik}}:y_{ik}>0\}$
									\State $\mathbf{B} \gets \mathbf{B} \backslash \{k\}$
									\State $\mathbf{N} \gets \mathbf{N} \cup \{k\}$
									\State $\mathbf{B} \gets \mathbf{B} \cup \{r\}$
									\State $\mathbf{N} \gets \mathbf{N} \backslash \{r\}$
								\Else
									\State Unbounded
								\EndIf
							\EndWhile
							\State $x_B^*=B^{-1}b=\bar{b}$
							\State $x_N=0$
							\State $z^*=c_BB^{-1}b=c_B\bar{b}\mathbf{a_{B_k}}$
						\end{algorithmic}
					\end{algorithm}

			\section{Tableau Method for Simplex Method}
				The following is an example of using tableau to solve simplex method.
					Initial tableau:\\
					\begin{align}
						\begin{tabular} {|c|c|c|c|c|c|c|c|}
							\hline
							& $z$ & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & RHS \\
							\hline
							$z$ & 1 & 1 & 3 & 0 & 0 & 0 & 0 \\
							\hline
							$x_3$ & 0 & 1 & -2 & 1 & 0 & 0 & 0 \\
							\hline
							$x_4$ & 0 & -2 & \framebox{1} & 0 & 1 & 0 & 4\\
							\hline
							$x_5$ & 0 & 5 & 3 & 0 & 0 & 1 & 15 \\
							\hline
						\end{tabular}
					\end{align}
					Last tableau:\\
					\begin{align}
						\begin{tabular} {|c|c|c|c|c|c|c|c|}
							\hline
							& $z$ & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & RHS \\
							\hline
							$z$ & 1 & 0 & 0 & 0 & $-\frac{12}{11}$ & $-\frac{7}{11}$ & $-\frac{153}{11}$ \\
							\hline
							$x_3$ & 0 & 0 & 1 & 1 & $\frac{13}{11}$ & $\frac{3}{11}$ & $\frac{97}{11}$ \\
							\hline
							$x_2$ & 0 & 1 & 0 & 0 & $\frac{5}{11}$ & $\frac{2}{11}$ & $\frac{50}{11}$ \\
							\hline
							$x_1$ & 1 & 0 & 0 & 0 & $-\frac{3}{11}$ & $\frac{1}{11}$ & $\frac{3}{11}$ \\
							\hline
						\end{tabular}
					\end{align}
					- The optimal basic variables are $x_3$, $x_2$, $x_1$. The optimal basis is the columns in the initial tableau with correspond columns
					\begin{equation}
						B = \left(\begin{matrix}
							\frac{13}{11} & \frac{3}{11} & \frac{97}{11}\\
							\frac{5}{11} & \frac{2}{11} & \frac{50}{11}\\
							-\frac{3}{11} & \frac{1}{11} & \frac{3}{11}\\
						\end{matrix}\right)
					\end{equation}
					- From the initial tableau, we can see the initial basis is built from slack variables $x_3$, $x_4$, $x_5$. The $B^{-1}$ is the correspond columns in final tableau.
					\begin{equation}
						B = \left(\begin{matrix}
							1 & -2 & 1\\
							0 & 1 & -2\\
							0 & 3 & 5\\
						\end{matrix}\right) 
					\end{equation}
					- The optimal basic variables are $x_3$, $x_2$, $x_1$. Find $c_B$ in the initial tableau.
					\begin{equation}
						c_B = \left(\begin{matrix}
							0\\3\\1
						\end{matrix}\right) 
					\end{equation}
					- Find $w=c_BB^{-1}$ from the final tableau, correspond to the slack variable.
					\begin{equation}
						w = c_BB^{-1} = \left(\begin{matrix}
							0\\-\frac{12}{11}\\-\frac7{11}
						\end{matrix}\right) 
					\end{equation}

			\section{Initial Basis}
				If some of the constraint is not in $\sum_{i=1}^na_ix_i \le 0$ form, we cannot add a positive slack variable. In this case, we add an artificial variable other than slack variable.
				\begin{equation}
					\sum_{i=1}^n a_ix_i \ge (or =) 0 \Rightarrow \sum_{j=1}^n a_ix_i + x_a = 0 
				\end{equation}
				Notice that in an optimal solution, $x_a = 0$, otherwise it is not valid.\\
				Artificial variables are only a tool to get the simplex method started.
				\subsection{Two-Phase Method}
					\subsubsection{Two-Phase Method}
						For \textbf{Phase I:}\\
							Solve the following program start with a basic feasible solution $x=0, x_a=b$, i.e., the artificial variable forms the basis.
							\begin{align}
								\min \quad & \mathbf{1}x_a \\
								\text{s.t.} \quad & Ax + x_a = b \\
												  & x \ge 0 \\
												  & x_a \ge 0 
							\end{align}
							If the optimal $\mathbf{1}x_a \ne 0$, infeasible, stop. Otherwise proceed Phase II.
						For \textbf{Phase II:}\\
							Remove the columns of artificial variables, replace the objective function with the original objective function, proceed to solve simplex method.
					\subsubsection{Discussion}
						\textbf{Case A:} $x_a \ne 0$\\
							Infeasible.\\
						\textbf{Case B.1:} $x_a = 0$ and all artificial variables are out of the basis\\
						At the end of Phase I, we derive
						\begin{align}
							\begin{tabular} {|c|c|c|c|c|}
								\hline
								$x_0$ & $x_B$ & $x_N$ & $x_a$ & RHS \\
								\hline
								1 & 0 & 0 & -1 & 0\\
								\hline
								0 & $I$ & $B^{-1}N$ & $B^{-1}$ & $B^{-1}b$ \\
								\hline
							\end{tabular} 
						\end{align}
						We can discard $x_a$ columns, (or we can leave it because it keeps track of $B^{-1}$), and then we do the Phase II
						\begin{align}
							\begin{tabular} {|c|c|c|c|}
								\hline
								$z$ & $x_B$ & $x_N$ & $RHS$ \\
								\hline
								1 & 0 & $c_BB^{-1}N - c_N$ & $c_BB^{-1}b$ \\
								\hline
								0 & $I$ & $B^{-1}N$ & $B^{-1}b$ \\
								\hline
							\end{tabular} 
						\end{align}
						\textbf{Case B.2:} Some artificial variables are in the basis at zero values\\
						This is because of degeneracy. We pivot on those artificial variables, once they leave the basis, eliminate them.
				\subsection{Big M Method}

				\subsection{Single Artificial Variable}

			\section{Degeneracy and Cycling}
				\subsection{Degeneracy}
					\subsubsection{Degeneracy in Simplex Method}
						If the basic variable $x_B$ is not strictly $> 0$, i.e. if some basic variable equals to 0, we call it degenerate.

					\subsubsection{Degeneracy for Bounded Variables}
						If some basic variables are at their upper bound or lower bound, we call it degenerate.

				\subsection{Cycling}
					In the degenerate case, pivoting by the simplex rule does not always give a strict decrease in the objective function value, because it may have $b_r = 0$. It is possible that the tableau may repeat if we use the simplex rule.\\
					Geometrically speaking, it means that at the same point - extreme point - it corresponds to more than one feasible solutions, so when we are pivoting, we stays at the same place.\\
					In computer algorithm, we rarely care about cycling because the data in computer is not precise, it is very hard to get into cycling.

				\subsection{Cycling Prevent}
					\subsubsection{Lexicographic Rule}
						- For entering variable, same as simplex rule\\
						- For leaving variable, if there is a tie, choose the variable with the smallest $\frac{y_{r1}}{y_{rk}}$.

					\subsubsection{Bland's Rule}
						- For entering variable, choose the variable with smallest index where $z_j - c_j \le 0$\\
						- For leaving variable, if there is a tie, choose the variable with smallest index.

					\subsubsection{Successive Ratio Rule}
						- Select the pivot column as any column $k$ where $z_k - c_k \le 0$\\
						- Given $k$, select the pivot row $r$ as the minimum successive ratio row associated with column $k$.\\
						In other words, for pivot columns where there is no tie in the usual minimum ratio, the successive ratio rule reduces to the simplex rule

			\section{As a Search Algorithm}
				\subsection{Improving Search Algorithm}
					A simplex method is a search algorithm, for each iteration it finds a not-worse solution, which can be represented as:\\
					\begin{equation}x^t = x^{t-1}+\lambda_{t-1}d^{t-1}  \end{equation}
					Where\\
					- $x^t$ is the solution of the $t$th iteration\\
					- $\lambda_t$ is the step length of $t$th iteration\\
					- $d^t$ is the direction of the $t$th iteration\\
					For each iteration, it contains three steps:\\
					- Optimality test\\
					- Find direction\\
					- Find the step length

				\subsection{Optimality Test}
					\begin{align}
						z &= cx \\
						& = \left[\begin{matrix}c_B & c_N\end{matrix} \right] \left [ \begin{matrix}x_B \\ x_N \end{matrix} \right]  \\
						& = c_B x_B + c_N x_N  \\
					\text{and} \because &Ax=b  \\
						\therefore & Bx_B + Nx_N = b, x_B\ge 0, x_N\ge 0 \\
						\therefore & x_B = B^{-1}b-B^{-1}Nx_N \\
						z &= c_BB^{-1}b-c_BB^{-1}Nx_N+c_Nx_N
					\end{align}
					for current solution $\hat{x}=\left [\begin{matrix}\hat{x_B} \\ 0\end{matrix}\right]$, $\hat{z} = c_BB^{-1}b$, then
					\begin{equation}
						z - \hat{z} = \left[\begin{matrix}0 & c_N - c_BB^{-1}N \end{matrix} \right] \left[ \begin{matrix}x_B \\ x_N \end{matrix}\right] 
					\end{equation}
					The $c_N - c_BB^{-1}N$ is the reduced cost, for a minimized problem, if $c_N - c_BB^{-1}N > 0$ means $z - \hat{z} \ge 0$, it reaches the optimality because we cannot find a solution less than $\hat{z}$.\\

				\subsection{Find Direction}
					Suppose we choose $x_k$ as a candidate to pivot into Basis\\
					\begin{equation}
						x = \left[ \begin{matrix}B^{-1}b-B^{-1}a_kx_k \\ 0+e_kx_k\end{matrix}\right]=\left[ \begin{matrix}B^{-1}b \\ 0\end{matrix} \right] + \left[ \begin{matrix} -B^{-1}a_k \\ e_k \end{matrix} \right]x_k 
					\end{equation}
					In this form, we can see: $x$ is the result after $t$th iteration, $\left[ \begin{matrix}B^{-1}b \\ 0\end{matrix} \right]$ is the result after $(t-1)$th iteration. $ \left[ \begin{matrix} -B^{-1}a_k \\ e_k \end{matrix} \right]$ is the iteration direction, $x_k$ is the step length.\\
					The only requirement of $x_k$ is $r_k < 0$ where $r_k=c_k - z_k$ is reduce cost, which is the $k$th entry of $c_N - c_BB^{-1}N$. \\
					Generally speaking, we usually take $r_k = \min\{c_j - z_j\}$ (which in fact can not guarantee the efficient of the algorithm.)

				\subsection{Find the Step Length}
					We need to guarantee the non-negativity, so for each iteration, we need to make sure $x\ge 0$. Which means\\
					\begin{equation}B^{-1}b-B^{-1}a_kx_k \ge 0  \end{equation}
					Denote $B^{-1}b$ as $\bar{b}$, denote $B^{-1}a_k$ as $y_k$\\
					If $y_k \le 0$, we can have $x_k$ as large as infinite, which means unboundedness. \\
					If $y_k > 0$ now we can use the minimum ratio to guarantee non-negativity.\\
					\textbf{Remember} hit the bound, basic variable leave the basis and become non-basic variable.

		\chapter{The Simplex Method - Improved}
			\section{Revised Simplex Method}
				\subsection{Key to Revised Simplex Method}
					The procedure of Simplex Method is (almost) exactly the same as original simplex method. However, notice that we don't need to use $N$ so for the revised simplex method, we don't calculate any matrix related to $N$\\
					The original matrix:
					\begin{align}
						\begin{tabular} {|c|c|c|c|}
							\hline
							$z$ & $x_B$ & $x_N$ & $RHS$ \\
							\hline
							1 & 0 & $c_BB^{-1}N - c_N$ & $c_BB^{-1}b$ \\
							\hline
							0 & $I$ & $B^{-1}N$ & $B^{-1}b$ \\
							\hline
						\end{tabular} 
					\end{align}
					The revised matrix:
					\begin{align}
						\begin{tabular} {|c|c|}
							\hline
							Basic Inverse & RHS \\
							\hline
							$w=c_BB^{-1}$ & $c_B\bar{b} = c_BB^{-1}b$\\
							\hline
							$B^{-1}$ & $\bar{b} = B^{-1}b$\\
							\hline
						\end{tabular} 
					\end{align}
					For each pivot iteration, calculate $z_j - c_j = wa_j - c_j = c_BB^{-1}a_j - c_j, \forall j\in N$, pivot rules are the same as simplex method, each time find a variable $x_k$ to enter basis
					\begin{align}
						\begin{tabular}{|c|c|c|c|}
							\cline{1-2} \cline{4-4} $B^{-1}$ & RHS & & $x_k$ \\
							\cline{1-2} \cline{4-4} $w$ & $c_B\bar{b}$ & & $z_k-c_k$ \\
							\cline{1-2} \cline{4-4} $B^{-1}$ & $\bar{b}$ & & $y_k$ \\
							\cline{1-2} \cline{4-4}
						\end{tabular} 
					\end{align}
					Do the minimum ratio rule to find the variable $x_r$ to leave the basis
					\begin{align}
						\begin{tabular}{|c|c|c|c|}
							\cline{1-2} \cline{4-4} $B^{-1}$ & RHS & & $x_k$ \\
							\cline{1-2} \cline{4-4} $w$ & $c_B\bar{b}$ & & $z_k-c_k$ \\
							\cline{1-2} \cline{4-4} & $\bar{b_1}$ & & $y_{1k}$\\
							& $\bar{b_2}$ & & $y_{2k}$\\
							& ... & & ...\\
							$B^{-1}$ & $\bar{b_r}$ & & $y_{rk}$(pivot at here)\\
							& ... & & ...\\
							& $\bar{b_m}$ & & $y_{mk}$\\
							\cline{1-2} \cline{4-4} 
						\end{tabular}
					\end{align}

				\subsection{Comparison between Simplex and Revised Simplex}
					\subsubsection{Advantage of Revised Simplex}
					- Save storage memory \\
					- Don\rq{}t need to calculate N (including $B^{-1}N$ and $c_BB^{-1}N$) \\
					- More accurate because round up errors will not be accumulated 

					\subsubsection{Disadvantage of Revised Simplex}
						- Need to calculate $wa_j$ for all $j \in N$ (in fact don\rq{}t need to calculated it for the variable just left the basis) 

					\subsubsection{Computation Complexity}
						\begin{align}
							\begin{tabular}{|c|c|c|}
								\hline Method & Type & Operations\\
								\hline Simplex & $\times$ & $(m+1)(n-m+1)$\\
								\cline{2-3} & $+$ & $m(n-m+1)$\\
								\hline Revised Simplex & $\times$ & $(m+1)^2+m(n-m)$ \\
								\cline{2-3} & $+$ & $m(m+1)+m(n-m)$ \\
								\hline
							\end{tabular} 
						\end{align}

					\subsubsection{When to use?}
						- When $m >> n$, do revise simplex method on the dual problem \\
						- When $m \simeq n$, revise simplex method is not as good as simplex method \\
						- When $m << n$ perfect for revise simplex method.

				\subsection{Decomposition of B inverse}
					Let $B=\{ a_{B_1}, a_{B_2}, ..., a_{B_r}, ..., a_{B_m}\}$ and $B^{-1}$ is known.
					If $a_{B_r}$ is replaced by $a_{B_k}$, then $B$ becomes $\bar{B}$. Which means $a_{B_r}$ enters the basis and $a_{B_k}$ leaves the basis. \\
					Then $\bar{B}^{-1}$ can be represent by $B^{-1}$. Noting that $a_k=By_k$ and $a_{B_i}=Be_i$, then
					\begin{align}
						\bar{B} & = (a_{B_1}, a_{B_2}, ...,a_{B_{r-1}}, a_k, a_{B_{r+1}}, a_m)  \\
						& = (Be_1, Be_2, ..., Be_{r-1}, By_k, Be_{r+1}, ..., Be_m)  \\
						& = BT 
					\end{align}
					where $T$ is
					\begin{equation}
						T=\left[ \begin{array}{cccccccc}
							1 & 0 & ... & 0 & y_{1k} & 0 & ... & 0 \\
							0 & 1 & ... & 0 &  y_{2k} & 0 & ... & 0 \\
							\vdots & \vdots & & \vdots & \vdots & \vdots & & \vdots \\
							0 & 0 & ... & 1 & y_{r-1,k} & 0 & ... & 0 \\
							0 & 0 & ... & 0 & y_{rk} & 0 & ... & 0 \\
							0 & 0 & ... & 0 &  y_{r+1,k} & 1 & ... & 0 \\
							\vdots & \vdots & & \vdots & \vdots & \vdots & & \vdots \\
							0 & 0 & ... & 0 &  y_{mk}& 0 & ... & 1 \\
						\end{array} \right] 
					\end{equation}
					and 
					\begin{equation}
						E =  T ^{-1}=\left[ \begin{array}{cccccccc}
							1 & 0 & ... & 0 & \frac{-y_{1k}}{y_{rk}} & 0 & ... & 0 \\
							0 & 1 & ... & 0 & \frac{-y_{2k}}{y_{rk}} & 0 & ... & 0 \\
							\vdots & \vdots & & \vdots & \vdots & \vdots & & \vdots \\
							0 & 0 & ... & 1 & \frac{-y_{r-1,k}}{y_{rk}} & 0 & ... & 0 \\
							0 & 0 & ... & 0 & \frac{1}{y_{rk}} & 0 & ... & 0 \\
							0 & 0 & ... & 0 & \frac{-y_{r+1,k}}{y_{rk}} & 1 & ... & 0 \\
							\vdots & \vdots & & \vdots & \vdots & \vdots & & \vdots \\
							0 & 0 & ... & 0 &  \frac{-y_{mk}}{y_{rk}} & 0 & ... & 1 \\
						\end{array} \right] 
					\end{equation}
					For each iteration, i.e. one variable enters the basis and one leaves the basis, $\bar{B}^{-1}=T^{-1}B^{-1}=EB^{-1}$. Given that the first iteration starts from slack variables, the first basis $B_1$ is $I$, then we have
					\begin{equation}
						B^{-1}_t=E_{t-1} E_{t-2} \cdots E_{2} E_{1} I
					\end{equation}
					Using $E$ in calculation can simplify the product of matrix where
					\begin{align}
						cE &= {c_1,c_2,...,c_m} \left[ \begin{array}{cccccc}
						1 & 0 & ... & g_1 & ... & 0 \\
						0 & 1 & ... & g_2 & ... & 0 \\
						\vdots & \vdots & & \vdots & & \vdots \\
						0 & 0 & ... & g_m & ... & 1 \\
						\end{array} \right]  \\
						&= (c_1, c_2, ... ,c_{r-1}, cg, c_{r+1}, ..., c_m) 
					\end{align}
					and
					\begin{align}
						Ea &=  \left[ \begin{array}{cccccc}
						1 & 0 & ... & g_1 & ... & 0 \\
						0 & 1 & ... & g_2 & ... & 0 \\
						\vdots & \vdots & & \vdots & & \vdots \\
						0 & 0 & ... & g_m & ... & 1 \\
						\end{array} \right]
						 \left[ \begin{array}{c}
						a_1 \\
						a_2 \\
						\vdots \\
						a_m \\
						\end{array} \right]  \\
						&= 
						\left[ \begin{array}{c}
						a_1 \\
						a_2 \\
						\vdots \\
						a_{r-1} \\
						0 \\
						a_{r+1} \\
						\vdots \\
						a_m \\
						\end{array} \right] +
						a_r\left[ \begin{array}{c}
						g_1 \\
						g_2 \\
						\vdots \\
						g_{r-1} \\
						g_r \\
						g_{r+1} \\
						\vdots \\
						g_m \\
						\end{array} \right]  \\
						&=\bar{a}+a_rg 
					\end{align}
					Then we can calculate $w$, $y_k$ and $\bar{b}$
					\begin{align}
						w&=c_BB^{-1} = c_BE_{t-1}E_{t-2}...E_2E_1  \\
						y_k &=B^{-1}a_k = E_{t-1}E_{t-2}...E_2E_1a_k  \\
						\bar{b}&=B^{-1}_{t+1}b=E_tE_{t-1}E_{t-2}...E_2E_1b 
					\end{align}

			\section{Dual Simplex Method}
				Maintain dual feasibility, i.e. primal optimality, and complementary slackness and work towards primal feasibility.
				\framebox{\textbf{Tip:}}The RHS become new $z_j - c_j$, the old $z_j - c_j$ become new RHS. We are actually solving the dual problem.

			\section{Simplex with Equations}

			\section{Simplex with Bounded Variables}
				\subsection{Bounded Variable Formulation}
					\begin{align}
						\min \quad & cx \\
						\text{s.t.} \quad & Ax=b \\
										  & l \le x\le b 
					\end{align}
					Reason why we don't the following formulation
					\begin{align}
						\min \quad & cx \\
						\text{s.t.} \quad & Ax=b \\
						                  & x - Ix_l = l \\
						                  & x + Ix_u = u \\
						                  & x \ge 0\\
						                  & x_l \ge 0\\
						                  & x_u \ge 0 
					\end{align}
					is that this formulation increase the number of variable from $n$ to $3n$, and the number of constraint from $m$ to $m+2n$, the size in increase significantly.

				\subsection{Basic Feasible Solution}
					Consider the system $Ax=b$ and $l\le x\le b$, where $A$ is a $m\times n$ matrix of rank $m$, the solution $\bar{x}$ is a \textbf{basic feasible solution} if $A$ can be partition into $[B, N_l, N_u]$ where the solution $x$ can be partition into $x=(x_B, x_{N_l}, x_{N_u})$, in which $\bar{x}_{N_l} = l_{N_l}$ and $\bar{x}_{N_u} = u_{N_u}$, therefore
					\begin{equation}
						\bar{x}_B = B^{-1}b - B^{-1}N_lx_{N_l} - B^{-1}N_ux_{N_u} 
					\end{equation}
					Furthermore, similar to definition of nonnegative variables, if $l_B \le x_B \le u_B$, $x_B$ is a basic feasible solution, if $l_B < x_B < u_B$, $x_B$ is a non-degenerate basic feasible solution. 

				\subsection{Improving Basic Feasible Solution}
					The basic variables and the objective function can be derived as following:
					\begin{align}
						x_B &= B^{-1}b - B^{-1}N_lx_{N_l} - B^{-1}N_ux_{N_u} \\
						  z &= c_Bx_B + c_{N_l}x_{N_l} + c_{N_u}x_{N_u} \\
						  	&= c_B(B^{-1}b - B^{-1}N_lx_{N_l} - B^{-1}N_ux_{N_u})  \\
						  	& \quad + c_Bx_B + c_{N_l}x_{N_l} + c_{N_u}x_{N_u}\\
						  	&= c_BB^{-1}b + (c_{N_l} - c_BB^{-1}N_l)x_{N_l} \\
						  	& \quad + (c_{N_u} - c_BB^{-1}N_u)x_{N_u} \\
						  	&= c_BB^{-1}b - \sum_{j \in J_1}(z_j - c_j)x_j - \sum_{j \in J_2}(z_j - c_j)x_j
					\end{align}
					$J_1$ is the set of variables at lower bound, $J_2$ is the set of the variables at upper bound.\\
					Notice that the right-hand-side no longer provide $c_BB^{-1}b$ and $B^{-1}b$. For the variable entering the basis, find the variable with
					\begin{equation}
						\max\{\max_{j\in J_1}\{z_j - c_j\}, \max_{j\in J_2}\{c_j - z_j\}\} 
					\end{equation}
					to enter the basis\\
					\framebox{\textbf{Tip:}} "Most violated rule"\\
					The minimum ratio rule is revised for bounded simplex
					\begin{align}
						\Delta &= \min \{\gamma_1, \gamma_2, u_k-l_k\} \\
						\gamma_1 &= \begin{cases}
										\min_{r\in J_1}\{\frac{\bar{b}_r-l_{B_r}}{y_{rk}}:y_{rk} > 0\} \\
										\min_{r\in J_2}\{\frac{\bar{b}_r-l_{B_r}}{-y_{rk}}:y_{rk} < 0\} \\
										\infty
									\end{cases} \\
						\gamma_2 &= \begin{cases}
										\min_{r\in J_1}\{\frac{u_{B_r} - \bar{b}_r}{-y_{rk}}:y_{rk} < 0\} \\
										\min_{r\in J_2}\{\frac{u_{B_r} - \bar{b}_r}{y_{rk}}:y_{rk} > 0\} \\
										\infty
									\end{cases} 
					\end{align}
					\framebox{\textbf{Tip:}}\\
					Use $l \le x+\Delta \le u$ to test the range of $\delta$, if it hits lower bound, it is called $\gamma_1$, if it hits upper bound, it is called $\gamma_2$.		

			\section{Simplex with Unrestricted Variables}

		\chapter{Duality and Sensitivity}
			\section{Duality}
				\subsection{Dual Formulation}
					For any prime problem
					\begin{align}
						\min \quad & cx \\
						\text{s.t.} \quad & Ax\ge b \\
									& x\ge 0 
					\end{align}
					we can have a dual problem
					\begin{align}
						\max \quad & wb  \\
						\text{s.t.} \quad & wA\le c\\
									& w \ge 0
					\end{align}

				\subsection{Mixed Forms of Duality}
					For the following prime problem
					\begin{align}
						\text{P(or D)} \quad \min \quad & c_1x_1 + c_2x_2 + c_3x_3 \\
						\quad \text{s.t.} \quad & A_{11}x_1 + A_{12}x_2 + A_{13}x_3 \ge b_1 \\
												& A_{21}x_1 + A_{22}x_2 + A_{23}x_3 \le b_2 \\
												& A_{31}x_1 + A_{32}x_2 + A_{33}x_3 = b_3 \\
												& x_1 \ge 0 \\
												& x_2 \le 0 \\
												& x_3 \quad \text{unrestricted} 
					\end{align}
					The dual of the problem
					\begin{align}
						\text{D(or P)} \quad \max \quad & w_1b_1 + w_2b_2 + w_3b3 \\
						\quad \text{s.t.} \quad & w_1A_{11} + w_2A_{21} + w_3A_{31} \le c_1 \\
												& w_1A_{12} + w_2A_{22} + w_3A_{32} \ge c_2 \\
												& w_1A_{13} + w_2A_{23} + w_3A_{33} = c_3 \\
												& w_1 \ge 0 \\
												& w_2 \le 0 \\
												& w_3 \quad \text{unrestricted} 
					\end{align}
					In sum, the relation between primal and dual problems are listed as following\\
					\begin{tabular}{|c|c|c|c|c|}
						\hline & Minimization& & Maximization& \\
						\hline & $\geq 0$ & $\longleftrightarrow$ & $\leq 0$ & \\
						Var & $\leq 0$ & $\longleftrightarrow$ & $\geq 0$ & Cons \\
						& Unrestricted & $\longleftrightarrow$ & = & \\
						\hline & $\geq 0$ & $\longleftrightarrow$ & $\geq 0$ & \\
						Cons & $\leq 0$ & $\longleftrightarrow$ & $\leq 0$ & Var \\
						& = & $\longleftrightarrow$ & Unrestricted & \\
						\hline
					\end{tabular} 

				\subsection{Dual of the Dual is the Primal}
					For a primal problem (P)
					\begin{align}
						(P) \quad \min \quad & cx \\
								\text{s.t.} \quad & Ax \ge b \\
												  & x \ge 0 
					\end{align}
					The dual problem (D) is 
					\begin{align}
						(D) \quad \max \quad & wb \\
								\text{s.t.} \quad & wA \le c \\
												  & w \ge 0 
					\end{align}
					Rewrite the dual
					\begin{align}
						\min \quad & -b^\top w^\top  \\
						\text{s.t.} \quad & -A^\top w^\top  \ge -c^\top  \\
										  & w^\top  \ge 0 
					\end{align}
					Find the dual of this problem
					\begin{align}
						\max \quad & x^\top (-c^\top )\\
						\text{s.t.} \quad & x^\top (-A^\top ) \le (-b^\top ) \\
										  & x^\top  \ge 0 \\
					\end{align}
					Rewrite the dual of the dual
					\begin{align}
						(P) \quad \min \quad & cx \\
								\text{s.t.} \quad & Ax \ge b \\
												  & x \ge 0 
					\end{align}

				\subsection{Primal-Dual Relationships}
					\subsubsection{Weak Duality Property}
						Let $x_0$ be any feasible solution of a primal minimization problem,
						\begin{equation}
							Ax_0 \ge b, \quad x_0\ge 0 
						\end{equation}
						Let $x_0$ be any feasible solution of a dual maximization problem,
						\begin{equation}
							w_0A \le c, \quad w_0\ge 0 
						\end{equation}
						Therefore, we have
						\begin{equation}
							cx_0 \ge w_0Ax_0 \ge w_0b 
						\end{equation}
						which is called the weak duality property. This property is for any feasible solution in the primal and dual problem.\\
						Therefore, any feasible solution in the maximization problem gives the lower bound of its dual problem, which is a minimization problem, vice versa. We use this to give the bounds in using linear relaxation to solve IP problem.

					\subsubsection{Fundamental Theorem of Duality}
						With regard to the primal and dual LP problems, one and only one of the following can be true.
						\begin{itemize}
							\item Both primal and dual has optimal solution $x^*$ and $w^*$, where $cx^* = w^*b$
							\item One problem has an unbounded optimal objective value, the other problem must be infeasible
							\item Both problems are infeasible.
						\end{itemize}

					\subsubsection{Strong Duality Property}
						From KKT condition, we know that in order to make $x^*$ the optimal solution, the following condition should be met.
						\begin{itemize}
							\item Primal Optimal: $Ax^* \ge b$, $x^*\ge 0$
							\item Dual Optimal: $w^*A \le c$, $w^*\ge 0$
							\item Complementary Slackness
							\begin{equation}
								\begin{cases}
									w^*(Ax^*-b) = 0\\
									(c-w^*A)x^* = 0
								\end{cases} 
							\end{equation}
						\end{itemize}
						
						The first condition means the primal has an optimal solution, the second condition means the dual has an optimal solution. The third condition means $cx^*=w^*b$, which is also called \textbf{strong duality property}\\
						\notice{$w$ in the dual problem is the same as the $w=c_BB^{-1}$ in primal problem.}

					\subsubsection{Complementary Slackness Theorem}
						Let $\bm{x^*}$ and $\bm{w^*}$ be any feasible solutions, they are optimal iff
						\begin{align}
							(c_j - \bm{w^*}\bm{a_j})x_j^* &= 0, \quad j = 1,...,n \\
							w_i^*(\bm{a^i}\bm{x^*} - b_i) &= 0, \quad i = 1,...,m
						\end{align}
						In particular
						\begin{align}
							x_j^*>0 &\Rightarrow \bm{w^*}\bm{a_j} = c_j  \\
							\bm{w^*}\bm{a_j} < c_j &\Rightarrow x_j^* = 0  \\
							w_i^* >0 &\Rightarrow \bm{a^i}\bm{x^*} = b_i  \\
							\bm{a^i}\bm{x^*} > b_i &\Rightarrow w_i^*=0
						\end{align}
						It means, if in optimal solution a variable is positive (has to be in the basis), the correspond constraint in the other problem is tight. If the constraint in one problem is not tight, the correspond variable in the other problem is zero.

						In the dual problem, we solved some $w$ which is positive, we can know that the correspond constraint in primal is tight, furthermore we can solve the basic variables from those tight constraints, which becomes equality and we can solve it using Gaussian-Elimination.

				\subsection{Shadow Price}
					\subsubsection{Shadow Price under Non-degeneracy}
						Let $B$ be an optimal basis for primal problem and the optimal solution $x^*$ is non-degenerated.
						\begin{equation}
							z=c_BB^{-1}b - \sum_{j\in N}(z_j - c_j)x_j = w^*b - \sum_{j\in N}(z_j - c_j)x_j 
						\end{equation}
						therefore
						\begin{equation}
							\frac{\partial z^*}{\partial b_i} = c_BB^{-1}_i = w_i^* 
						\end{equation}
						$w^*$ is the shadow prices for the right-hand-side vectors. We can also regard it as the \textbf{incremental cost} of producing one more unit of the $i$th product. Or $w^*$ is the \textbf{fair price} we would pay to have an extra unit of the $i$th product.

					\subsubsection{Shadow Price under Degeneracy}
						For shadow price under degeneracy, the $w^*$ may not be the true shadow price, for it may not be the right basis.\\
						In this case, the partial differentiation may not be valid, for component $b_i$, if $x_i = 0$ and $x_i$ is a basic variable, we can't find the differentiation.

			\section{Sensitivity}
				\begin{itemize}
					\item Change in the Cost Vector
					\begin{itemize}
						\item Nonbasic Variable: $c_B$ is not affected, $z_j = c_BB^{-1}a_j$ is not changed, say nonbasic variable cost coefficient $c_k$ changed into $c_k^\prime$. For now $z_k - c_k \le 0$, if $z_k - c_k^\prime$ is positive, $x_k$ must into the basis, the optimal value changed. Otherwise stays at the same.
						\item Basic Variable: If $c_{B_t}$ is replaced by $c_{B_t}^\prime$, then $z_j^\prime -c_j$ is
						\begin{equation}
							z_j^\prime  - c_j = c_B^\prime B^{-1}a_j - c_j = (z_j - c_j) - (c_{B_t}^\prime -c_{B_t})B^{-1}a_{B_t} 
						\end{equation}
						for $j=k$, it is a basic variable, therefore original $z_k - c_k = 0$, $B^{-1}a_k=1$. Hence $z_k^\prime -c_k = c_k^\prime  - c_k \Rightarrow z_k^\prime  - c_k^\prime  = 0$. The basis stays the same. The optimal solution updated as $c_B^\prime B^{-1}b=c_BB^{-1}b + (c_{B_t}^\prime  - c_{B_t})B^{-1}b_{B_t}$.
					\end{itemize}
					\item Change in the Right-Hand-Side: If $b$ is replaced by $b^\prime$, then $B^{-1}b$ is replaced by $B^{-1}b^\prime$. If $B^{-1}b^\prime  \ge 0$, the basis remains optimal. Otherwise, we perform dual simplex method to continue.
					\item Change in the Matrix
					\begin{itemize}
						\item Changes in Activity(Variable) Vectors for Nonbasic Columns: If a nonbasic column $a_j$ is replaced by $a_j^\prime$, then $z_j=c_BB^{-1}a_j$ is replaced by $z_j^\prime =c_BB^{-1}a_j^\prime$, if new $z_j^\prime  - c_j \le 0$, the basis stays optimal basis, the optimal value is the same because $c_B$ stays the same.
						\item Changes in Activity(Variable) Vectors for Basic Columns: If a basic columns changed, it means $B$ and $B^{-1}$ changed, and every column changed. We can do this in two steps:
						\begin{itemize}
							\item step I: add a new column with $a_j^\prime$
							\item step II: remove the original column $a_j$
						\end{itemize}
						If in step I the new variable can enter basis, i.e. $z_j^\prime  - c_j \le 0$, let it enter the basis and eliminate the original column directly (because at this time the original column leave the basis the nonbasic variable is 0); otherwise, if the new column can not form a new basis, treat $x_j$, the original variable as an artificial variable.
						\item Add a New Activity(Variable): Suppose we add a new variable $x_{n+1}$ and $c_{n+1}$ and $a_{n+1}$ respectively. Calculate $z_{n+1} - c_{n+1}$ to determine if the new variable enters the basis, if not, remains the same optimal solution, otherwise, continue on to find a new optimal solution.
						\item Add a New Constraint: This is the basic of Branch-and-Cut/Bound, also, we can perform dual simplex method after we add a new constraint(cut).
					\end{itemize}
				\end{itemize}			

		\chapter{Interior Point Methods}

	\part{Integer Programming}\label{IP}
		\chapter{Branch and Bound}
			\section{LP based Branch and Bound}
				\subsection{Idea of Divide and Conquer}
					For each iteration, divide the feasible region of LP into two feasible parts and an infeasible part, solve the LP in those parts.\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.5]
							\draw [<->] (10, 0) -- (0,0) -- (0,10);
							\draw (0, 8.5) -- (10, 2.5);
							\draw (2, 8) -- (9, 0);
							\draw [dashed] (3,0) -- (3,10);
							\draw [dashed] (4,0) -- (4,10);
							\node at (1.5, 5) {$S_1$};
							\node at (3.5, 4) {$S_2$};
							\node at (5, 3) {$S_3$}; 
						\end{tikzpicture}
						\caption{Divide and Conquer}
					\end{figure}
					In this iteration, the original feasible region have been partition into three parts, where $S_2$ is infeasible for IP because there is not integer point in it. We continue the iteration for $S_1$ and $S_2$. Each partition is suppose to give a new upper bound / lower bound and reduce the infeasible space.\\
					If the temp optimal integer in $S_1$ is larger than the LP relaxation in $S_3$, we can cut $S_3$.\\
					For each iteration, we use dual simple method, for the following two reasons:\\
					\indent - We can process new constraint very fast\\
					\indent - Always gives us a valid bound.\\

				\subsection{Relation Between LP Relaxation and IP}
					Let
					\begin{align}
						Z_{IP} &=\max_{x\in S} cx, \quad \text{where } s \text{ is a set of integer solutions} \\
						Z_{LP} &=\max cx, \quad \text{the LP relaxation of IP}  
					\end{align}
					then
					\begin{align}
						Z_{IP} &= \max_{1\le i \le k} \{\max_{x \in S_i} cx \} \\
						\text{iff} \quad S&=\bigcup_{1\le i \le k} S_i 
					\end{align}
					Notice that $S_i$ don\rq{}t need to be disjointed.\\
					\textbf{Important!} (For maximization problem)\\
					\indent - Any feasible solution provides a lower bound $L$, which is also the \textit{Prime Bound}\\
					\begin{equation}\hat{x}\in S \rightarrow Z_{IP}\ge c\hat{x}\end{equation}
					\indent - After branching, solving the LP relaxation over sub-feasible-region $S_i$ produces an upper bound, which is also the \textit{Dual Bound}, on each sub-problem\\
					\indent - If $u(S_i)\le L$, remove $S_i$\\
					\indent - LP can produce the first upper bound, but there might be possible to find other upper bound with other method (e.g. Lagrangian relaxation)

				\subsection{LP feasibility and IP(or MIP) feasibility}
					Solve the LP relaxation, one of the following things can happen\\
					\indent - LP is infeasible $\rightarrow$ MIP is infeasible\\
					\indent - LP is unbounded $\rightarrow$ MIP is infeasible or unbounded\\
					\indent - LP has optimal solution $\hat{x}$ and $\hat{x}$ are integer ($\hat{x} \in S$), $\rightarrow$ $Z_{IP} = Z_{LP}$\\
					\indent - LP has optimal solution $\hat{x}$ and $\hat{x}$ are not integer ($\hat{x} \notin S$), now defines a new upper bound, $Z_{LP} \ge Z_{IP}$\\
					If the first three happens, stop, if the fourth happens, we branch and recursively solve the sub-problems.

			\section{Terminology in Branch and Bound}
				- If we picture the sub-problems, they will form a \textbf{search tree} (typically a binary tree)\\
				- Each node in the search tree is a \textbf{sub-problem}\\
				- Eliminating a node is called \textbf{pruning}, we also stop considering its children\\
				- A sub-problem that has not being processed is called a \textbf{candidate}, we keep a list of candidates

			\section{Bounding}
			 	\textbf{Notice!} this section is for maximization, if it is for minimization, reverse upper bound and lower bound.
				\subsection{Upper Bound}
					- Upper bound it the Prime bound. which means it has to be a feasible solution\\
					- Some methods to get an upper bound:\\
					\indent - Rounding\\
					\indent - Heuristic\\
					\indent - Meta-heuristic

				\subsection{Lower Bound}
					- Lower Bound is the Dual bound,we can use LP relaxation to get it\\
					- The tighter the better, LP is better\\

			\section{Branch and Bound Algorithm}
				\begin{algorithm}[!ht]
					\caption{Branch and Bound}
					\begin{algorithmic}[1]
						\State find a feasible solution as the initial Lower bound $L$
						\State put the original LP relaxation in candidate list $S$
						\While {$S \ne \emptyset$}
							\State select a problem $\hat{S}$ from $S$
							\State solve the LP relaxation of $\hat{S}$ to obtain $u(\hat{S})$
							\If {LP is infeasible}
								\State $\hat{S}$ pruned by infeasibility
							\ElsIf {LP is unbounded}
								\State $\hat{S}$ pruned by unboundness or infeasibility
							\ElsIf{LP $u(\hat{S}) \le L$}
								\State $\hat{S}$ pruned by bound
							\ElsIf{LP $u(\hat{S}) > L$}
								\If {$\hat{x}\in S$}
									\State $u(\hat{S})$ becomes new $L$, $L=u(\hat{S})$
								\ElsIf {$\hat{x}\notin S$}
									\State branch and add the new sub-problems to $S$
									\If {LP $u(\hat{S})$ is at current best upper bound}
										\State set $U=u(\hat{S})$
									\EndIf
								\EndIf
							\EndIf
						\EndWhile
						\If {Lower bound exists}
							\State find the optimal at $L$
						\Else
							\State Infeasible
						\EndIf
					\end{algorithmic}
				\end{algorithm}

			\section{The goal of Branching}
				- Divide the problem into easier sub-problems\\
				- We want to chose the branching variables that minimize the sum of the solution times of the sub-problems\\
				- If after branching the $u(S_i)$ changes a lot,\\
				\indent - I can find a good L first\\
				\indent - The branch may get worse than the current bound first\\
				- Instead of solving the potential two branches for all candidates to optimality, solve a few iterations of the dual simplex, each iteration of pivoting yields an upper bound.

			\section{Choose Branching Variables}
				\subsection{The Most Violated Integrality constraint}
					Pick the $j$ of which $x_j - \lfloor \hat{x_j} \rfloor$ is closer to 0.5

				\subsection{Strong Branching}
					Select a few candidates $(K)$, create all sub-problems for each of these variables, run a few dual simplex iterations to see the improved bounds, select the variable with the best bounds.\\
					for variable $x_j\in K$, we branch and do a few iterations to find two reductions of gaps, i.e. $D_j^+$ and $D_j^-$,
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\hat{x_j}$};
							\node [left] at (-2.5, 2.5) {$x_j \le \lfloor \hat{x_j} \rfloor$};
							\node [right] at (2.5, 2.5) {$x_j \ge \lceil \hat{x_j} \rceil$};
							\draw [->] (-5, -1) -- (-5, -3);
							\draw [->] (5, -1) -- (5, -3);
							\node [below] at (-5, -3) {$D_j^+$};
							\node [below] at (5, -3) {$D_j^-$};
						\end{tikzpicture}
						\caption{Strong Branching}
					\end{figure}

				\subsection{pseudo-cost Branching}
					Pseudo-cost is an estimate of per-unit change in the objective function, for each variable
					\begin{equation}\begin{cases}P_j^+, & \text{bound reduction if rounded up} \\ P_j^-, & \text{bound reduction if rounded down}\end{cases}\end{equation}
					define $f_j = x_j -\lfloor x_j \rfloor$
					\begin{equation}\begin{cases}D_j^+ = P_j^+ (1-f_j) \\ D_j^- = P_j^- f_j\end{cases}\end{equation}

			\section{Choose the Node to Branch}
				\subsection{Update After Branching}
					For those variables in $K$ find the \\
					- $\max \{\min\{ {D_j^+},  {D_j^-}\}\}$, or\\
					- $\max \{\max\{ {D_j^+},  {D_j^-}\}\}$, or\\
					- $\max \{\frac{D_j^+ + D_j^-}{2}\}$, or\\
					- $\max \{\alpha_1\min\{ {D_j^+},  {D_j^-}\} + \alpha_2\max\{ {D_j^+},  {D_j^-}\}\}$\\
					to branch.

				\subsection{Branch on Important Variables First}
					Branch on variables that affects many decisions.

				\subsection{Some Search Strategy}
					- Best Bound First: select the node with the largest bound (good for closing the gap)\\
					- Deep First: Good for finding Lower bound and easier to do dual simplex\\
					- Mix: Start with \lq\lq{}Deep First\rq\rq{} until we find a good bound and do \lq\lq{}Best Bound First\rq\rq{}

			\section{Types of Branching}
				\subsection{Traditional Branching}
					For $\hat{x} \notin S$, $\exists j \in N$ such that
					\begin{equation}\hat{x_j} -\lfloor\hat{x_j}\rfloor > 0 \end{equation}
					Create two sub-problems
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\hat{x_j}$};
							\node [left] at (-2.5, 2.5) {$x_j \le \lfloor \hat{x_j} \rfloor$};
							\node [right] at (2.5, 2.5) {$x_j \ge \lceil \hat{x_j} \rceil$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

				\subsection{Constraint Branching}
					Use parallel constraints to branch, e.g.
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\hat{x}$};
							\node [left] at (-2.5, 2.5) {$x_1 - x_2 \ge 0$};
							\node [right] at (2.5, 2.5) {$-x_1 + x_2 \ge 1$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

				\subsection{SOS}
					For SOS1,
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\sum_{i=1}^{n}x_i=1$};
							\node [left] at (-2.5, 2.5) {$\sum_{i=1}^{k}x_i=1$};
							\node [right] at (2.5, 2.5) {$\sum_{i=k+1}^{n}x_i=1$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}
					For SOS2 (using the first definition),
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\sum_{i=1}^{n}x_i \le 1$};
							\node [left] at (-2.5, 2.5) {$\sum_{i=1}^{k-1}x_i = 0$};
							\node [right] at (2.5, 2.5) {$\sum_{i=k+1}^{n}x_i =  0$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

				\subsection{GUB}
					This is where $x_i \in \{0, 1\}$, at most one variable can be 1,
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [above] at (0, 5.5) {$\sum_{i=1}^{n}x_i\le 1$};
							\node [left] at (-2.5, 2.5) {$\sum_{i=1}^{k}x_i=0$};
							\node [right] at (2.5, 2.5) {$\sum_{i=k+1}^{n}x_i=0$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

				\subsection{Ryan-Foster}
					Ryan-Foster is for Set covering problem. The typical model is
					\begin{align}
						\min \quad & \sum_{i \in C} x_i \\
						\text{s.t.} \quad & \sum_{i \in C} a_{ij}x_{i} \ge 1, \quad \forall j \in U \\
								& x_i \in \{0, 1\}, \quad \forall i \in C 
					\end{align}
					\textbf{Observation} For any fractional solution, there are at least two elements $(i,j)$ so that $i$ and $j$ are both partially covered by the same set $S$, but there is another set $T$ that only covers $i$
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.2]
							\draw (0, 5) circle [radius=0.5];
							\draw (-5, 0) circle [radius=0.5];
							\draw (5, 0) circle [radius=0.5];
							\draw (0.353, 4.647) -- (4.647, 0.353);
							\draw (-0.353, 4.647) -- (-4.647, 0.353);
							\node [left] at (-7, 2.5) {$(i,j)$ are};
							\node [left] at (-5, 0.5) { in the same set};
							\node [left] at (-4, 4.5) {if $a_{ik}a_{jk}=0 \Rightarrow x_k = 0$};
							\node [right] at (7, 2.5) {$(i,j)$ are};
							\node [right] at (5, 0.5) { in the different set};
							\node [right] at (3, 4.5) {if $a_{ik}=a_{jk}=1 \Rightarrow x_k=0$};
						\end{tikzpicture}
						\caption{Traditional Branching}
					\end{figure}

		\chapter{Branch and Cut}
			\section{Separation Algorithm}
				Basic idea is to separate the feasible region so that the current "solution" (which is an fractional solution) is not included in the feasible region.
				\subsection{Vertices Packing}
					The current solution is $\bar{x} \in [0,1]^n$, we have two options to do the separation:\\
					\textbf{Option 1 - find the maximum clique:}\\
					(This approach is as hard as the original problem)\\
					denote
					\begin{equation}
						y_i=\begin{cases}1, \text{ if } i \in C \\ 0, \text{ otherwise}\end{cases}
					\end{equation}
					Find the maximum clique via:
					\begin{align}
						\max \quad &\sum \bar{x_i} y_i  \\
						\text{s.t.} \quad & y_i + y_j \le 1, \forall \{i, j\} \notin E 
					\end{align}
					\textbf{Option 2 - Heuristic:}
					\begin{algorithm}[!ht]
						\caption{Heuristic method to find a clique}
						\begin{algorithmic}[1]
							\State find $v=\text{argmax}_{i\in V} \{\bar{x_i}\}, C=\{v\}$
							\While {$u\in \text{argmax}_{i \in \cap_{i \notin C}N_{(i, j)}\notin C} \{\bar {x_1}\}$ exists}
								\State C.add(u)
							\EndWhile
							\State return C
						\end{algorithmic}
					\end{algorithm}\\
					If $\sum_{i\in C} \bar{x_i} > 1$ then add cut $\sum_{i\in C} x_i \le 1$

				\subsection{TSP}
					When we have a solution, i.e. $\bar{x}$, perform the sub-tour searching algorithm, if there exists any sub-tour, add the corresponded constraint. That is the separation.

			\section{Optional v.s. Essential Inequalities}
				\subsection{Valid (Optional) Inequalities}
					See Figure \ref{OptInq}
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 2 cm]
							\node (LR) [circleNode] {LR};
							\node (CG) [process, below of=LR] {Cut Generation};
							\node (CLimit) [decision, below of=CG] {Generated?};
							\node (Cont) [process, below of=CG, xshift=4 cm] {Continue};
							\node (NLR) [circleNode, below of=CLimit] {New LR};
							\node (LNLR) [circleNode, below of=NLR, xshift=-2 cm] {Branch 1};
							\node (RNLR) [circleNode, below of=NLR, xshift=2 cm] {Branch 2};
							\draw [arrow] (LR) -- (CG);
							\draw [arrow] (CG) -- (CLimit);
							\draw [arrow] (CLimit) -- node [right] {yes} (NLR);
							\draw [arrow] (CLimit) -- node [below] {no} (Cont);
							\draw [arrow] (Cont) |- (CG);
							\draw [arrow] (NLR) -- (LNLR);
							\draw [arrow] (NLR) -- (RNLR);
						\end{tikzpicture}
						\caption {Branch and Cut for Optional Inequality}\label{OptInq}
					\end{figure}

				\subsection{Essential Inequalities (Lazy Cuts)}
					See Figure \ref{EssInq}
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1.8 cm]
							\node (IS) [circleNode, label = above:integer solution] {IS};
							\node (CG) [process, below of=IS] {Cut Generation};
							\node (CLimit) [decision, below of=CG] {Generated?};
							\node (FI) [circleNode, below of=CLimit, label = below:feasible integer solution] {FI};
							\node (LP) [process, below of=CG, xshift = 3.5 cm] {Solve Linear Relaxation};
							\node (LPF) [decision, below of=LP] {Feasible?};
							\node (NLR) [circleNode, below of=LPF] {New LR};
							\node (LNLR) [circleNode, below of=NLR, xshift=-2 cm] {Branch 1};
							\node (RNLR) [circleNode, below of=NLR, xshift=2 cm] {Branch 2};
							\node (Cont) [process, below of=LP, xshift=2.5 cm] {Continue};
							\draw [arrow] (IS) -- (CG);
							\draw [arrow] (CG) -- (CLimit);
							\draw [arrow] (CLimit) -- node [right] {no} (FI);
							\draw [arrow] (CLimit) -- node [below] {yes} (LP);
							\draw [arrow] (LP) -- (LPF);
							\draw [arrow] (LPF) -- node [right] {no} (NLR);
							\draw [arrow] (LPF) -- node [below] {yes} (Cont);
							\draw [arrow] (NLR) -- (LNLR);
							\draw [arrow] (NLR) -- (RNLR);
							\draw [arrow] (Cont) |- (CG);
						\end{tikzpicture}
						\caption {Branch and Cut for Essential Inequality}\label{EssInq}
					\end{figure}

			\section{Chvatal-Gomory Cut}
				\subsection{Chvatal-Gomory Rounding Procedure}
					For $x=P\cap \mathbb{Z}_+^n$, where $P=\{x\in \mathbb{R}_+^n|Ax \le b\}$, A is an mxn matrix with columns $\{a_1, ..., a_n\}$ and $u \in \mathbb{R}_+^n$\\
					- The inequality
					\begin{equation}
						\sum_{j=1}^n ua_jx_j\le ub 
					\end{equation}
					is valid\\
					- Therefore the inequality
					\begin{equation}
						\sum_{j=1}^n \lfloor ua_j \rfloor x_j \le ub 
					\end{equation}
					is valid\\
					- Furthermore, the inequality
					\begin{equation}
						\sum_{j=1}^n \lfloor ua_j \rfloor x_j \le \lfloor ub \rfloor 
					\end{equation}
					is valid.

				\subsection{Gomory Cutting Plane}
					For a IP problem
					\begin{align}
						\max \quad & cx  \\
						\text{s.t.} \quad & Ax=b  \\
							& x \in \mathbb{B}^n 
					\end{align}
					let $\bar{x}$ be an optimal basic solution for the LR of P.
					\begin{equation}
						\bar{x} = \left[\begin{matrix} B^{-1}b \\ 0 \end{matrix}\right] = \left[ \begin{matrix}x_B \\ x_N\end{matrix}\right] 
					\end{equation}
					We have
					\begin{align}
						& Bx_B + Nx_N = b \\
						\Rightarrow \quad & x_B + B^{-1}Nx_N=B^{-1}b  \\
						\Rightarrow \quad & x_B + [\bar{a}_1, \bar{a}_2, ...]x_N = \bar{b} \\
						\Rightarrow \quad & x_i + \sum_{j\in NB} \bar{a}_{ij}x_j = \bar{b}_i \quad \text{(for the $i$th row)} 
					\end{align}
					Assume that $x_i \in \{0, 1\}$, use CG-Procedure
					\begin{equation}
						x_i + \sum_{j \in NB} \lfloor \bar{a}_{ij} \rfloor x_j \le \lfloor \bar{b}_i \rfloor 
					\end{equation}
					is a valid constraint for $P$, furthermore,
					\begin{equation}
						(\bar{b}_i - \sum_{j\in NB} \bar{a}_{ij}x_j) + \sum_{j\in NB}\lfloor \bar{a}_{ij} \rfloor x_j\le \lfloor \bar{b}_i \rfloor 
					\end{equation}
					Move the item, we get a new Gomory Cutting Plane
					\begin{equation}
						\sum_{j\in NB} (\bar{a}_{ij} - \lfloor \bar{a}_{ij} \rfloor)x_j \ge \bar{b}_i - \lfloor \bar{b}_i \rfloor  
					\end{equation}
					Add this inequality to the LR, use the dual simplex method to do one pivot, we get a new solution. Use Gomory cutting plane iteratively and we can find the optimal solution for IP.

		\chapter{Packing and Matching}
			\section{Vertices Packing and Matching Formulation}
				Given a graph $G=(V, E)$, with $|V|=n$. A vertices packing solution is that no two neighboring vertices can be chosen at the same time.
				\begin{equation}
					PACK(G) = \{x\in \mathbb{B}^n|x_i + x_j \le 1, \forall (i, j)\in E\} 
				\end{equation}
				\begin{example}
					The following is an example:\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1.8 cm]
							\node (1) [circleNode] {1};
							\node (2) [circleNode, below of=1] {2};
							\node (3) [circleNode, below of=2, xshift=-2 cm] {3};
							\node (4) [circleNode, below of=2, xshift=2 cm] {4};
							\draw (1) -- (2);
							\draw (1) -- (3);
							\draw (1) -- (4);
							\draw (2) -- (4);
							\draw (3) -- (4);
						\end{tikzpicture}
						\caption{Example of vertices packing problem}
					\end{figure}
					The PACK of this graph is\\
					\begin{equation}
						PACK = conv\left(
							\left(\begin{matrix}0 \\ 0 \\ 0 \\ 0\end{matrix}\right),
							\left(\begin{matrix}1 \\ 0 \\ 0 \\ 0\end{matrix}\right),
							\left(\begin{matrix}0 \\ 1 \\ 0 \\ 0\end{matrix}\right),
							\left(\begin{matrix}0 \\ 0 \\ 1 \\ 0\end{matrix}\right),
							\left(\begin{matrix}0 \\ 0 \\ 0 \\ 1\end{matrix}\right),
							\left(\begin{matrix}0 \\ 1 \\ 1 \\ 0\end{matrix}\right)
							\right)
					\end{equation}
				\end{example}

				Given a graph $G=(V, E)$, denote $\delta(i)$ as the set of all the edges introduced to vertice $i\in V$. A matching solution is that no two edges introduced to the same vertice can be chosen at the same time.
				\begin{equation}
					MATCH(G) = \{\sum_{e\in \delta(i)}x_e \le 1|i\in V\}
				\end{equation}

			\section{Dimension of PACK(G)}
				The dimension of PACK, i.e. $dim(PACK(G))$ is (full-dimensional)
				\begin{equation}
					dim(PACK(G)) = |V| 
				\end{equation}
				To prove that $dim(PACK(G)) = |V|$, we need to find $|V| + 1$ affinely independent vectors.\\
				\begin{proof}
					\begin{equation}
						rank\left(\left[\begin{matrix}0 & I_{|V|} \\ 1 & 1\end{matrix}\right]\right) = |V| + 1 
					\end{equation}
					Therefore, in PACK, $rank(A^=,b^=)=0$ 
				\end{proof}

			\section{Clique}
				- A \textbf{clique} is a subset of a graph that in the clique every two vertices linked with each other (complete sub-graph).
				- A \textbf{maximum clique} is a clique that any other vertice can not form a clique with all the points in this clique.

			\section{Inequalities and Facets of conv(VP)}
				\framebox{\textbf{Example:}}\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1.2 cm]
							\node (1) [circleNode] {1};
							\node (2) [circleNode, below of=1, xshift=-2 cm] {2};
							\node (3) [circleNode, below of=1, xshift=2 cm] {3};
							\node (4) [circleNode, below of=2, xshift=2 cm] {4};
							\node (5) [circleNode, below of=4, xshift=-0.9 cm] {5};
							\node (6) [circleNode, below of=4, xshift=0.9 cm] {6};
							\draw (1) -- (2);
							\draw (1) -- (3);
							\draw (1) -- (4);
							\draw (3) -- (4);
							\draw (2) -- (5);
							\draw (5) -- (6);
							\draw (6) -- (3);
						\end{tikzpicture}
						\caption{Example}
					\end{figure}

				\subsection{Type 1 (Nonnegative Constraints)}
					$x_i \ge 0$ induce facets.\\
					\framebox{\textbf{Proof:}}
					\begin{equation}
						rank\left(\left[\begin{matrix}0 & 0 \\ 0 & I_{|V|}\end{matrix}\right]\right) = |V| + 1 
					\end{equation}

				\subsection{Type 2 (Neighborhood Constraints)}
					$x_i + x_j \le 1$ is a valid constraint, but it \textbf{DOES NOT} always induce facet.

				\subsection{Type 3 (Odd Hole)}
					$H$ is an odd hole if it contains circle of $k$ nodes, such that $k$ is odd and there is no cords. e.g. \{1, 2, 5, 6, 3\}. Then, the following inequality is valid,
					\begin{equation}
						\sum_{i\in H}x_i\le \frac{|H|-1}2 
					\end{equation}
					Odd Hole inequality \textbf{DOES NOT} always induce facets.\\
					This inequality can be derived from Gomory cut.

				\subsection{Type 4 (Maximum Clique)}
					$C$ is a maximum clique, then the following inequality is valid and induce a facet,
					\begin{equation}
						\sum_{i\in C} x_i \le 1 
					\end{equation}
					\framebox{\textbf{Proof:}}\\
						First, if $C=V$
						\begin{equation}
							rank\left(\left[I\right]\right) = |C| = |V| 
						\end{equation}
						Second, if $C$ is a subset of $V$, for each vertice in $V \setminus C$, there should be at least one vertice in $C$ that is not linked with it. Therefore for each vertice in $C$ we can find a packing.

			\section{Gomory Cut in Set Covering}
				Consider a graph $G=(V, E)$, the covering problem is
				\begin{equation}
					\sum_{e\in \delta(i)}x_e \le 1, i\in V, x_e\in \{0, 1\}, e\in E
				\end{equation}
				For $T\subset V$, denote $\delta(i)$ as all edges induce to $i\in V$, denote $E(T) \subset E$ as all the edges linked between $(i, j), i\in T, j\in T$, therefore we have
				\begin{equation}
					\sum_{i\in T}\sum_{e\in \delta(i)}x_e \le |T| 
				\end{equation}
				For edges linking $i \in T, j \in T$, count them twice, for edges linking $i\in T, j\notin T$, count them once.We can have a new constraint
				\begin{equation}
					2\sum_{e\in E(T)}x_e + \sum_{e\in \delta(V\setminus T, T)}x_e \le |T| 
				\end{equation}
				Perform the Gomory Cut, the following constraint is a valid:
				\begin{equation}
					\sum_{e\in E(T)}x_e \le \lfloor \frac{|T|}2 \rfloor 
				\end{equation}

		\chapter{Knapsack Problem}
			\section{Knapsack Problem Formulation}
				Consider the knapsack set KNAP
				\begin{equation}conv(KNAP)= conv(\{x\in \mathbb{B}^n|\sum_{j\in N}a_jx_j\le b\})\end{equation}
				in where\\
				- $N = \{1, 2, ..., n\}$\\
				- With out lost of generality, assume that $a_j > 0, \forall j \in N$ and $a_j < b, \forall j \in N$

			\section{Valid Inequalities for a Relaxation}
				For $P=\{x\in \mathbb{B}^n | Ax\le b\}$, each row can be regard as a Knapsack problem, i.e. for row $i$
				\begin{equation}
					P_i = \{x\in \mathbb{B}^n | a_i^T x \le b_i\} 
				\end{equation}
				is a relaxation of $P$, therefore,
				\begin{equation}
					P\subseteq P_i, \forall i=1,2,...,m 
				\end{equation}
				\begin{equation}
					P\subseteq \cap_{i=1}^m P_i 
				\end{equation}
				So any inequality valid for a relaxation of an IP is also valid for IP itself.

			\section{Cover and Extended Cover}
				A set $C\subseteq N$ is a cover if $\sum_{j\in C} a_j > b$, a cover $C$ is minimal cover if
				\begin{equation}
					C\subseteq N | \sum_{j\in C}a_j>b, \sum_{j\in C\setminus k} a_j < b, \forall k \in C 
				\end{equation}
				For a cover $C$, we can have the cover inequality
				\begin{equation}
					\sum_{j\in C}x_j \le |C|-1
				\end{equation}
				The inequality is trivial considering the pigeonhole principle.\\
				$C\subseteq N$ is a minimal cover, then $E(C)$ is defined as following:
				\begin{equation}
					E(C) = C\cup \{j \in N | a_j \ge a_i, \forall i \in C\}
				\end{equation}
				is called an extended cover. Then we have,
				\begin{equation}
					\sum_{i\in E(C)} x_i \le |C| - 1 \text{ dominates } \sum_{i\in C} x_i \le |C| - 1
				\end{equation}
				and
				\begin{equation}
					\sum_{i\in E(C)} x_i \le |C| - 1 \text{ dominates } \sum_{i\in E(C)} x_i \le |E(C)| - 1
				\end{equation}
				Hereby we need to prove that $\sum_{i\in E(C)} x_i \le |C| - 1$ is valid, by contradiction.\\
				\framebox{\textbf{Proof:???}} Suppose $x^R \in KNAP$, $R$ is a feasible solution, Where
				\begin{equation}
					x^R_j = \begin{cases}1, \quad \text{if $j\in R$} \\ 0, \quad \text{otherwise}\end{cases} 
				\end{equation}
				Then
				\begin{equation}
					\sum_{j\in E(C)}x^R_j \ge |C| \Rightarrow |R \cap E(C)| \ge |C|  
				\end{equation}
				therefore
				\begin{equation}
					\sum_{j\in R}a_j \ge \sum_{j\in R \cap E(C)} a_j \ge \sum_{j\in C} a_j > b 
				\end{equation}
				which means $R$ is a cover, it is contradict to $\sum_{j\in E(C)}x^R_j \ge |C|$ so $x^R \notin KNAP$

			\section{Dimension of KNAP}
				$conv(KNAP)$ is full dimension, i.e. $dim(conv(KNAP))=n$.\\
				\framebox{\textbf{Proof:}} $0, e_j, \forall j\in N$ are $n + 1$ affinely independent points in $conv(KNAP)$\\

			\section{Inequalities and Facets of conv(KNAP)}
				\subsection{Type 1 (Lower Bound and Upper Bound Constraints):}
					- $x_k\ge 0$ is a facet of $conv(KNAP)$\\
					\framebox{\textbf{Proof:}} $0, e_j, \forall j\in N\setminus k$ are $n$ affinely independent points that satisfied $x_k=0$\\
					- $x_k\le 1$ is a facet iff $a_j + a_k \le b, \forall j\in N \setminus k$\\
					\framebox{\textbf{Proof:}} $e_k, e_j+e_k, \forall j \in N\setminus k$ are $n$ affinely independent points that satisfied $x_k = 1$

				\subsection{Type 2 (Extended Cover)}
					Order the variables so that $a_1 \ge a_2 \ge \dots \ge a_n$, therefore $a_1 = a_{max}$\\
					Let $C$ be a cover with $\{j_1, j_2, \dots, j_r\}$ where $j_1 < j_2 < \dots < j_r$ so that $a_{j_1} \ge a_{j_2} \ge \dots \ge a_{j_r}$\\
					Let $p = \min\{j | j\in N \setminus E(C)\}$\\
					Then
					\begin{equation}
						\sum_{j\in E(C)} x_j \le |C| - 1 
					\end{equation}
					is a facet of $conv(KNAP)$ if\\
					- $C = N$\\
					\framebox{\textbf{Proof:}}
					\begin{equation}
						R_k = C\setminus k, \forall k \in C = N \setminus k, \forall k \in N 
					\end{equation}
					have $|N|$ affinely independent vectors\\
					- $E(C) = N$ and $\sum_{j\in C \setminus \{j_1, j_2\}} a_j + a_{max} \le b$\\
					\framebox{\textbf{Proof:}} ($j_1, j_2$ are two heaviest elements in $C$)
					\begin{equation}
						S_k = C\setminus \{j_1, j_2\}\cup \{k\}, \forall k\in E(C)\setminus C 
					\end{equation}
					$R_k\cup S_k$ have $|C|+|E(C) \setminus C| = |E(C)| = |N|$ affinely independent vectors\\
					- $C = E(C)$ and $\sum_{j\in C \setminus j_1} a_j + a_p \le b$)\\
					\framebox{\textbf{Proof:}} ($j_1$ is the heaviest element in $C$, $k$ is the lightest element outside extended cover)
					\begin{equation}
						T_k = C \setminus j_i \cup \{k\}, \forall k\in N\setminus E(C) 
					\end{equation}
					$R_k \cup T_k$ have $|N \setminus E(C)| + |E(C)| = |N\setminus C| + |C| = |N|$ affinely independent vectors\\
					- $C \subset E(C) \subset N$ and $\sum_{j\in C \setminus \{j_1, j_2\}} a_j + a_{max} \le b$ and $\sum_{j\in C \setminus j_1} a_j + a_p \le b$\\
					\framebox{\textbf{Proof:}}$S_k \cup T_k$ have $|E(C) \setminus C| + |N \setminus E(C)| = |N|$ affinely independent vectors

			\section{Lifting}
				\subsection{Up Lifting}
					For KNAP problem
					\begin{equation}
						KNAP = \{x\in \mathbb{B}^n | \sum_j a_jx_j \le b\} 
					\end{equation}
					For $P=conv(KNAP)$ denote\\
					\begin{align}
						&P_{k_1, k_2, ..., k_m} \\
						&\quad =conv(KNAP\cap \{x\in \mathbb{B}^{n}|x_{k_1}=x_{k_2}=\dots=x_{k_m}=0\}) 
					\end{align}
					Therefore
					\begin{align}
						&P_{k_1, k_2, ..., k_m} \\
						&\quad=conv(KNAP\cap \{x\in \mathbb{B}^{n}|\sum_{j\in N\setminus \{k_1, k_2, ..., k_m\}} a_j x_j \le b\}) 
					\end{align}
					The $C=N$ cover inequality for $P_{k_1, k_2, ..., k_m}$ implies
					\begin{equation}
						\sum_{j\in N\setminus \{k_1, k_2, ..., k_m\}} x_j \le n-m-1 
					\end{equation}
					is a facet of $P_{k_1, k_2, ..., k_m}$\\
					The lifting process is to find a facet for $P_{k_1, k_2, ..., k_{m-1}}$ using facet of $P_{k_1, k_2, ..., k_m}$, i.e. find $\alpha_{m}$ for the following constraint to be a facet.
					\begin{equation}
						\alpha_m x_m + \sum_{j\in N\setminus \{k_1, k_2, ..., k_m\}} x_j \le n-m-1 
					\end{equation}
					If $x_m=0$, $\alpha_m \ge 0$,\\
					If $x_m=1$, $\alpha_m \le (n-m+1) - \gamma$ where
					\begin{align}
						\gamma &= \max\{\sum_{j\in N\setminus\{k_1, k_2, ..., k_m\}}x_j|x\in P_{k_1, k_2, ..., k_{m-1}}, x_m=1\} \\
						&= \max\{\sum_{j\in N\setminus\{k_1, k_2, ..., k_m\}}x_j|\sum_{j \in N \setminus \{k_1, k_2, ..., k_m\}}a_jx_j\le b-a_m\} 
					\end{align}
					Then let $\alpha_m = n-m+1-\gamma$, we uplifted a constraint. Repeat this procedure for $\{k_1, k_2, ..., k_m\}$ and finally we can find a family of facets for $conv(KNAP)$


				\subsection{Down Lifting}
					Similar to up lifting, we can perform the lifting in a different way.\\
					Denote
					\begin{align}
						&P_{k_1, k_2, ..., k_m}^\prime  \\
						&\quad =conv(KNAP\cap \{x\in \mathbb{B}^{n}|x_{k_1}=x_{k_2}=\dots=x_{k_m}=1\}) 
					\end{align}

			\section{Separation of a Cover Inequality}
				$C\in N$ is a cover if $\sum_{i\in C} a_i > b$, let $C$ be a minimal cover
				\begin{align}
					&\sum_{i\in C}x_i \le |C| - 1 \\
					\Rightarrow \quad & |C| - \sum_{i\in C}x_i = \sum_{i \in C}(1-x_i)\ge 1 \\
				\end{align}
				let $\bar{x}$ be a fractional solution of $\{\sum_{i\in N} a_ix_i \le b, x_i \in [0, 1], i\in N\}$, find a cover $C$ of which $\sum_{i\in C}(1-\bar{x_i})<1$\\
				Decision variable:
				\begin{equation}
					y_i = \begin{cases}1, \quad \text{if } i\in C\\ 0, \quad \text{otherwise}\end{cases}
				\end{equation}
				\begin{align}
					\min \quad & \sum_{i\in N} (1-\bar{x_i})y_i = z \\
					\text{s.t.} \quad & \sum_{i\in N}a_iy_i \ge b+1 \\
					&y_i \in \{0, 1\}, i\in N
				\end{align}
				if $z<1$, then the cover cut associated with $y$ is violation by $\bar{x}$

		\chapter{Network Flow Problem}
			\section{Shortest Path Problem}
				A graph $G=(A, N)$ is a directed graph\\
				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 1.8 cm]
						\node (1) [circleNode, label=above:start] {1};
						\node (2) [circleNode, right of=1, yshift=1 cm] {2};
						\node (3) [circleNode, right of=1, yshift=-1 cm] {3};
						\node (4) [circleNode, right of=2] {4};
						\node (5) [circleNode, right of=3] {5};
						\node (6) [circleNode, label=above:end, right of=4, yshift=-1 cm] {6};
						\draw [arrow] (1) -- (2);
						\draw [arrow] (1) -- (3);
						\draw [arrow] (2) -- (3);
						\draw [arrow] (2) -- (4);
						\draw [arrow] (3) -- (5);
						\draw [arrow] (4) -- (6);
						\draw [arrow] (5) -- (6);
						\draw [arrow] (5) -- (4);
						\draw [arrow] (2) -- (5);
					\end{tikzpicture}
					\caption{Example of directed graph}
				\end{figure}
				Denote:\\
				\begin{equation}
					x_{ij} = \begin{cases}1, &\text{if goes from } i \text{ to } j\\ 0, & \text{otherwise}\end{cases}
				\end{equation}
				The shortest path problem can be formulated as the following:\\
				\begin{align}
					\min &\sum_{(i, j)\in A} c_{ij}x_{ij} \\
					& \sum_{i \in N\setminus(\{S\}\cup\{E\}), (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 0 \\
					& \sum_{i=\{S\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 1 \\
					& \sum_{i=\{E\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = -1 \\
					& x_{ij} \in [0,1], (i,j)\in A 
				\end{align}
				Although initially $x_{ij} \in [0,1]$, in the optimized solution, $x\in \{0, 1\}$.

			\section{Maximum Flow Problem}
				\begin{align}
					\min &\sum_{(i, j)\in A} F \\
					& \sum_{i \in N\setminus(\{S\}\cup\{E\}), (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 0 \\
					& \sum_{i=\{S\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = F \\
					& \sum_{i=\{E\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = -F \\
					& l_{ij} \le x_{ij} \le u_{ij}, (i,j)\in A 
				\end{align}
				In where $F$ means the flow from source to target.

			\section{Minimum Cost Flow}
				The shortest path problem is a special case of Minimum Cost Flow Problem, which can be formulated as the following:\\
				\begin{align}
					\min &\sum_{(i, j)\in A} c_{ij}x_{ij} \\
					& \sum_{i \in N\setminus(\{S\}\cup\{E\}), (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 0 \\
					& \sum_{i=\{S\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = 1 \\
					& \sum_{i=\{E\}, (i,j)\in A} x_{ij} - \sum_{j \in N, (i,j)\in A} x_{ji} = -1 \\
					& x_{ij} \in [0,1], (i,j)\in A 
				\end{align}

			\section{Unimodularity}
				\subsection{Unimodular Matrix and Total Unimodular Matrix}
					- A unimodular matrix $M$ is a squared matrix, where $det(M)=1$ or $-1$.\\
					- Total unimodular matrix is a matrix where all its sub-matrices are unimodular matrix.

				\subsection{Importance of Unimodular Matrix}
					Let $M_{m\times m}$ be a unimodular matrix, if $b\in \mathbb{Z}^m$, the solution for $Mx=b$ is always integer.\\
					\framebox{\textbf{Proof:}} By Cramer's Rule
					\begin{equation}
						x_i = \frac{det{M_i}}{det{M}} 
					\end{equation}
					in which $M_i$ is matrix $M$ replace $i$th column with $b$. Therefore $det(M_i)$ is integer. Also, $det(M)\ne 0$, so $det(M)=1$ or $det(M)=-1$. Proved.

				\subsection{Structures of Total Unimodular Matrix}
					\textbf{Structure 1:}
						Matrix $M$ that has only 1, -1, 0 enters and each column has at most 2 non-zeros is a TU matrix if it satisfies the following conditions:\\
						We can split the rows in to tops and bottoms, such that for all columns $j$ having 2 non-zeros\\
						- If the non-zeros have the same sign, then one value should be in top and the other should be in bottom\\
						- If the non-zeros have the different sign, then both of them should be in top or both of them should be in bottom\\
					\textbf{Structure 2:}
						If all the columns in matrix $M$ has only 0 or consecutive 1s (or -1s), matrix $M$ is a TU matrix

				\subsection{Construct a New Unimodular Matrix}
					Let $F$ be a unimodular matrix, then
					\begin{equation}
						\left[\begin{matrix}F \\ I\end{matrix}\right] 
					\end{equation}
					is a unimodular matrix, also
					\begin{equation}
						\left[\begin{matrix}F & 0 \\ I & I\end{matrix}\right] 
					\end{equation}
					is a unimodular matrix.

		\chapter{Duality and Relaxation}


			\section{Relaxation}
				\subsection{Why Rounding Can be Bad - IP Example}
					Rounding can be bad because the optimal of IP can be far away from optimal of LP. For example,
					\begin{align}
						\max \quad & z=x_1 +0.64x_2  \\
						\text{s.t.} \quad & 50x_1 +31x_2 \le 250  \\
									& 3x_1-2x_2\ge -4  \\
									& x_1,x_2\ge 0 \quad \text{(for LP)}\\
									& x_1,x_2 \in Z^+ \quad \text{(for IP)} 
					\end{align}
					\begin{figure}[h!]
						\centering
						\begin{tikzpicture}[scale=0.7]
							\draw [->] (1,1) -- (1, 7);
							\draw [->] (1,1) -- (7, 1);
							\draw (1,3) -- (2.948, 5.922);
							\draw (2.948, 5.922) -- (6, 1);
							\draw [->] (1,1) -- (2, 1.64);
							\node [right] at (2, 1.64) {z};
							\node [above] at (2.984, 5.922) {(1.948, 4.922)};
							\node [above] at (2.984, 6.422) {Optimal for LP};
							\node [below] at (6,1) {(5, 0)};
							\node [below] at (6, 0.5) {Optimal for IP};
						\end{tikzpicture}
						\caption{Optimal solution for LP / IP}
					\end{figure}

				\subsection{Why Rounding Can be Bad - QAP example}
					Rounding can make the LP useless. For example, for QAP problem, the IP model is
					\begin{align}
						\min \quad z &= \sum_{i\in D} \sum_{s\in O} c_is x_is + \sum_{i\in D} \sum_{j \in D} \sum_{s \in O} \sum_{t\in O} w_{ij}^{st}y_{ij}^{st}  \\
						\text{s.t.} \quad & \sum_{i \in D} x_{is} =1, \quad s\in D  \\
									&\sum_{s \in O} x_{is} = 1, \quad i \in D  \\
									&x_{is} \in \{0, 1\}, \quad i \in D, s\in O  \\
									& y_{ij}^{st} \ge x_{is} + x_{jt} - 1, \quad i\in D, j\in D, s\in O, t \in O  \\
									& y_{ij}^{st} \ge 0, \quad i\in D, j\in D, s\in O, t \in O  \\
									& y_{ij}^{st} \le x_{is}, \quad i\in D, j\in D, s\in O, t \in O  \\
									& y_{ij}^{st} \le x_{jt}, \quad i\in D, j\in D, s\in O, t \in O  
					\end{align}
					We can get the optimal solution for LP supposing $\forall i, s \quad x_{is}\in [0, 1]$
					\begin{align}
						x_{is} &= \frac{1}{|D|}, \quad i \in D, s\in O   \\
						y_{ij}^{st} & = 0, \quad i\in D, j\in D, s\in O, t \in O 
					\end{align}

				\subsection{IP and Convex Hull}
					For IP problem
					\begin{align}
						Z_{IP} \quad \max \quad &z = cx  \\
								\text{s.t.} \quad &Ax \le b \\
										&x\in {Z^n} 
					\end{align}
					In feasible region $S = \{x\in Z^n, Ax\le b\}$ , the optimal solution $Z_{IP} = \max\{cx: x\in S\}$.\\
					Denote $conv(S)$ as the convex hull of $S$ then\\
					\begin{equation}Z_{IP}(S) = Z_{IP}(conv(S))  \end{equation}

				\subsection{Local Optimal and Global Optimal}
					Let 
					\begin{align}
						Z_s &= \min \{f(x):x\in S\} \\
						Z_t &= \min \{f(x):x\in T\}  \\
						& S \subset T 
					\end{align}
					then\\
					\begin{equation}Z_t \le Z_s  \end{equation}
					\textbf{Notice} that if $x_T^* \in S$ then $x_S^*=x_T^*$, to generalized it, \\
					We have
					\begin{align}
						\begin{cases}x_T^* \in \text{arg min} \{f(x): x\in T\} \\ x_T^* \in S\end{cases} \\ \Rightarrow x_T^*\in \text{arg min} \{f(x): x\in S\} 
					\end{align}
					Especially for IP, we can take the LP relaxation as $T$ and the original feasible region of IP as $S$, therefore, if we find an optimal solution from LP relaxation $T$ which is also a feasible solution of $S$, then it is the optimal solution for IP ($S$)

				\subsection{LP Relaxation}
					To perform the LP relaxation, we expand the feasible region
					\begin{align}
						x \in \{0,1\} & \rightarrow x\in [0, 1]  \\
						y\in Z^+ & \rightarrow y \ge 0 
					\end{align}
					If we have $Z_LP(s) = conv(s)$ then
					\begin{equation} LP(s): {x\in R_+^n: Ax\le b}\end{equation}
					The closer $LP(s)$ is to $conv(s)$ the better. Interestingly, we only need to know the convex in the direction of $c$.\\
					There are several formulation problem have the property of $Z_{LP}(s) = conv(s)$, such as:\\
					\indent- Assignment Problem\\
					\indent- Spawning Tree Problem\\
					\indent- Max Flow Problem\\
					\indent- Matching Problem

	\part{Graph and Network Theory}\label{Graph}
		\chapter{Basic Structures}
			\section{Graph}
				\begin{definition}[Graph]
					A \textbf{graph} G consists of a finite set $V(G)$ on vertices, a finite set $E(G)$ on edges and an \textbf{incident relation} than associates with any edge $e\in E(G)$ an unordered pair of vertices not necessarily distinct called \textbf{ends}.
				\end{definition}

				\begin{example}
					The following graph can be represented as
					\begin{align}
						V &= V(G) = \{v_1, v_2, v_3, v_4, v_5, v_6\} \\
						E &= E(G) = \{e_1, e_2, e_3, e_4, e_5, e_6, e_7\}\\
						e_1 &= v_1v_2, \quad e_2 = v_2v_4, \quad \dots
					\end{align}

					\begin{figure}[!h]
						\centering
						\begin{tikzpicture}[node distance = 1.3 cm]
							\node (v_2) [solidNode, label=above:{$v_2$}] {};
							\node (v_3) [solidNode, label=above:{$v_3$}, right of = v_2] {};
							\node (v_1) [solidNode, label=below:{$v_1$}, below of = v_2] {};
							\node (v_4) [solidNode, label=below:{$v_4$}, below of = v_3] {};
							\node (v_5) [solidNode, label=above:{$v_5$}, right of = v_3] {};
							\node (v_6) [solidNode, label=below:{$v_6$}, below of = v_5] {};
							\draw [link] (v_2) -- node [left] {$e_2$} (v_1);
							\draw [link] (v_2) -- node [below] {$e_5$} (v_4);
							\draw [link] (v_1) to [out = 180, in = 270, looseness = 5] node [left] {$e_1$} (v_1);
							\draw [link] (v_2) to [out = 45, in = 135] node [above] {$e_3$} (v_3);
							\draw [link] (v_2) -- node [below] {$e_4$} (v_3);
							\draw [link] (v_3) -- node [right] {$e_6$} (v_1);
							\draw [link] (v_5) -- node [right] {$e_7$} (v_6);
						\end{tikzpicture}
					\end{figure}
				\end{example}

				\begin{definition}[Loop, Parallel, Simple Graph]
					An edge with identical ends is called a \textbf{loop}, Two edges having the same ends are said to be \textbf{parallel}, A graph without loops or parallel edges is called \textbf{simple graph}
				\end{definition}

				\begin{definition}[Adjacent]
					Two edges of a graph are \textbf{adjacent} if they have a common end, two vertices are \textbf{adjacent} if they are jointed by an edge.
				\end{definition}

				Saving a graph in computer program can be implemented in the following ways:
				\begin{itemize}
					\item Adjacency matrix: $m \times n$ matrix, for $A[u, v] = 1$ if $(u, v) \in E$ and $A[u, v] = 0$ otherwise
					\item Linked list: For every vertex $v$, there is a linked list containing all neighbors of $v$.
				\end{itemize}
				Assuming we are dealing with undirected graphs, $n$ is the number of vertices, $m$ is the number of edges, $n - 1 \le m \le n(n-1)/2$, $d_v$ is the number of neighbors of $v$ then
				\begin{table}[h]
					\centering
					\begin{tabular}{|c|c|c|}
						\hline
						 & Matrix & Linked lists\\
						\hline
						memory usage & $O(n^2)$ & $O(m)$\\
						\hline
						time to check $(u, v) \in E$ & $O(1)$ & $O(d_u)$\\
						\hline
						time to list all neighbors of $v$ & $O(n)$ & $O(d_v)$\\
						\hline
					\end{tabular}
				\end{table}

			\section{Subgraph}
				\begin{definition}[Subgraph]
					Given two graphs $G$ and $H$, $H$ is a \textbf{subgraph} of $G$ if $V(H)\subseteq V(G)$, $E(H)\subseteq E(G)$ and an edge has the same ends in $H$ as it does in $G$. Furthermore, if $E(H)\neq E(G)$ then $H$ is a proper subgraph.
				\end{definition}

				\begin{definition}[Spanning]
					A subgraph $H$ on $G$ is \textbf{spanning} if $V(H) = V(G)$
				\end{definition}

				\begin{definition}[Vertex-induced, Edge-induced]
					For a subset $V^\prime \subseteq V(G)$ we define an \textbf{vertex-induced} subgraph $G[V^\prime ]$ to be the subgraph with vertices $V^\prime$ and those edges of $G$ having both ends in $V^\prime$. The \textbf{edge-induced} subgraph $G[E^\prime ]$ has edges $E^\prime$ and those vertices of $G$ that are ends to edges in $E^\prime$.
				\end{definition}

				\notice{If we combine node-induced or edge-induced subgraphs $G(V^\prime )$ and $G(V - V^\prime )$, we cannot always get the entire graph.}

			\section{Degree}
				\begin{definition}[Degree]
					Let $v\in V(G)$, then the \textbf{degree} of $v\in V(G)$ denote by $d_G(v)$ is defines to be the number of edges incident of $v$. Loops counted twice.
				\end{definition}

				\begin{theorem}
					For any graph $G=(V, E)$
					\begin{equation}
						\sum_{v\in V}d(v) = 2|E|
					\end{equation}
				\end{theorem}

				\begin{proof}
					$\forall$ edge $e=uv$ with $u \neq v$, $e$ is counted once for $u$ and once for $v$, a total of two altogether. If $e=uu$, a loop, then it is counted twice for $u$.
				\end{proof}

				\begin{problem}
					Explain clearly, what is the largest possible number of vertices in a graph with 19 edges and all vertices of degree at least 3. Explain why this is the maximum value.
				\end{problem}

				\begin{solution}
					The maximum number is 12.
				\end{solution}

				\begin{proof}
					First we prove 12 vertices is possible, then we prove 13 vertices is not possible
					\begin{itemize}
						\item The following graph contains 12 vertices and 18 edges, each vertex has a degree of 3.\\
							\begin{figure}[!ht]
								\centering
								\begin{tikzpicture}[node distance=0.5 cm]
									\node (1) [solidNode] {};
									\node (2) [solidNode, below of=1] {};
									\node (3) [solidNode, below of=2] {};
									\node (4) [solidNode, below of=3] {};
									\node (5) [solidNode, below of=4] {};
									\node (6) [solidNode, below of=5] {};
									\node (7) [solidNode, right of=1, xshift=1 cm] {};
									\node (8) [solidNode, right of=2, xshift=1 cm] {};
									\node (9) [solidNode, right of=3, xshift=1 cm] {};
									\node (10) [solidNode, right of=4, xshift=1 cm] {};
									\node (11) [solidNode, right of=5, xshift=1 cm] {};
									\node (12) [solidNode, right of=6, xshift=1 cm] {};
									\draw [link] (1) -- (7);
									\draw [link] (1) -- (8);
									\draw [link] (1) -- (9);
									\draw [link] (2) -- (8);
									\draw [link] (2) -- (9);
									\draw [link] (2) -- (10);
									\draw [link] (3) -- (9);
									\draw [link] (3) -- (10);
									\draw [link] (3) -- (11);
									\draw [link] (4) -- (10);
									\draw [link] (4) -- (11);
									\draw [link] (4) -- (12);
									\draw [link] (5) -- (11);
									\draw [link] (5) -- (12);
									\draw [link] (5) -- (7);
									\draw [link] (6) -- (12);
									\draw [link] (6) -- (7);
									\draw [link] (6) -- (8);
								\end{tikzpicture}
							\end{figure}
						\item For 13 vertices and each vertex has a degree of at least 3 will require at least
							\begin{equation}
								2|E| = \sum_{v \in V}d(v) \ge 3 \times |N| = 3 \times 13 \Rightarrow |E| \ge 19.5 > 19
							\end{equation}
							edges, i.e., 13 vertices is not possible.
					\end{itemize}
				\end{proof}

				\begin{corollary}
					Every graph has an even number of odd degree vertices.
				\end{corollary}

				\begin{proof}
					\begin{equation}
						V = V_E\cup V_O \Rightarrow 
						\sum_{v\in V}d(v) = \sum_{v\in V_E} d(v) + \sum_{v\in V_O}d(v) = 2|E|
					\end{equation}
				\end{proof}

			\section{Special Graphs}
				\begin{definition}[Complete Graph]
					A \textbf{complete} graph $K_n (n \ge 1)$ is a simple graph with $n$ vertices and with exactly one edge between each pair of distinct vertices.
				\end{definition}

				\begin{definition}[Cycle]
					A \textbf{cycle} graph $C_n (n \ge 3)$ consists of $n$ vertices $v_1, ... v_n$ and $n$ edges $\{v_1, v_2\}, \{v_2, v_3\}, ... \{v_{n-1}, v_n\}$
				\end{definition}

				\begin{definition}[Wheel]
					A \textbf{wheel} graph $W_n (n \ge 3)$ is a simple graph obtains by adding one vertex to the cycle graph $C_n$, and connecting this new vertex to all vertices of $C_n$ 
				\end{definition}

				\begin{definition}[Bipartite Graph]
					A simple graph is said to be \textbf{bipartite} if the vertex set can be expressed as the union of two disjoint non-empty subsets $V_1$ and $V_2$ such that every edges has one end in $V_1$ and another end in $V_2$
				\end{definition}

				\begin{example}
					Here is an example for bipartite graph
					\begin{figure}[!h]
						\centering
						\begin{tikzpicture}[node distance = 0.7 cm]
							\node (A) [solidNode] {};
							\node (B) [solidNode, right of = A] {};
							\node (C) [solidNode, below of = A] {};
							\node (D) [solidNode, right of = C] {};
							\node (E) [solidNode, below of = C] {};
							\node (F) [solidNode, right of = E] {};
							\draw [link] (A) -- (B);
							\draw [link] (A) -- (D);
							\draw [link] (C) -- (B);
							\draw [link] (C) -- (F);
							\draw [link] (E) -- (D);
							\draw [link] (E) -- (F);
							\draw [link] (A) -- (F);
						\end{tikzpicture}
					\end{figure}
				\end{example}

				\begin{definition}[Complete Bipartite Graph]
					The \textbf{complete bipartite graph} $K_{mn}$ is the bipartite graph $V_1$ containing $m$ vertices and $V_2$ containing $n$ vertices such that each vertiex in $V_1$ is adjacent to every vertex in $V_2$
				\end{definition}

				\begin{example}
					Here is an example for $K_{53}$
					\begin{figure}[!h]
						\centering
						\begin{tikzpicture}[node distance = 0.7 cm]
							\node (A) [solidNode] {};
							\node (B) [solidNode, below of = A] {};
							\node (C) [solidNode, below of = B] {};
							\node (D) [solidNode, below of = C] {};
							\node (E) [solidNode, below of = D] {};
							\node (F) [solidNode, right of = B] {};
							\node (G) [solidNode, right of = C] {};
							\node (H) [solidNode, right of = D] {};
							\draw [link] (A) -- (F);
							\draw [link] (A) -- (G);
							\draw [link] (A) -- (H);
							\draw [link] (B) -- (F);
							\draw [link] (B) -- (G);
							\draw [link] (B) -- (H);
							\draw [link] (C) -- (F);
							\draw [link] (C) -- (G);
							\draw [link] (C) -- (H);
							\draw [link] (D) -- (F);
							\draw [link] (D) -- (G);
							\draw [link] (D) -- (H);
							\draw [link] (E) -- (F);
							\draw [link] (E) -- (G);
							\draw [link] (E) -- (H);
						\end{tikzpicture}
					\end{figure}
				\end{example}

				\begin{theorem}
					A graph $G$ is bipartite iff every cycle is even.
				\end{theorem}

				\begin{proof}
					($\Rightarrow$) If the graph $G$ is bipartite, by definition, the vertices of graph can be partition into two groups, that within the group there is no connection between vertices. Therefore, for each cycle, the odd index of vertices and even index of vertices has to be choose alternatively from each groups. Therefore the cycle has to be even.

					($\Leftarrow$) Prove by contradiction. A graph can be connected or not connected.

					If $G$ is connected and has at least two vertices, for an arbitrary vertex $v\in V(G)$, we can calculate the minimum number of edges between the other vertices $v^\prime$ and $v$ (i.e., length, denoted by $l(v^\prime, v)$), for all the vertices that has odd length to $v$, assign them to set $V_1$, for the rest of vertices (and $v$), assign to set $V_2$. Assume that $G$ is not bipartite, which means there are at least one edge between distinct vertices in set $V_1$ or set $V_2$, without lost of generality, assume that edge is $uw$, $u, w\in V_1$. For all vertices in $V_1$ there is an odd length of path between the vertex and $v$, therefore, there exists an odd $l(u,v)$, and an odd $l(w-v)$. The length of cycle $l(u, w, v) = 1 + l(u, v) + l(w, v)$, which is an odd number, it contradict with the prerequisite that all cycles are even, which means the assumption that $G$ is not bipartite is incorrect, $G$ should be bipartite.

					If $G$ is not connected. Then $G$ can be partition into a set of disjointed subgraphs which are connected with at least two vertices or contains only one vertex. For the component that has more that one vertices, we already proved that it has to be bipartite. For the subgraph $G_i \subset G, i = 1, 2, ..., n$, the vertices can be partition into $V_{i1} \in V(G_i)$ and $V_{i2} \in V(G_i)$, where $V_{i1} \cap V_{i2} = \emptyset$, the union of those subgraphs are bipartite too because $V_1 = \cup_{i=1}^n V_{i1} \in V(G)$ and $V_2 = \cup_{i=1}^n V_{i2} \in V(G)$ satisfied the condition of bipartite. For the subgraph that has one one vertices, those vertices can be assigned into either $V_1$ or $V_2$.
				\end{proof}

				\begin{example}
					The following graph is bipartite, it only contains even cycles.\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance=0.7 cm]
							\node (a) [solidNode, label=above:{a}] {};
							\node (b) [solidNode, label=above:{b}, right of = a] {};
							\node (c) [solidNode, label=above:{c}, right of = b] {};
							\node (d) [solidNode, label=above:{d}, right of = c] {};
							\node (e) [solidNode, label=below:{e}, below of = a] {};
							\node (f) [solidNode, label=below:{f}, below of = b] {};
							\node (g) [solidNode, label=below:{g}, below of = c] {};
							\node (h) [solidNode, label=below:{h}, below of = d] {};
							\draw [link] (a) -- (b);
							\draw [link] (b) -- (c);
							\draw [link] (c) -- (d);
							\draw [link] (a) -- (e);
							\draw [link] (b) -- (f);
							\draw [link] (c) -- (g);
							\draw [link] (d) -- (h);
							\draw [link] (e) -- (f);
							\draw [link] (f) -- (g);
							\draw [link] (g) -- (h);
							\draw [link] (a) to [out = 60, in = 120, looseness = 1] (d);
							\draw [link] (e) to [out = 300, in = 240, looseness = 1] (h);
						\end{tikzpicture}
					\end{figure}
					We can rearrange the graph to be more clear as following\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance=0.7 cm]
							\node (a) [solidNode, label=right:{a}] {};
							\node (b) [solidNode, label=left:{b}, left of=a] {};
							\node (c) [solidNode, label=right:{c}, below of=a] {};
							\node (d) [solidNode, label=left:{d}, below of=b] {};
							\node (e) [solidNode, label=left:{e}, below of=d] {};
							\node (f) [solidNode, label=right:{f}, below of=c] {};
							\node (g) [solidNode, label=left:{g}, below of=e] {};
							\node (h) [solidNode, label=right:{h}, below of=f] {};
							\draw [link] (a) -- (b);
							\draw [link] (b) -- (c);
							\draw [link] (c) -- (d);
							\draw [link] (a) -- (e);
							\draw [link] (b) -- (f);
							\draw [link] (c) -- (g);
							\draw [link] (d) -- (h);
							\draw [link] (e) -- (f);
							\draw [link] (f) -- (g);
							\draw [link] (g) -- (h);
							\draw [link] (a) -- (d);
							\draw [link] (e) -- (h);
						\end{tikzpicture}
					\end{figure}
					The vertices of graph $G$ can be partition into two sets, $\{a, c, f, h\}$ and $\{b, d, e, g\}$
				\end{example}

				\begin{example}
					The following graph is not bipartite\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance=0.5 cm]
							\node (11) [solidNode, label=left:{11}] {};
							\node (4) [solidNode, label=below:{4}, below of=11, yshift=-0.205 cm] {};
							\node (3) [solidNode, label=below:{3}, below of=4, xshift=-0.5 cm] {};
							\node (5) [solidNode, label=below:{5}, below of=4, xshift=0.5 cm] {};
							\node (1) [solidNode, label=below:{1}, below of=3, xshift=-1.205 cm] {};
							\node (2) [solidNode, label=below:{2}, below of=3, xshift=-0.5 cm] {};
							\node (6) [solidNode, label=below:{6}, below of=3, xshift=0.5 cm] {};
							\node (10) [solidNode, label=below:{10}, below of=5, xshift=0.5 cm] {};
							\node (12) [solidNode, label=below:{12}, below of=5, xshift=1.205 cm] {};
							\node (7) [solidNode, label=below:{7}, below of=2, xshift=0.5 cm] {};
							\node (8) [solidNode, label=left:{8}, below of=7, xshift=0.5 cm] {};
							\node (9) [solidNode, label=below:{9}, below of=6, xshift=0.5 cm] {};
							\node (13) [solidNode, label=below:{13}, below of=8, yshift=-0.205 cm] {};
							\draw [link] (11) -- (1);
							\draw [link] (11) -- (12);
							\draw [link] (11) -- (4);
							\draw [link] (4) -- (3);
							\draw [link] (4) -- (5);
							\draw [link] (3) -- (2);
							\draw [link] (3) -- (6);
							\draw [link] (5) -- (6);
							\draw [link] (5) -- (10);
							\draw [link] (1) -- (2);
							\draw [link] (10) -- (12);
							\draw [link] (2) -- (7);
							\draw [link] (6) -- (7);
							\draw [link] (6) -- (9);
							\draw [link] (10) -- (9);
							\draw [link] (7) -- (8);
							\draw [link] (9) -- (8);
							\draw [link] (8) -- (13);
							\draw [link] (1) -- (13);
							\draw [link] (12) -- (13);
						\end{tikzpicture}
					\end{figure}
					The cycle $c=v_1v_11v_4v_3v_2$ have odd number of vertices.
				\end{example}

			\section{Directed Graph}
				\begin{definition}
					A graph $G=(V, E)$ is called directed if for each edge $e\in E$, there is a \textbf{head} $h(e)\in V$ and a \textbf{tail} $t(e)\in V$ and the edges of $e$ are precisely $h(e)$ and $t(e)$, denoted $e = (t(e), h(e))$
				\end{definition}

				\begin{figure}
					\centering
					\begin{tikzpicture}[node distance = 2 cm]
						\node (1) [circleNode] {u};
						\node (2) [circleNode, right of = 1] {w};
						\node (3) [circleNode, below of = 1] {v};
						\node (4) [circleNode, right of = 3] {z};
						\draw [arrow] (1) -- (3);
						\draw [arrow] (1) -- (4);
						\draw [arrow] (3) -- (4);
						\draw [arrow] (1) to [out = 30, in = 150] (2);
						\draw [arrow] (2) -- (1);
						\draw [arrow] (4) to [looseness = 3] (4);
					\end{tikzpicture}
				\end{figure}

				\begin{definition}
					We call directed graphs \textbf{digraphs}, we call edges in a digraph are called \textbf{arcs}, and vertices in a digraph \textbf{nodes}
				\end{definition}

				\begin{definition}
					Similar as in the undirected case we have walks, traces, paths and cycles in digraphs.
				\end{definition}

				\begin{definition}
					A vertex $v\in V$ is \textbf{reachable} from a vertex $u \in V$ if there is a $(u,v)$-dipath. If at the same time $u$ is reachable from $v$, they are \textbf{strongly connected}
				\end{definition}

				\begin{definition}
					A digraph is strongly connected if every pair of vertices are strongly connected.
				\end{definition}

				\begin{definition}
					A digraph is \textbf{strict} if it has no loops and whenever $e$ and $f$ are parallel, $h(e) = t(f)$
				\end{definition}

				\begin{definition}
					For a vertex $v$ in a digraph $D$, the \textbf{indegree} of $v$ in $D$, denoted by $d^+(v)$ is the number of arcs of $D$ having head $V$. The \textbf{outdegree} of $v$ is denoted by $d^-(v)$ is the number of arcs of $D$ having tail $v$.
				\end{definition}

				Let $D=(V, A)$ be a digraph with no loops a vertex-arc \textbf{incident matrix} for $D$ is a $(0, 1, -1)$ matrix $N$ with rows indexed by $V = \{v_1, ..., v_n\}$ and column indexed by $A = \{e_1, ..., e_m\}$ and where entry $(i, j)$ in the matrix $n_{ij}$ is
				\begin{equation}
					n_{ij} = \begin{cases}
						1, \quad \text{if} \ v_i = h(e_j) \\
						-1, \quad \text{if} \ v_i = t(e_j) \\
						0, \quad \text{otherwise}
					\end{cases}
				\end{equation}

				\begin{equation}
					\begin{bmatrix}
						-1 & 0 & -1 & -1 & 1 \\
						1 & -1 & 0 & 0 & 0 \\
						0 & 0 & 0 & 1 & -1 \\
						0 & 1 & 1 & 0 & 0
					\end{bmatrix}
				\end{equation}

			\section{Sperner's Lemma}

		\chapter{Paths, Trees, and Cycles}
			\section{Walk}
				\begin{definition}[walk]
					A \textbf{walk} in a graph $G$ is a finite sequence $w=v_0e_1v_1e_2...e_kv_k$, where for each $e_i=v_{i-1}v_i$ the edge and its ends exists in $G$. We say that walk $v_0$ to $v_k$ on $(v_0, v_k)$-walk.
				\end{definition}

				\begin{example}
					\begin{equation}
						w = v_2e_4v_3e_4v_2e_5v_3
					\end{equation}
					is a walk, or $(v_2, v_3)$-walk
				\end{example}

				\begin{definition}[origin, terminal, internal, length]
					For $(v_0, v_k)$-walk, The vertices $v_0$ and $v_k$ are called the \textbf{origin} and the \textbf{terminal} of the walk w, $v_1..v_{k-1}$ are called \textbf{internal} vertices. The integer $k$ is the \textbf{length} of the walk. Length of $w$ equals to the number of edges.
				\end{definition}

				We can create a reverse walk $w^{-1}$ by reversing $w$.
				\begin{equation}
					w^{-1} = v_ke_kv_{k-1}e_{k-1}...e_2v_1
				\end{equation}
				(The reverse walk is guaranteed to exist because it is an undirected graph)

				Given two walks $w$ and $w'$ we can create a third walk denoted by $ww'$ by concating $w$ and $w'$. The new walk's origin is the same as terminal.

			\section{Path and Cycle}
				\begin{definition}[trail]
					A \textbf{trail} is a walk with no repeating edges. e.g., $v_3e_4v_2e_5v_3$
				\end{definition}

				\begin{definition}[path]
					A \textbf{path} is a trail with no repeating vertices. e.g., $v_3e_4v_2$
				\end{definition}

				\notice{Paths $\subseteq$ Trails $\subseteq$ Walks}

				\begin{definition}[closed, cycle]
					A path is \textbf{closed} if it has positive length and its origin and terminal are the same. e.g., $v_1e_2v_2e_4v_3e_3v_1$. A closed trail where origin and internal vertices are distinct is called a \textbf{cycle} (The only time a vertex is repeated is the origin and terminal)
				\end{definition}

				\begin{definition}[even/odd cycle]
					A cycle is \textbf{even} if it has a even number of edges otherwise it is \textbf{odd}.
				\end{definition}

				\begin{problem}
					Prove that if $C_1$ and $C_2$ are cycles of a graph, then there exists cycles $K_1, K_2, ..., K_m$ such that $E(C_1)\Delta E(C_2) = E(K_1)\cup E(K_2) \cup...\cup E(K_m)$ and $E(K_i)\cap E(K_j)=\emptyset, \forall i \neq j$. (For set $X$ and $Y$, $X\Delta Y = (X-Y)\cup(Y-X)$, and is called the symmetric difference of $X$ and $Y$)
				\end{problem}

				\begin{proof}
					Proof by constructing $K_1, K_2, ... K_m$. Denote 
					\begin{align}
						C_1 & = v_{11}e_{11}v_{12}e_{12}v_{13}e_{13}...v_{1n}e_{1n}v_{11}\\
						C_2 & = v_{21}e_{21}v_{22}e_{22}v_{23}e_{23}...v_{2k}e_{2k}v_{21}
					\end{align}
					Assume both cycle start at the same vertice, $v_{11} = v_{12}$. (If there is no intersected vertex for $C_1$ and $C_2$, just simply set $K_1 = C_1$ and $K_2 = C_2$)\\
					The following algorithm can give us all $K_j, j=1, 2, ... , m$ by constructing $E(C_1)\Delta E(C_2)$.  Also, the complexity is $O(mn)$, which makes the proof doable.\\
					\begin{algorithm}[!ht]
						\caption{Find $K_1, K_2, ... K_m$ by constructing $E(C_1)\Delta E(C_2)$}
						\begin{algorithmic}[1]
							\Require Graph $G$, cycle $C_1$ and $C_2$
							\Ensure $K_1, K_2, ... K_m$
							\State Initial, $K \gets \emptyset$, $j = 1$
							\State Set temporary storage units, $v_o \gets v_{11}$, $v_t \gets \emptyset$
							\For {$i = 1, 2, ..., n$}
								\If {$e_{1i} \in C_2$}
									\If {$v_o \ne v_{1i}$}
										\State $v_t \gets v_{1i}$
										\State concate $(v_o, v_t)$-path $\subset C_1$ and $(v_o, v_t)$-path $\subset C_2$ to create a new $K_j$
										\State Append $K$ with $K_j$, $K \gets K \cup K_j$
										\State Reset temporary storage unit. $v_o \gets v_{1(i+1)}$ (or $v_{11}$ if $i = n$), $v_t \gets \emptyset$
									\Else
										\State $v_o \gets v_{1(i+1)}$ (or $v_{11}$ if $i = n$)
									\EndIf
								\EndIf
							\EndFor
						\end{algorithmic}
					\end{algorithm}
					Now we prove that $K_i\cap K_j = \emptyset, \forall i \ne j$. For each $K_j$, it is defined by two $(v_o, v_t)$-paths in the algorithm. From the algorithm we know that all the edges in $(v_o, v_t)$-path in $C_1$ are not intersecting with $C_2$, because if the edge in $C_1$ is intersected with $C_2$, either we closed the cycle $K_j$ before the edge, or we updated $v_o$ after the edge (start a new $K_j$ after that edge). By definition of cycle, all the $(v_o, v_t)$-path that are subset of $C_1$ are not intersecting with each other, as well as all the $(v_o, v_t)$-path that are subset of $C_2$. Therefore, $K_i\cap K_j = \emptyset, \forall i \ne j$.
				\end{proof}

				\begin{definition}[connected vertices]
					Two vertices $u$ and $v$ in a graph are said to be \textbf{connected} if there is a path between $u$ and $v$.
				\end{definition}

				\begin{definition}[component]
					Connectivity between vertices is an equivalence relation on $V(G)$, if $V_1, ... V_k$ are the corresponding equivalent classes then $G[V_1]...G[V_k]$ are \textbf{components} of G. If graph has only one component, then we say the graph is connected. A graph is connected iff every pair of vertices in G are connected, i.e., there exists a path between every pair of vertices.
				\end{definition}

				\begin{problem}
					If $G$ is a simple graph with at least two vertices, prove that $G$ has two vertices with the same degree.
				\end{problem}

				\begin{proof}
					A simple graph can only be connected or not connected.
					\begin{itemize}
						\item If $G$ is connected, i.e., for all vertices, the degree is greater than 0. Also the graph is simple, for a graph with $|N|$ vertices, the degree of each vertex is less or equal to $|N| - 1$ (cannot have loop or parallel edge). For $|N|$ vertices, to make sure there is no two vertices that has same degree, it will need $|N|$ options for degrees, however, we only have $|N| - 1$ option. According to pigeon in holes principle, there has to be at least two vertices with the same degree.
						\item If $G$ is not connected, i.e., the graph has more than one component. One of the following situation will happen:
						\begin{itemize}
							\item For all components, each component contains only one vertex. Since we have at least two vertices, which means there are at least two component that has only one vertex. For those vertices, at least two vertices has the same degree as 0.
							\item At least one component has more than one vertices. In this situation, we can find a component that has more than one vertices as a subgraph $G^\prime$ of the graph $G$. That $G^\prime$ is a connected simple graph by definition. We have already proved that a connected simple graph has two vertices with the same degree, which means $G$ has two vertices with the same degree.
						\end{itemize}
					\end{itemize}
				\end{proof}

			\section{Tree and forest}
				\begin{definition}[acyclic graph]
					A graph is called \textbf{acyclic} if it has no cycles
				\end{definition}

				\begin{definition}[forest, tree]
					A acyclic graph is called a \textbf{forest}. A connected forest is called a \textbf{tree}. 
				\end{definition}

				\begin{theorem}
					Prove that $T$ is a tree, if $T$ has exactly one more vertex than it has edges.
				\end{theorem}

				\begin{proof}
					\begin{enumerate}
						\item First we prove for any tree $T$ that has at least two vertices, there has to be at least one leaf, i.e., now we prove that we can find $u$ with degree of 1. Proof by constructing algorithm. (In fact we can prove that there are at least two leaves.)\\
							\begin{algorithm}[!ht]
								\caption{Find one leaf in a tree}
								\begin{algorithmic}[1]
									\Require $d(u)=1$
									\Ensure A tree $T$ has at least one vertex
									\State Let $u$ and $v$ be any distinct vertex in a tree $T$
									\State Let $p$ be the path between $u$ and $v$
									\While {$d(u) \neq 1$}
										\If {$d(u) > 1$}
											\State Let $n(u)$ be the set of neighboring vertices of $u$
											\State In $n(u)$, find a $u^\prime$ that the edge between $u$ and $u^\prime$, denoted by $e$, $e \notin p$
											\State $u \gets u^\prime$
											\State $p \gets p \cup e$
										\EndIf
									\EndWhile
								\end{algorithmic}
							\end{algorithm}
							The above algorithm is guaranteed to have an end because a tree is acyclic by definition
						\item Then, if we remove one leaf in the tree, i.e., we remove an edge and a vertex, where that vertex only connects to the edge we removed. One of the following situations will happen:
						\begin{enumerate}
							\item Situation 1: The remaining of $T$ is one vertex. In this case, $T$ has two vertices an one edge. (Exactly one more vertex than it has edges)
							\item Situation 2: The remaining of $T$ is another tree $T^\prime$ (removal of edges will not change acyclic and connectivity), where $|V(T)| = |V(T^\prime )| + 1$ and $|E(T)| = |E(V^\prime | + 1$. (one edge and one vertex has been removed)
						\end{enumerate}
						\item Do the leaf removal process recursively to $T^\prime$ if Situation 2 happens until Situation 1 happens. 
					\end{enumerate}
				\end{proof}

			\section{Spanning tree}
				\begin{definition}[spanning tree]
					A subgraph T of G is a \textbf{spanning tree} if it is spanning ($V(T)=V(G)$) and it is a tree.
				\end{definition}

				\begin{example}
					In the following graph\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.6, node distance = 1.2 cm]
							\node (v_2) [circleNode] {$v_2$};
							\node (v_3) [circleNode, right of = v_2] {$v_3$};
							\node (v_1) [circleNode, below of = v_2] {$v_1$};
							\node (v_4) [circleNode, below of = v_3] {$v_4$};
							\node (v_5) [circleNode, right of = v_3] {$v_5$};
							\draw [link] (v_1) -- (v_2);
							\draw [link] (v_2) -- (v_3);
							\draw [link] (v_1) -- (v_4);
							\draw [link] (v_2) -- (v_4);
							\draw [link] (v_3) -- (v_5);
							\draw [link] (v_4) -- (v_5);
							\draw [link] (v_1) -- (v_3);
						\end{tikzpicture}
					\end{figure}
					This is a spanning tree\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[scale=0.6, node distance = 1.2 cm]
							\node (v_2) [circleNode] {$v_2$};
							\node (v_3) [circleNode, right of = v_2] {$v_3$};
							\node (v_1) [circleNode, below of = v_2] {$v_1$};
							\node (v_4) [circleNode, below of = v_3] {$v_4$};
							\node (v_5) [circleNode, right of = v_3] {$v_5$};
							\draw [link] (v_2) -- (v_3);
							\draw [link] (v_1) -- (v_4);
							\draw [link] (v_1) -- (v_3);
							\draw [link] (v_3) -- (v_5);
						\end{tikzpicture}
					\end{figure}
				\end{example}

				\begin{problem}
					Prove that if $T_1$ and $T_2$ are spanning trees of $G$ and $e\in E(T_1)$, then there exists a $f\in E(T_2)$, such that $T_1 - e + f$ and $T_2 + e - f$ are both spanning trees of $G$.
				\end{problem}

				\begin{proof}
					One of the following situation has to happen:
					\begin{enumerate}
						\item If for given $e \in E(T_1)$, $\exists f = e \in E(T_2)$, then $T_1 - e + f = T_1$, $T_2 + e - f = T_2$ are both spanning trees of $G$
						\item If for given $e \in E(T_1)$, $e \notin E(T_2)$, the following will find an edge $f$ that $T_1 - e + f$ and $T_2 + e - f$ are both spanning trees of $G$.
						\begin{enumerate}
							\item $T_1$ is a spanning tree, removal of $e \in E(T_1)$ will disconnect the spanning tree into two components (by definition of spanning tree), denoted by $G_1 \subset G$ and $G_2 \subset G$, by definition, $V(G_1)$ and $V(G_2)$ is a partition of $V(G)$.
							\item Add $e$ into $T_2$. We can proof that by adding an edge into a tree will create exactly one cycle, denoted by $C$, $e \in E(C)$.
							\item For $C$, since it is a cycle and one end of $e$ is in $V(G_1)$, the other end of $e$ is in $V(G_2)$, there has to be at least two edges (can be more) that has one end in $V(G_1)$ and the other end in $V(G_2)$, denote the set of those edges as $E \subset E(C)$, one of those edges is $e \in E$
							\item Choose any $f \in E$ and $f \neq e$, for that $f$, $T_1 - e + f$ and $T_2 + e - f$ are both spanning trees of $G$.
							\item Prove that $T_1 - e + f$ is a spanning tree
							\begin{enumerate}
								\item $T_1 - e + f$ have the same set of vertices as $T_1$, therefore it is spanning.
								\item It is connected both within $G_1$ and $G_2$, for $f$, one end is in $V(G_1)$, the other end is in $V(G_2)$ therefore $T_1 - e + f$ is connected.
								\item $T_1 - e + f$ have the same number of edges as $T_1$, which is $|T_1| - 1$, therefore $T_1 - e + f$ is a tree. (We have proven the connectivity in the previous step.)
								\item $T_1 - e + f$ is spanning, connected, a tree, therefore it is a spanning tree.
							\end{enumerate}
							\item Prove that $T_2 + e - f$ is a spanning tree
							\begin{enumerate}
								\item $T_2 + e - f$ have the same set of vertices as $T_2$, therefore it is spanning.
								\item $T_2$ is connected, adding an edge will not break connectivity, therefore $T_2 + e$ is connected, removing an edge in a cycle will not break connectivity, therefore $T_2 + e - f$ is connected.
								\item $T_2 - e + f$ have the same number of edges as $T_2$, which is $|T_2| - 1$, therefore $T_2 + e - f$ is a tree. (We have proven the connectivity in the previous step.)
								\item $T_2 - e + f$ is spanning, connected, a tree, therefore it is a spanning tree.
							\end{enumerate}
						\end{enumerate}
					\end{enumerate}
				\end{proof}

				\begin{theorem}
					Every connected graph has a spanning tree.
				\end{theorem}

				\begin{proof}
					Prove by constructing algorithm:
					\begin{algorithm}[!ht]
						\caption{Find a spanning tree for connected graph (Prim's Algorithm in unweighted graph)}
						\begin{algorithmic}[1]
							\Require a connected graph G and an enumeration $e_1,...e_m$ of the edges of G
							\Ensure a spanning tree T of G
							\State Let T be the spanning subgraph of $G$ with $V(T)=V(G)$ and $E(T)=\emptyset$
							\State $i \gets 1$
							\While {$i \le |E|$}
								\If {$T + e_i$ is acyclic}
									\State $T \gets T + e_i$
									\State $i \gets i + 1$
								\EndIf
							\EndWhile
						\end{algorithmic}
					\end{algorithm}
				\end{proof}

				\notice{This algorithm can be improved, one idea is to make summation of edges in spanning subgraph less or equation to $|V| - 1$}

				For the complexity of spanning tree algorithm:
				\begin{enumerate}
					\item Space complexity, $2|E|$, which is $O(|E|)$
					\item Time complexity
					\begin{enumerate}
						\item How to check for acyclic?
						\begin{enumerate}
							\item At every stage $T$ has certain components $V_1, ... V_t$, (every time we add an edge, the number of components minus 1)
							\item So at the beginning $t = |V|$ with $|V_i| = 1 \forall i$ and at the end, $t = 1$.
						\end{enumerate}
						\item Count the amount of work for the algorithm.
						\begin{enumerate}
							\item Need to check for acyclic for each edge, which costs $O(|E|)$
							\item Need to flip the pointer for each vertex, for each vertex, at most will be flipped $\log|V|$ times, altogether $|V|\log|V|$ times.
							\item The time complexity is $O(|E| + |V|\log|V|)$
						\end{enumerate}
					\end{enumerate}


					\item First we need to input the data, create an array such that the first and the second entries are the ends of $e_1$, third and fourth are the ends of $e_2$, and so on.
					\item The amount of storage needs in $2|E|$, which is $O(|E|)$
					\item The main work involved in the algorithm is for each edges $e_i$ and the current $T$, to determine if $T+e_i$ creates a cycle.

					\item suppose we keep each component $V_i$ by keeping for each vertex a pointer from the vertex to the name of the component containing it. Thus if $\mu \in V_3$, there will be a pointer from $\mu$ to integer 3.
					\item Then when edge $e_i = \mu v$ is encountered in Step 2, we see that $T+e_i$ contains a cycle if and only if $\mu$ and $v$ point to same integer which means they are in the same component
					\item If they are not in the same component, we want to add the edge which means then I have to update the pointers.
				\end{enumerate}

				To prove algorithm we need to show the output is a spanning tree, which means three properties must hold:
				\begin{itemize}
					\item spanning (Step I)
					\item acyclic (We never add an edge that create a cycle)
					\item connected (Proof by contradiction)
				\end{itemize}
				So it is sufficient to show that the output will be connected.
				\begin{proof}
					(Proof by Contradiction) Suppose the output graph $T$ of the algorithm is NOT connected. Let $T_1$ be a component of $T$, let $x\in T_1$ and $y \notin T_1$. But $G$ is a connected graph (given from the beginning), so there must be a path in $G$ that connects $x$ and $y$. Let such a path in $G$ be $p=xe_1v_1e_2,..v_{k-1}e_ky$. Clearly, $p\notin T_1$. So there must be a first vertex in $P$ that not in $T_1$. So $e_i \notin E(T)$, the only way this can happen when applying the algorithm is if $T + e_i$ creates a cycle $C$, i.e., $e_i \in C$, so $C - e_i$ is a path connecting $v_{i-1}$ and $v_i$. So $c - e_i \in T$, so $v_{i-1}$ is connected to $v_i \in T$. Contradiction. 
				\end{proof}

			\section{Cayley's Formula}

			\section{Connectivity, DFS, BFS}
				\subsection{Connectivity Problem}
					For connectivity problem, the input is a graph $G = (V, E)$, with linked list representation, and two vertices $s, t\in V$. The problem is to find whether there is a path connecting $s$ to $t$ in $G$

					There are two commonly use methods to solve connectivity problem: depth-first search and breath-first search.
				\subsection{Depth-First Search (DFS)}
					The idea of DFS is enumerating children before siblings when search in the graph/tree. It is a recursive algorithm. First, start with $s$, then travel through the fist edge leading out of the current vertex, when reached a ``visited'' vertex, go back and travel through next edge. If tried all edges leading out of the current vertex, go back.

					The algorithm is as following
					\begin{algorithm}[h]
						\caption{Depth-First Search}
						\begin{algorithmic}[1]
							\State Initialize, make all vertices as ``unvisited''
							\State \texttt{RecursiveDFS(s)}
						\end{algorithmic}
					\end{algorithm}

					\begin{algorithm}[h]
						\caption{RecursiveDFS(v)}
						\begin{algorithmic}[1]
							\State Mark $v$ as ``visited''
							\For {$\forall u \in d_s$}:
								\If {$u$ is ``unvisited''}
									\State \texttt{RecursiveDFS(u)}
								\EndIf
							\EndFor
						\end{algorithmic}
					\end{algorithm}

					The running time of DFS is $O(n + m)$.

					If we only want to know if $s$ and $t$ are connected, the algorithm can be terminated if $u = t$.

				\subsection{Breadth-First Search (BFS)}
					The idea of BFS is enumerating siblings before children when search in the graph/tree. The general steps of BFS is as following: First, build layers $L_0, L_1, \cdots$; Next, set $L_0 = \{s\}$, where $s$ is the starting vertex; Then, $L_{j + 1}$ contains all nodes that are not in $\cup_{i = 0}^j L_i$ and have an edge to a vertex in $L_j$

					The algorithm is as following
					\begin{algorithm}[h]
						\caption{Breadth-First Search}
						\begin{algorithmic}[1]
							\State Initialize, $head \gets 1$, $tail \gets 1$, $queue[1] \gets s$, mark all vertices as ``unvisited''
							\State Mark $s$ as ``visited''
							\While {$head \ge tail$}
								\State $v \gets queue[tail]$, $tail \gets tail + 1$
								\For {$\forall u \in d_v$}
									\If {$u$ is ``unvisited''}
										\State $head \gets head + 1$, $queue[head] = u$
										\State Mark $u$ as ``visited''
									\EndIf
								\EndFor
							\EndWhile
						\end{algorithmic}
					\end{algorithm}

					The running time of BFS is $O(n + m)$

					If we only want to know if $s$ and $t$ are connected, the algorithm can be terminated if $u = t$.

				\subsection{Cycle detection}
					The following algorithm is for connected graph, if the graph is not connected, run the algorithm for each component until cycle is detected or all the components have been calculated. Since the complexity for running in connected graph is $O(n + m)$, $n$ as the number of vertices/nodes, and $m$ as the number of edges/arcs, the running time of disconnected graph is the \textbf{summation} of running time in each component, where each component is connected. Therefore the complexity is the same in disconnected graph as in connected graph.

					The main idea is starting with arbitrary vertex/node, using DFS or BFS to search on the graph try to revisit the vertex/node we start with. If succeed, a cycle is detected, otherwise if all the vertices/nodes has been visited, then no cycle exists. And in linked-list representation, the complexity is $O(|V| + |E|)$, i.e. $O(n + m)$. However, there is slightly different in undirected graph and directed graph, for undirected graph needs at least three vertices to form a cycle while directed graph needs at least two.

					Here is the detail algorithm (DFS) for undirected graph:
					\begin{algorithm}[H]
						\caption{Main algorithm}
						\begin{algorithmic}[1]
							\State For all nodes, labeled as ``unvisited''
							\State Arbitrary choose a vertex $v$, add a dummy vertex $w$, add a dummy edge $(w, v)$, label $w$ as ``visited''
							\State run $DFS(w, v)$
							\State Remove dummy vertex $w$ and dummy edge $(w, v)$
							\If {$DFS(w, v)$ returns ``Cycle is found''}
								\State \Return ``Cycle is found''
							\Else
								\State \Return ``No cycle detected''
							\EndIf
						\end{algorithmic}
					\end{algorithm}

					\begin{algorithm}[H]
						\caption{DFS(w, v)}
						\begin{algorithmic}[1]
							\State Label $v$ as ``visited''
							\If {number of $v$ 's neighbor is 1}
								\State \Return null
							\Else
								\For {all neighbor $u$ in linked-list of $v$ excepts $w$}
									\If {$u$ is labeled as ``visited''} 
										\State \Return ``Cycle is found'' 
									\Else
										\State run $DFS(v, u)$
									\EndIf
								\EndFor
							\EndIf
						\end{algorithmic}
					\end{algorithm}

					Now check the complexity. Denote $v\in N$ as a node in graph $G$, total number of nodes denoted by $n$, denote $d_v$ as number of neighbors of node $v$. The complexity of $DFS(w, v)$ is $O(d_v)$ for each node $v$ it visited (it should be $O(v)$ because we need $O(1)$ to check if a node is $w$), each node can only be visited, by ``visited'' it means $DFS(w, v)$ is executed, at most once, which is controlled by the ``visited'' label. The total complexity is $O(n + \sum_{v\in N} d_v) = O(n + m)$

					For directed graph, we assume in the linked-list of each node, 1) it only contains the vertices where the node leading out to, or 2) we can distinguish the vertices it leading out to in $O(1)$. Here is the detail algorithm (DFS) for directed graph:
					\begin{algorithm}[H]
						\caption{Main algorithm}
						\begin{algorithmic}[1]
							\State For all nodes, labeled as ``unvisited''
							\State Arbitrary choose a node $v$, run $DFS(v)$
							\If {$DFS(v)$ returns ``Cycle is found''}
								\State \Return ``Cycle is found''
							\Else
								\State \Return ``No cycle detected''
							\EndIf
						\end{algorithmic}
					\end{algorithm}

					\begin{algorithm}[H]
						\caption{DFS(v)}
						\begin{algorithmic}[1]
							\State Label $v$ as ``visited''
							\If {number of vertices $v$ leading out to is 0}
								\State \Return null
							\Else
								\For {all leading out vertices $u$ in linked-list of $v$}
									\If {$u$ is labeled as ``visited''} 
										\State \Return ``Cycle is found'' 
									\Else
										\State run $DFS(u)$
									\EndIf
								\EndFor
							\EndIf
						\end{algorithmic}
					\end{algorithm}

					Now check the complexity. Similar to undirected case, denote $v\in N$ as a node in graph $G$, total number of nodes denoted by $n$, denote $d_v$ as number of edges leading out from node $v$. The complexity of $DFS(v)$ is $O(d_v)$ for each node $v$ it visited, each node can only be visited, by ``visited'' it means $DFS(v)$ is executed, at most once, which is controlled by the ``visited'' label. The total complexity is $O(n + \sum_{v\in N} d_v) = O(n + m)$

					Also, for undirected (connected) graph, if we don't need to find the cycle and only need to decide if there is a cycle or not, we can just check the number of vertices, $n$, and number of edges, $m$. If $n \le m$ then $G$ contains a cycle, otherwise no cycle, the complexity is $O(1)$.

				\subsection{Test Bipartiteness}
					We have proved that a bipartite graph only has even cycles, and the graph with only even cycles are bipartite graph, however, that is not very continent to test if a graph is bipartite because it needs to enumerate all cycles.

					The other idea to test bipartiteness is try to color the vertices of the graph, if it can be 2-colored, then the graph is bipartite, otherwise it is not.

					The following is the algorithm (using BFS)
					\begin{algorithm}[h]
						\caption{Test Bipartiteness}
						\begin{algorithmic}[1]
							\State Initialize, $head \gets 1$, $tail \gets 1$, $queue[1] \gets s$, mark all vertices as ``unvisited''
							\State Mark $s$ as ``visited''
							\State $color[s] \gets 0$
							\While {$head \ge tail$}
								\State $v \gets queue[tail]$, $tail \gets tail + 1$
								\For {$\forall u \in d_v$}
									\If {$u$ is ``unvisited''}
										\State $head \gets head + 1$, $queue[head] = u$
										\State Mark $u$ as ``visited''
										\State $color[u] \gets 1 - color[v]$
									\Else
										\If {$color[u] == color[v]$}
											\Return False
										\EndIf
									\EndIf
								\EndFor
							\EndWhile
							\Return True
						\end{algorithmic}
					\end{algorithm}

				\subsection{Topological Ordering}
					The topological ordering problem is given a directed acyclic graph $G = (V, E)$, output a 1-to-1 function $\pi: V\rightarrow\{1, 2, \cdots, n\}$ so that if $(u, v) \in E$, $\pi(u) < \pi(v)$

					The idea is each time take a vertex without incoming edges, then remove the vertex and all its outgoing edges. To make the algorithm efficient, we can 1) use linked lists of outgoing edges; 2) maintain the in degree $d_v$ of vertices; 3) maintain a queue (or stack) of vertices $v$ with $d_v = 0$

					The following is the algorithm
					\begin{algorithm}[h]
						\caption{topological-sort(G)}
						\begin{algorithmic}[1]
							\State Initialize, let $\forall v\in V, d_v = 0$
							\For {$v \in V$}
								\For {$u, (u, v) \in E$}
									\State $d_u \gets d_u + 1$
								\EndFor
							\EndFor
							\State $S \gets \{v: d_v = 0\}, i\gets 0$
							\While {$S\neq \emptyset$}
								\State $v \gets v \in S$, $S \gets S \setminus \{v\}$
								\State $i \gets i + 1, \pi(v) \gets i$
								\For {$u, (u, v) \in E$}
									\State $d_u \gets d_u - 1$
									\If {$d_u = 0$}
										\State $S \gets S \cup \{u\}$
									\EndIf
								\EndFor
							\EndWhile
							\If {$i < n$}
								\Return Not directed acyclic graph
							\EndIf
						\end{algorithmic}
					\end{algorithm}

					Running time $O(n + m)$
				\subsection{Bridge}
					\begin{definition}[Tree Edge, Cross Edge, Vertical Edge]
						Given a graph $G = (V, E)$ and a rooted tree $T$ in $G$, edge in $G$ can be classified by three types:
						\begin{itemize}
							\item Tree edges: edge in $T$
							\item Cross edges: $(u, v)$, $u$ and $v$ do not have an ancestor-descendant relation
							\item Vertical edges: $(u, v)$, $u$ is an ancestor of $v$, or $v$ is an ancestor of $u$
						\end{itemize}
						\begin{figure}[h]
							\centering
							\begin{tikzpicture}[node distance=2 cm]
								\node (1) [circleNode] {1};
								\node (2) [circleNode, below of=1, xshift = -2 cm] {2};
								\node (3) [circleNode, below of=1] {3};
								\node (4) [circleNode, below of=1, xshift = 2 cm] {4};
								\node (5) [circleNode, below of=2, xshift = -1 cm] {5};
								\node (6) [circleNode, below of=2, xshift = 1 cm] {6};
								\node (7) [circleNode, below of=4] {7};
								\draw [link] (1) -- node [left] {tree edge} (2);
								\draw [link] (1) -- (3);
								\draw [link] (1) -- (4);
								\draw [link] (2) -- (5);
								\draw [link] (2) -- (6);
								\draw [link] (4) -- (7);
								\draw [link] (3) -- node [right] {cross edge} (6);
								\draw [link] (1) to [out = 150, in = 150] node [left] {vertical edge} (5);
							\end{tikzpicture}
						\end{figure}
					\end{definition}

					In a BFS tree $T$ of a graph $G$, there can not be vertical edges, there cannot be cross edges $(u, v)$ with $u$ and $v$ 2 levels apart. (Cross edge at most 1 level apart)

					In a DFS tree $T$ of a graph $G$, there can not be cross edges, there can only be tree edges and vertical edges.

					\begin{definition}[bridge]
						Given a connected graph $G=(V, E)$, an edge $e \in E$ is called a \textbf{bridge} if the graph $G=(V, E\setminus \{e\})$ is disconnected.
					\end{definition}

					The idea to find bridge is through a DFS tree. Notice that there are only tree edges and vertical edges in DFS tree. Vertical edges are not bridges, a tree edge $(u, v)$ is not a bridge if some vertical edge jumping from below $u$ to above $v$. Other tree edges are bridges.

					Define $level(v)$ as the level of vertex $v$ in DFS tree. $T_v$ as the sub tree rooted at $v$, $h(v)$ as the smallest level that can be reached using a vertical edge from vertices in $T_v$. $(parent(u), u)$ is a bridge if $h(u) \ge level(u)$. The algorithm is as following:
					\begin{algorithm}[h]
						\caption{FindBridge(G)}
						\begin{algorithmic}[1]
							\State Mark all vertices as ``unvisited''
							\For {$v \in V$}
								\If {$v$ is ``unvisited''}
									\State $level(v) \gets 0$
									\State \texttt{RecursiveDFS(v)}
								\EndIf
							\EndFor
						\end{algorithmic}
					\end{algorithm}

					\begin{algorithm}[h]
						\caption{RecursiveDFS(v)}
						\begin{algorithmic}[1]
							\State mark $v$ as ``visited''
							\State $h(v) \gets \infty$
							\For {$u \in d_v$}
								\If {$u$ is ``unvisited''}
									\State $level(u) \gets level(v) + 1$
									\State \texttt{RecursiveDFS(u)}
									\If {$h(u) \ge level(u)$}
										\State $(u, v)$ is a bridge
									\EndIf
									\If {$h(u) < h(v)$}
										\State $h(v) \gets h(u)$
									\EndIf
								\Else
									\If {$level(u) < level(v) - 1$ and $level(u) < h(v)$}
										\State $h(v) \gets level(u)$
									\EndIf
								\EndIf
							\EndFor
						\end{algorithmic}
					\end{algorithm}
			\section{Blocks}

		\chapter{Euler Tours and Hamilton Cycles}
			\section{Euler Tours}

			\section{Hamilton Cycles}

			\section{The Chinese Postman Problem}

			\section{The Traveling Salesman Problem}

		\chapter{Matriod, Planarity}
			\section{Plane and Planar Graphs}

			\section{Dual Graphs}

			\section{Matroids}
				\begin{definition}[Matroids]
					Let $S$ be a finite set of \textbf{elements} and let $d$ be a collection of subsets of $S$ satisfying the property
					\begin{equation}
						\text{If } x \le y, y\in d, \Rightarrow x \in d
					\end{equation}
					The pair $(S, d)$ is called an \textbf{independent system} and the members of $d$ are called \textbf{independent sets}.
				\end{definition}

				\begin{example}
					Let $G$ be a graph and let $S \in E(G)$ define $M\subseteq S$ to be independent if $M$ is a matching
				\end{example}

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\node (1) [circleNode] {1};
						\node (2) [circleNode, right of=1] {2};
						\node (3) [circleNode, right of=2] {3};
						\node (5) [circleNode, right of=3] {5};
						\node (4) [circleNode, below of=3] {4};
						\node (6) [circleNode, below of=5] {5};
						\draw [link] (1) -- (2);
						\draw [link] (2) -- (3);
						\draw [link] (2) -- (4);
						\draw [link] (3) -- (5);
						\draw [link] (4) -- (6);
						\draw [link] (5) -- (6);
					\end{tikzpicture}
				\end{figure}

				\begin{align}
					S &= \{(1, 2), (2, 3), (2, 4), (3, 5), (4, 6), (5, 6)\} \\
					d &= \{\emptyset, \{(1, 2)\}, \{(2, 3)\}, ... , \{\text{other matching...}\}\}
				\end{align}

				\begin{example}
					Let $G$ be a graph and let $S = V(G)$ define $X \subseteq S$ to be independent if no two member of $x$ are adjacent.
				\end{example}

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\node (1) [circleNode] {1};
						\node (2) [circleNode, right of=1] {2};
						\node (3) [circleNode, right of=2] {3};
						\node (4) [circleNode, below of=2] {4};
						\draw [link] (1) -- (2);
						\draw [link] (2) -- (3);
						\draw [link] (2) -- (4);
					\end{tikzpicture}
				\end{figure}

				\begin{align}
					S &= \{1, 2, 3, 4\} \\
					d &= \{\emptyset, 1, 2, 3, 4, (1, 3), (1, 4), (3, 4), (1, 3, 4)\}
				\end{align}

				\begin{example}
					Let $G$ be a connected graph and let $S = E(G)$, define $X \subseteq S$ to be independent if $G[X]$ contains cycles.
				\end{example}

				Given any independent system, there is a natural combinatorial optimization problem of finding the maximum cardinality independent set. 

				Let's try the following: \textbf{Greedy algorithm}

				Step 1: Set $I=\emptyset$\\
				Step 2: If there exists $e\in S \setminus I$ such that $I + e$ is independent, set $I \leftarrow I+e$ and go to Step 1, otherwise stop.

				Those Independent systems for which the greedy algorithm guarantee to find a maximum cardinality independent set are very special called \textbf{matroids}

			\section{Independent Sets}

			\section{Ramsey's Theorem}

			\section{Tur\'{a}n's Theorem}

			\section{Schur's Theorem}

			\section{Euler's Formula}

			\section{Bridges}

			\section{Kuratowski's Theorem}

			\section{Four-Color Theorem}

			\section{Graphs on other surfaces}

		\chapter{Matchings}
			\section{Maximum Matching}
				\begin{definition}[Matching]
					Let $G = (V, E)$ be a graph, a \textbf{matching} is a subset of edges $M \subseteq E$ such that no two elements of $M$ are adjacent. The two ends of an edge in $M$ are said to be \textbf{matched under} $M$. A matching $M$ saturates a vertex $v$, and $v$ is said to be \textbf{M-saturated} or \textbf{M-covered}, if some edge of $M$ is incident with $v$. Otherwise, $v$ is \textbf{M-unsaturated} or \textbf{M-exposed}.
				\end{definition}

				\begin{definition}[Perfect matching, Maximum matching]
					If every vertex of $G$ is M-saturated, then the matching is said to be \textbf{perfect matching}. $M$ is a \textbf{maximum matching} if $G$ has no matching $M^\prime$ with $|M^\prime| > |M|$. Every perfect matching is maximum. The maximum matching does not necessarily to be perfect. Perfect matching and maximum matching may not be unique.
				\end{definition}

				\begin{definition}[M-alternating]
					An \textbf{M-alternating} path in $G$ is a path whose edges are alternately in $E\setminus M$ and $M$.
				\end{definition}

				\begin{definition}[M-augmenting]
					An \textbf{M-augmenting} path in $G$ is an $M$-alternating path whose origin and terminus are $M$-unsaturated.
				\end{definition}

				\begin{lemma}
					Every augmenting path $P$ has property that let $M^\prime = P\Delta M = (M \cup P) \setminus (M \cap P)$ then $M^\prime$ contains one more edge then $M$
				\end{lemma}

				The following path is an $M$-augmenting path
				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}
						\node (0) [solidNode] {};
						\node (1) [solidNode, right of=0] {};
						\node (2) [solidNode, right of=1] {};
						\node (3) [solidNode, right of=2] {};
						\node (4) [solidNode, right of=3] {};
						\node (5) [solidNode, right of=4] {};
						\draw (0) [link] -- (1);
						\draw (1) [matchedLink] -- (2);
						\draw (2) [link] -- (3);
						\draw (3) [matchedLink] -- (4);
						\draw (4) [link] -- (5);
					\end{tikzpicture}
				\end{figure}

				The following path is $M^\prime = P\Delta M (M \cup P) \setminus (M \cap P)$ and all the vertices are $M$-saturated.

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}
						\node (0) [solidNode] {};
						\node (1) [solidNode, right of=0] {};
						\node (2) [solidNode, right of=1] {};
						\node (3) [solidNode, right of=2] {};
						\node (4) [solidNode, right of=3] {};
						\node (5) [solidNode, right of=4] {};
						\draw (0) [matchedLink] -- (1);
						\draw (1) [link] -- (2);
						\draw (2) [matchedLink] -- (3);
						\draw (3) [link] -- (4);
						\draw (4) [matchedLink] -- (5);
					\end{tikzpicture}
				\end{figure}

				\begin{theorem}[Berge, 1957]
					A matching $M$ in a graph $G$ is maximum iff $G$ has no M-augmenting path.
				\end{theorem}

				\begin{proof}
					($\Rightarrow$) It is clear that if $M$ is maximum, it has no augmenting paths since otherwise by problem claim we can increase by one.

					($\Leftarrow$) Suppose $M$ is not maximum and let $M^\prime$ be a bigger matching. Let $A = M \Delta M^\prime$ now no vertex of $G$ is incident to more than two members of $A$. For otherwise either two members of $M$ or two members of $M^\prime$ would be adjacent. Contradict the definition of matching. It follows that every component of the edges incident subgraph $G[A]$ is either an even cycle with edge augmenting in $M\Delta M^\prime$ or else $A$ path with edges alternating between $M$ and $M^\prime$.

					Since $|M^\prime| \ge |M|$ then the even cycle cannot help because exchanging $M$ and $M^\prime$ will have same cardinality.

					The path case implies that $p$ is alternating in $M$ and since $|M^\prime| > |M|$ the end arc exposed so that $p$ is augmenting.
				\end{proof}

				\begin{definition}[Vertex-cover]
					The \textbf{vertex-cover} is a subset of vertices $X$ such that every edge of $G$ is incident to some member of $X$.
				\end{definition}

				\begin{lemma}
					The cardinality of any matching is less than or equal to the cardinality of any vertex cover.
				\end{lemma}

				\begin{proof}
					Consider any matching. Any vertex cover must have nodes that at least incident to the edges in the matching. Since all the edges in the matching are disjointed, so for a single node can at most cover one edge in the matching. If the matching is not perfect, for the edges that not in the tree, they may or may not be possible to be covered by the nodes incident to the edges in the matching, with an easy triangle graph example, we can prove this lemma.
				\end{proof}

				\begin{theorem}[K\"onig Theorem]
					If $G$ is bipartite, the cardinality of the maximum matching is equal to the cardinality of the minimum vertex cover.
				\end{theorem}

				\begin{proof}
					Let $G$ be a bipartite graph, $G = (V, E)$ where $V = X\cup Y$ as $X$ and $Y$ are two disjointed sets of vertices. Let $M$ be a maximum matching on $G$. For each edge in $M$, denoted by $e_i = a_ib_i$ where $e_i \in M$, $a_i \in A$ and $b_i \in B$ and $A = \{a_i: e_i \in M\} \subseteq X$, and $B = \{b_i: e_i \in M\} \subseteq Y$. Therefore, we can partition $X$ by $A$ and $U = X\setminus A$, partition $Y$ by $B$ and $W = Y\setminus B$.

					We can further partition the matching $M$ into $M_1$ and $M_2$. For all the edges in $M_1$ can be included into an $M$-alternating path starts from a vertex in $U$ (which includes the edges directly linked to vertices in $U$), and $M_2 = M \setminus M_1$. For edges in $M_1$, we take the ends of edges in $B$ in the vertex cover, denoted by $B_1$, take the ends of edges in $A$ as a subset denoted by $A_1 \subseteq A$. For the edges in $M_2$, we take the ends of edges in $A$ in the vertex cover, denoted by $A_2$, and the ends of edges in $B$ as a subset denoted by $B_2 \subseteq B$. 

					We claim that all the vertices in $U$ can only be connected to vertices in $B_1$ and vertices in $W$ can only be connected to vertices in $A_2$.

					$U \subset X$ connects to vertices in $B_1$ by definition. If vertices in $W \subset Y$ is connected to vertices in $A_1$, then we will have $M$-augmenting path which is contradicted to the assumption that $M$ is maximum matching.
				\end{proof}

				The following is an example. Where the edge in the matching that accessible from members of $U = \{1, 2\}$ in an $M$-alternating path is edge $3a, 4b, 5c, 6d$.

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}
						\node (1) [solidNode, label=above:{1}] {};
						\node (2) [solidNode, right of = 1, label=above:{2}] {};
						\node (3) [solidNode, right of = 2, label=above:{3}] {};
						\node (4) [solidNode, right of = 3, label=above:{4}] {};
						\node (5) [solidNode, right of = 4, label=above:{5}] {};
						\node (6) [solidNode, right of = 5, label=above:{6}] {};
						\node (7) [solidNode, right of = 6, label=above:{7}] {};
						\node (8) [solidNode, right of = 7, label=above:{8}] {};
						\node (9) [solidNode, right of = 8, label=above:{9}] {};
						\node (a) [solidNode, below of = 3, yshift=-2 cm, label=below:{a}] {};
						\node (b) [solidNode, right of = a, label=below:{b}] {};
						\node (c) [solidNode, right of = b, label=below:{c}] {};
						\node (d) [solidNode, right of = c, label=below:{d}] {};
						\node (e) [solidNode, right of = d, label=below:{e}] {};
						\node (f) [solidNode, right of = e, label=below:{f}] {};
						\node (g) [solidNode, right of = f, label=below:{g}] {};
						\node (h) [solidNode, right of = g, label=below:{h}] {};
						\node (i) [solidNode, right of = h, label=below:{i}] {};
						\draw [matchedLink] (3) -- (a);
						\draw [matchedLink] (4) -- (b);
						\draw [matchedLink] (5) -- (c);
						\draw [matchedLink] (6) -- (d);
						\draw [matchedLink] (7) -- (e);
						\draw [matchedLink] (8) -- (f);
						\draw [matchedLink] (9) -- (g);
						\draw [link] (1) -- (b);
						\draw [link] (2) -- (a);
						\draw [link] (8) -- (i);
						\draw [link] (8) -- (h);
						\draw [link] (3) -- (b);
						\draw [link] (3) -- (d);
						\draw [link] (2) -- (d);
						\draw [link] (4) -- (c);
						\draw [link] (5) -- (a);
						\draw [link] (6) -- (b);
						\draw [link] (7) -- (b);
						\draw [link] (7) -- (d);
						\draw [link] (7) -- (h);
						\draw [link] (8) -- (e);
						\draw [link] (8) -- (a);
						\draw [link] (9) -- (h);
					\end{tikzpicture}
				\end{figure}

				In which $U = \{1, 2\}$, $M_1 = \{3a, 4b, 5c, 6d\}$. $U = \{1, 2,\}, A_1 = \{3, 4, 5, 6\}, A_2 = \{7, 8, 9\}, W=\{h, i\}, B_1 = \{a, b, c, d\}, B_2 = \{e, f, g\}$. The vertex cover is $\{a, b, c, d, 7, 8, 9\}$.

				The above theorem does not apply to non-bipartite graph. The following is an example
				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}
						\node (1) [solidNode, label=above:{1}] {};
						\node (2) [solidNode, below of = 1, xshift = -1 cm, label=below:{2(cover)}] {};
						\node (3) [solidNode, below of = 1, xshift = 1 cm, label=below:{3(cover)}] {};
						\draw [matchedLink] (2) -- (3);
						\draw [link] (1) -- (2);
						\draw [link] (1) -- (3);
					\end{tikzpicture}
				\end{figure}

				The maximum matching has one edge, where the minimum cover has two vertices.

			\section{Maximum Matching Algorithm}
				\begin{definition}[M-alternating tree]
					An \textbf{M-alternating tree} $T$ is a rooted tree satisfied the following condition:
					\begin{itemize}
						\item The root $r$ is $M$-unsaturated
						\item The unique path from $r$ to any vertex $T$ is $M$-alternating
						\item Every vertex in $T$, except $r$ is incident to a matching edge of $T$
					\end{itemize}
					A vertex $x$ of $T$ is called \textbf{inner} if $(r, s)$-path in $T$ has an odd number of edges. Otherwise $x$ is called \textbf{outer}.
				\end{definition}

				\begin{lemma}
					Let $M$ be a matching in $G$ and let $T$ be an $M$-alternating tree with root $r$, then the following conclusion hold
					\begin{itemize}
						\item If $v \neq r$ is an outer vertex and $p$ is the unique $(r,v)$-path in $T$ then the edge of $p$ incident to $v$ is in $M$
						\item The number of inner vertices in $T$ equals the number of matching edges in $T$.
					\end{itemize}
				\end{lemma}

				\begin{definition}[Alternating forest]
					An \textbf{alternating forest} is a forest of $G$ where every components is an alternating tree. An alternating forest $(F, e)$ is an alternating forest to be then with an edge $e = M, V$ where $M$ and $V$ are outer vertices contained in two distinct components of $F$.
				\end{definition}

				\begin{example}[Hungarian forest]
					A \textbf{Hungarian forest} $F$ is an alternating forest containing all exposed vertices of $G$ and such that the outer vertices of $F$ are adjacent in $G$ only to inner vertices of $F$
				\end{example}

				\begin{definition}[Augmenting forest]
					An \textbf{augmenting forest} $(F, e)$ where $F$ is an augmenting forest and $e = uv$ connects two outer vertices in distinct components of $F$.
				\end{definition}

				The plan is to grow an alternating forest that eventually become augmenting or Hungarian. Augmenting forest will increase the cardinality of the match, Hungarian implies that you have found optimal maximum cardinality matching.

				\begin{theorem}
					Let $(F, e)$ be an augmenting forest. And let $T_1$ and $T_2$ be the two components of $F$ containing an end of $e$. Let $p_i$ be the unique path in $T_i$ from the root to the end of $e$, then $p_1ep_2^{-1}$ is an augmenting path.
				\end{theorem}

				\begin{theorem}
					If $F$ is a Hungarian forest for some matching $M$ then $M$ is a maximum match.
				\end{theorem}

				The above theorems suggest a method for computing maximum matching. Let $M$ be a matching of $G$ and let $F$ be an alternating forest in $G$ made up of all M-exposing vertices. If $F$ happens to be Hungarian, stop with the max matching $M$. If $(F, e)$ for some $e$ is augmenting then we increase our matching by 1 and start process.

				Suppose $F$ is neither Hungarian nor augmenting, by definition, there must be an edge $e$ incident to an outer vertex of $F$ to no inner vertex of $F$. But $e$ cannot be incident to two outer vertices of $F$ in distinct components, since $(F, e)$ is not augmenting. Hence there are only two cases:

				Case 1: $e=uv$ where $u$ is outer in $F$ and $v$ is not outer in $F$. The only way its possible if $v$ is covered. Augmenting $F$ to $M$ will increase the matching. 

				Case 2: $e=uv$ where $u$ and $v$ are outer vertices in $F$. Let $r$ be the root of the component of $F$ containing $u$ and $v$, let $p_u$ and $p_v$ be $(r,u)$-path and $(r, v)$-path in $F$. Let $b$ be the last vertex these two paths have in common and let $p$ be the $(u, v)$-path in $F$, let $p_u^\prime$ be $(b-u)$-path and $p_v^\prime$ be the $(b,v)$-path respectively. Let $n$ be the length of $p_u$, let $m$ be the length of $p_v$, let k be the length of $(r, b)$-path. Let $c$ be the cycle $(p_u^\prime e {p_v^{\prime}}^{-1})$. Number of vertices in $c$ is $n + m-2k + 1$, $n, m$ are even, so $c$ is always an odd length cycle. If $G$ has no odd cycles, we call those graph bipartite. To this case can not happen in bipartite graph so algorithm without case 2 will solve bipartite matching.

				If a graph has no odd cycles, i.e., bipartite, then we have an algorithm using augmenting forest and Hungarian forest and case 1.

				We now have to deal with odd cycles. The idea is to "shrink" add cycles to a super node

				Let $S\subseteq E(G)$, denote by $G: S$ the subgraph with edge set $S$
				\begin{equation}
					G:S = G\setminus (E(G)-S)
				\end{equation}
				The contraction of $S$ to be the $G\setminus S$ with $E(G / S) = E(G) - S$. $V(G/S)$ to be the components of $G:S$ and if $e\in E(G/S)$ then the ends of $e$ in $G/S$ are in components of $G:S$ containing both ends in $G$

				Let network to general case 2. $b$ is outer vertex. Let $B$ be the set of edges of cycle $C$, we call $B$ a \textbf{blossom}. We propose to replace $M$ by $M - B$, $G$ by $G/B$ and $F$ by $F/B$

			\section{Edmonds's Blossom Algorithm}


				$O(|V|^4)$
				- Non-bipartite matching is one of very few problems in $P$, for which LP relaxation will not provide optimal solution.

			\section{Hall's Marriage Theorem}

			\section{Transversal Theory}

			\section{Menger's Theorem}

			\section{The Hungarian Algorithm}

		\chapter{Colorings}
			\section{Edge Chromatic Number}

			\section{Vizing's Theorem}

			\section{The Timetabling Problem}

			\section{Vertex Chromatic Number}

			\section{Brooks' Theorem}

			\section{Haj\'{o}s' Theorem}

			\section{Chromatic Polynomials}

			\section{Girth and Chromatic Number}

		\chapter{Minimum Spanning Tree Problem}
			\section{Basic Concepts}
				\begin{example}
					A company wants to build a communication network for their offices. For a link between office $v$ and office $w$, there is a cost $c_{vw}$. If an office is connected to another office, then they are connected to with all its neighbors. Company wants to minimize the cost of communication networks.
				\end{example}

				\begin{definition}[Cut vertex]
					A vertex $v$ of a connected graph $G$ is a \textbf{cut vertex} if $G\setminus v$ is not connected.
				\end{definition}

				\begin{definition}[Connection problem]
					Given a connected graph $G$ and a positive cost $C_e$ for each $e\in E$, find a minimum-cost spanning connnected subgraph of $G$. (Cycles all allowed)
				\end{definition}

				\begin{lemma}
					An edge $e = uv \in G$ is an edge of a cycle of $G$ iff there is a path $G\setminus e$ from $u$ to $v$.
				\end{lemma}

				\begin{definition}[Minumum spanning tree problem]
					Given a connected graph graph $G$, and a cost $C_e, \forall e\in E$, find a minimum cost spanning tree of $G$
				\end{definition}

				The only way a connection problem will be different than MSP is if we relax the restriction on $C_e > 0$ in the connection problem.

			\section{Kroskal's Algorithm}
				\begin{algorithm}
					\caption{Kroskal's Algorithm, $O(m \log m)$}
					\begin{algorithmic}
						\Require A connected graph
						\Ensure A MST
						\State Keep a spanning forest $H=(V, F)$ of $G$, with $F=\emptyset$
						\While {$|F| < |V| - 1$}
							\State add to $F$ a least-cost edge $e\notin F$ such that $H$ remains a forest.
						\EndWhile
					\end{algorithmic}
				\end{algorithm}

			\section{Prim's Algorithm}
				\begin{algorithm}
					\caption{Prim's Algorithm, $O(nm)$}
					\begin{algorithmic}
						\Require A connected graph
						\Ensure A MST
						\State Keep $H = (V(H), T)$ with $V(H) = \{v\}$, where $r\in V(G)$ and $T=\emptyset$
						\While {$|V(T)| < |V|$}
							\State Add to $T$ a least-cost edge $e \notin T$ such that $H$ remains a tree.
						\EndWhile
					\end{algorithmic}
				\end{algorithm}

			\section{Extensible MST}
				\begin{definition}[cut]
					For a graph $G=(V, E)$ and $A \subseteq V$ we denote $\delta(A) = \{e \in E :\text{$e$ has an end in $A$ and an end in $V\setminus A$}\}$. A set of the form $\delta(A)$ for some $A$ is called a \textbf{cut} of $G$.
				\end{definition}

				\begin{definition}
					We also define $\gamma(A) = \{e\in E: \text{both ends of $e$ are in $A$}\}$
				\end{definition}

				\begin{theorem}
					A graph $G=(V, E)$ is connected iff there is no $A\subseteq V$ such that $\emptyset \ne A \ne V$ with $\delta(A) = \emptyset$
				\end{theorem}

				\begin{definition}
					Let us call a subset $A \in E$ \textbf{extensible} to a minimum spanning tree problem if $A$ is contained in the edge set of some MST of $G$
				\end{definition}

				\begin{theorem}
					Suppose $B \subseteq E$, that $B$ is extensible to an MST and that $e$ is a minimum cost edge of some cut $D$ satisfying $D\cap B = \emptyset$, then $B\cup \{e\}$ is extensible to an MST.
				\end{theorem}

			\section{Solve MST in LP}
				Given a connected graph $G=(V, E)$ and a cost on the edges $C_e$ for all $e\in E$, Then we can formulate the following LP
				\begin{equation}
					X_e = \begin{cases}
						1, \text{if edge $e$ is in the optimal solution} \\
						0, \text{otherwise}
					\end{cases}
				\end{equation}

				The formulation is as following
				\begin{align}
					\min \quad & \sum_{e\in E} c_ex_e \\
					\text{s.t.} \quad & \sum_{e\in E} x_e = |V| - 1 \\
					                  & x_e \ge 0\\
					                  & e\in E \\
					                  & \sum_{e\in E(S)} x_e = |S| - 1, \forall S\subseteq V, S\ne \emptyset \\
				\end{align}

		\chapter{Shortest-Path Problem}
			\section{Basic Concepts}
				All Shortest-Path methods are based on the same concept, suppose we know there exists a dipath from $r$ to $v$ of a cost $y_v$. For each vertex $v \in V$ and we find an arc $(v, w) \in E$ satisfying $y_v + v_{vw} < y_w$. Since appending $(v, w)$ to the dipath to $v$ takes a cheaper dipath to $w$ then we can update $y_w$ to a lower cost dipath.

				\begin{definition}[feasible potential]
					We call $y = (y_v: v\in V)$ a \textbf{feasible potential} if it satisfies
					\begin{equation}
						y_v + c_{vw} \ge y_w \quad \forall (v, w) \in E
					\end{equation}
					and $y_r = 0$
				\end{definition}

				\begin{proposition}
					Feasible potential provides lower bound for the shortest path cost.
				\end{proposition}

				\begin{proof}
					Suppose that you have a dipath $P = v_0e_1v_1,...,e_kv_k$ where $v_0 = r$ and $v_k = v$, then
					\begin{equation}
						C(P) = \sum_{i=1}^k C_{e_i} \ge \sum_{i=1}^k(y_{v_i} - y_{v_{i-1}}) = y_{v_k} - y_{v_0}
					\end{equation}
				\end{proof}

			\section{Breadth-First Search Algorithm}

			\section{Ford's Method}
				Define a predecessor function $P(w), \forall w \in V$ and set $P(w)$ to $v$ whenever $y_w$ is set to $y_v + c_{vw}$

				\begin{algorithm}
					\caption{Ford's Method}
					\begin{algorithmic}
						\Ensure Shortest Paths from $r$ to all other nodes in $V$
						\Require A digraph with arc costs,starting node $r$
						\State Initialize, $y_r = 0$ and $y_v = \infty, v\in V\setminus r$
						\State Initialize, $P(r) = 0, P(v) = -1, \forall v \in V \setminus r$
						\While {$\mathbf{y}$ is not a feasible potential}
							\State Let $e = (v, w)\in E$ (this could be problematic)
							\If {$y_v + c_{vw} < y_w$ (incorrect)}
								\State $y_w \gets y_v + c_{vw}$ (correct it)
								\State $P(w) = v$ (set $v$ as predecessor)
							\EndIf
						\EndWhile
					\end{algorithmic}
				\end{algorithm}

				\notice{Technically speaking, this is not an algorithm, for the following reasons: 1) We did not specify how to pick $e$, 2) This procedure might not stop given some situations, e.g., if there is a cycle with minus total weight}

				\notice{This method can be really bad. Here is another example that could take $O(2^n)$ to solve.}
				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 1 cm]
						\node (0) [circleNode] {$v_0$};
						\node (1) [circleNode, right of = 0] {$v_1$};
						\node (2) [circleNode, right of = 1] {$v_2$};
						\node (3) [circleNode, right of = 2] {$v_3$};
						\node (4) [circleNode, right of = 3] {$v_4$};
						\node (5) [circleNode, right of = 4] {$v_5$};
						\node (6) [circleNode, right of = 5] {$v_6$};
						\draw [arrow] (0) -- node [below] {4} (1);
						\draw [arrow] (1) -- node [below] {4} (2);
						\draw [arrow] (2) -- node [below] {2} (3);
						\draw [arrow] (3) -- node [below] {2} (4);
						\draw [arrow] (4) -- node [below] {1} (5);
						\draw [arrow] (5) -- node [below] {1} (6);
						\draw [arrow] (0) to [out = 30, in = 150] node [above] {4} (2);
						\draw [arrow] (2) to [out = 30, in = 150] node [above] {2} (4);
						\draw [arrow] (4) to [out = 30, in = 150] node [above] {1} (6);
					\end{tikzpicture}
				\end{figure}

			\section{Ford-Bellman Algorithm}
				\begin{algorithm}
					\caption{Ford-Bellman Algorithm}
					\begin{algorithmic}
						\Ensure Shortest Paths from $r$ to all other nodes in $V$
						\Require A digraph with arc costs,starting node $r$
						\State Initialize $y$ and $p$
						\For {$i = 0; i < N; i++$}
							\For {$\forall e = (v, w) \in E$}
								\If {$y_v + c_{vw} < y_w$ (incorrect)}
									\State $y_w \gets y_v + c_{vw}$ (correct it)
									\State $P(w) = v$ (set $v$ as predecessor)
								\EndIf
							\EndFor
						\EndFor
						\For {$\forall e = (v, w) \in E$}
							\If {$y_v + c_{vw} < y_w$ (incorrect)}
								\State Return error, negative cycle
							\EndIf
						\EndFor
					\end{algorithmic}
				\end{algorithm}
				\notice{Only correct the node that comes from a node that has been corrected.}

				A usual representation of a digraph is to store all the arcs having tail $v$ in a list $L_v$ to \textbf{scan} $v$ means the following:
				\begin{itemize}
					\item For $(v, w) \in L_v$, if $(v, w)$ is incorrect, then correct $(v, w)$
				\end{itemize}

				For Bellman, will either terminate with shortest path from $r$ to all $v\in V\setminus r$ or it will terminate stating that there is a negative cycle. In $O(mn)$

				In the algorithm if $i = n$ and there exists a feasible potential, the problem has a negative cycle.

				Suppose that the nodes of $G$ can be ordered from left to right so that all arcs go from left to right. That is suppose there is an ordering $v_1, v_2, ..., v_n \in V$ so that $(v_i, v_j) \in V$ implies $i < j$. We call such an ordering \textbf{topological} sort.

				If we order $E$ in the sequence that $v_iv_j$ precedes $v_kv_i$ if $i<k$ based on topological order then ford algorithm will terminate in one pass.

			\section{SPFA Algorithm}

			\section{Dijkstra Algorithm}
				\begin{algorithm}
					\caption{Dijkstra Algorithm}
					\begin{algorithmic}
						\Ensure Shortest Paths from $r$ to all other nodes in $V$
						\Require A digraph with arc costs,starting node $r$
						\State Initialize $y$ and $p$
						\State $S \gets V$
						\While {$S \ne \emptyset$}
							\State Choose $v \in S$ with minimum $y_v$
							\State $S \gets S\setminus v$
							\For {$\forall w, (v, w) \in E$}
								\If {$y_v + c_{vw} < y_w$ (incorrect)}
									\State $y_w \gets y_v + c_{vw}$ (correct it)
									\State $P(w) = v$ (set $v$ as predecessor)
								\EndIf
							\EndFor
						\EndWhile
					\end{algorithmic}
				\end{algorithm}

			\section{A* Algorithm}

			\section{Floyd-Warshall Algorithm}
				If all weights/distances in the graph are nonnegative then we could use Dijkstra within starting nodes being any one of the vertices of the graph. This method will take $O(n^3)$

				If weight/distances are arbitrary and we would like to find shortest path between all pairs of vertices or detect a negative cycle we could use Bellman-Ford Algorithm with $O(n^4)$

				We would like an algorithm to find shortest path between any two pairs in a graph for arbitrary weights (determined, negative, cycles) in $O(n^3)$

				Let $d_{ij}^k$ denote the length of the shortest path from $i$ to $j$ such that all intermediate vertices are contained in the set $\{1, ..., k\}$

				\begin{figure}
					\centering
					\begin{tikzpicture}[node distance = 1 cm]
						\node (2) [circleNode] {2};
						\node (1) [circleNode, below of=2, xshift = -1 cm] {1};
						\node (3) [circleNode, below of=2] {3};
						\node (4) [circleNode, below of=2, xshift = 1 cm] {4};
						\node (5) [circleNode, below of=3] {5};
						\draw (1) -- node [above] {1} (2);
						\draw (1) -- node [above] {2} (3);
						\draw (1) -- node [above] {4} (5);
						\draw (2) -- node [left] {2} (3);
						\draw (3) -- node [left] {-3} (5);
						\draw (2) -- node [above] {5} (4);
						\draw (3) -- node [above] {3} (4);
						\draw (5) -- node [above] {1} (4);
					\end{tikzpicture}
				\end{figure}

				In this case $d_{14}^5 = 5$

				If the vertex $k$ is not an intermediate vertex on $p$, then $d_{ij}^k = d_{ij}^{k-1}$, notice that $d_{15}^4 = -1$, node 4 is not intermediate, so $d_{15}^3 = -1$

				If the vertex $k$ is an intermediate on $p$, then $d_{ij}^k = d_{ik}^{k-1} + d_{kj}^{k-1}$, $d_{14}^5 = 0$ ($p=1\rightarrow3\rightarrow5\rightarrow4$), i.e., $d_{14}^5 = d_{15}^4 + d_{54}^4 = 0$

				Therefore $d_{ij}^k = \min\{d_{ij}^{k-1}, d_{ik}^{k-1} + d_{kj}^{k-1}\}$


				Input: graph $G=(V, E)$ with weight on edges
				Output: Shortest path between all pairs of vertices on existence of a negative cycle
				Step 1: Initialize
				\begin{align}
					d_{ij}^0 = 
					\begin{cases}
						c_{ij} \quad \text{distance from } i \text{ to } j \text{ if } (i, j) \in E \\
						0 \quad \text{if } i = j \\
						\infty \quad \text{if } (i, j) \notin E
					\end{cases}
				\end{align}
				Step:
				For k = 1 to n
					For i = 1 to n
						For j = 1 to n
							$d_{ij}^k = \min\{d_{ij}^{k-1}, d_{ik}^{k-1} + d_{kj}^{k-1}\}$
						Next j
					Next i
				Next k
				Between optimal matrix $D^n$

				\begin{figure}
					\centering
					\begin{tikzpicture}[node distance = 2 cm]
						\node (2) [circleNode] {2};
						\node (1) [circleNode, below of=2, xshift = -2 cm] {1};
						\node (3) [circleNode, below of=2, xshift = 2 cm] {3};
						\node (4) [circleNode, below of=3, xshift = -1 cm] {4};
						\node (5) [circleNode, below of=1, xshift = 1 cm] {5};
						\draw [arrow] (1) -- node [above] {-4} (5);
						\draw [arrow] (1) -- node [above] {3} (2);
						\draw [arrow] (1) -- node [above] {8} (3);
						\draw [arrow] (4) -- node [above] {2} (1);
						\draw [arrow] (2) -- node [left] {1} (4);
						\draw [arrow] (2) -- node [left] {7} (5);
						\draw [arrow] (3) -- node [left] {4} (2);
						\draw [arrow] (4) -- node [left] {-5} (3);
						\draw [arrow] (5) -- node [left] {6} (4);
					\end{tikzpicture}
				\end{figure}

				\begin{equation}
					D^0 = \begin{bmatrix}
						0 & 3 & 8 &\infty & -4 \\
						\infty & 0  &\infty & 1 & 7 \\
						\infty & 4 & 0 &\infty &\infty\\
						2 &\infty & -5 & 0 &\infty\\
						\infty & \infty & \infty & 6 & 0
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					\Pi^0 = \begin{bmatrix}
						& 1 & 1 & & 1 \\
						& & & 2 & 2\\
						& 3 & & & \\
						4 & & 4 & & \\
						& & & 5 & \\
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					D^1 = \begin{bmatrix}
						0 & 3 & 8 &\infty & -4 \\
						\infty & 0  &\infty & 1 & 7 \\
						\infty & 4 & 0 &\infty &\infty\\
						2 & \mathbf{5} & -5 & 0 &\mathbf{-2}\\
						\infty & \infty & \infty & 6 & 0
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					\Pi^1 = \begin{bmatrix}
						& 1 & 1 & & 1 \\
						& & & 2 & 2\\
						& 3 & & & \\
						4 & \mathbf{1} & 4 & & \mathbf{1}\\
						& & & 5 & \\
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					D^2 = \begin{bmatrix}
						0 & 3 & 8 &\mathbf{4} & -4 \\
						\infty & 0  &\infty & 1 & 7 \\
						\infty & 4 & 0 &\mathbf{5} &\mathbf{11}\\
						2 & 5 & -5 & 0 &-2\\
						\infty & \infty & \infty & 6 & 0
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					\Pi^2 = \begin{bmatrix}
						& 1 & 1 & \mathbf{2} & 1 \\
						& & & 2 & 2\\
						& 3 & & \mathbf{2} & \mathbf{2} \\
						4 & 1 & 4 & & 1\\
						& & & 5 & \\
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					D^3 = \begin{bmatrix}
						0 & 3 & 8 & 4 & -4 \\
						\infty & 0  &\infty & 1 & 7 \\
						\infty & 4 & 0 & 5 & 11\\
						2 & \mathbf{-1} & -5 & 0 &-2\\
						\infty & \infty & \infty & 6 & 0
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					\Pi^3 = \begin{bmatrix}
						& 1 & 1 & 2 & 1 \\
						& & & 2 & 2\\
						& 3 & & 2 & 2 \\
						4 & \mathbf{3} & 4 & & 1\\
						& & & 5 & \\
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					D^4 = \begin{bmatrix}
						0 & 3 & \mathbf{-1} & 4 & -4 \\
						\mathbf{3} & 0  &\mathbf{-4} & 1 & \mathbf{-1} \\
						\mathbf{7} & 4 & 0 & 5 & \mathbf{3}\\
						2 & -1 & -5 & 0 &-2\\
						\mathbf{8} & \mathbf{5} & \mathbf{1} & 6 & 0
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					\Pi^4 = \begin{bmatrix}
						& 1 & \mathbf{4} & 2 & 1 \\
						\mathbf{4} & & \mathbf{4} & 2 & \mathbf{1}\\
						\mathbf{4} & 3 & & 2 & \mathbf{1} \\
						4 & 3 & 4 & & 1\\
						\mathbf{4} & \mathbf{3} & \mathbf{4} & 5 & \\
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					D^5 = \begin{bmatrix}
						0 & \mathbf{1} & \mathbf{-3} & \mathbf{2} & -4 \\
						3 & 0  & -4 & 1 & -1 \\
						7 & 4 & 0 & 5 & 3\\
						2 & -1 & -5 & 0 &-2\\
						8 & 5 & 1 & 6 & 0
					\end{bmatrix}
				\end{equation}

				\begin{equation}
					\Pi^5 = \begin{bmatrix}
						& \mathbf{3} & 4 & \mathbf{5} & 1 \\
						4 & & 4 & 2 & 1\\
						4 & 3 & & 2 & 1 \\
						4 & 3 & 4 & & 1\\
						4 & 3 & 4 & 5 & \\
					\end{bmatrix}
				\end{equation}

				Time complexity $O(n^3)$

				If during the previous processes, there exist an element of negative value in the diagonal, it means there exists negative cycle.

			\section{Johnson's Algorithm}

		\chapter{Maximum Flow Problem}
			\section{Basic Concept}
				Let $D=(V, A)$ be a strict diagraph with distinguished vertices $s$ and $t$. We call $s$ the source and $t$ the sink, let $u=\{u_e: e\in A\}$ be a nonnegative integer-valued capacity function defined on the arcs of $D$. The maximum flow problem on $(D, s, t, u)$ is the following Linear program.
				\begin{align}
					\max \quad & v\\
					\text{s.t.} \quad & \sum_{h(e)=i}x_e - \sum_{t(e) = i} x_e = \begin{cases}
						-v, \quad \text{if } i = s\\
						v, \quad \text{if } i = t \\
						0, \quad \text{otherwise}
					\end{cases}\\
					& 0\le x_e \le u_e, \quad \forall e\in A
				\end{align}
				We think of $x_e$ as being the flow on arc $e$. Constraint says that for $i \neq s, t$ the flow into a vertex has to be equal to the flow out of vertex. That is, flow is conceded at vertex $i$ for $i=s$ and for $i=t$ the net flow in the entire digraph must be equal to $v$.
				A $\mathbf{x_e}$ that satisfied the above constraints is an $(s,t)$-flow of value $v$. If in addition it satisfies the bounding constraints, then it is a feasible $(s,t)$-flow.
				A feasible $(s,t)$-flow that has maximum $v$ is optimal on maximum.

			\section{Solving Maximum Flow Problem in LP}
				\begin{theorem}
					For $S \subseteq V$ we define $(S, \bar{S})$ to be a $(s, t)$-cut if $s\in S$ and $t\in \bar{S}=V-S$, the capacity of the cut, denoted $u(S, \bar{S})$ as $\sum \{u_e: e\in \delta^-(S)\}$ where $\delta^-(S) = \{e\in A: t(e) \in S \text{ and } h(e) \in \bar{S}\}$
				\end{theorem}

				\begin{example}
					For the following graph:\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1 cm]
							\node (1) [circleNode] {1};
							\node (3) [circleNode, right of=1] {3};
							\node (s) [circleNode, below of=1, xshift = -1 cm] {s};
							\node (2) [circleNode, below of=s, xshift = 1 cm] {2};
							\node (t) [circleNode, below of=3, xshift = 1 cm] {t};
							\node (4) [circleNode, below of=t, xshift = -1 cm] {4};
							\draw [arrow] (s) -- node [above] {6} (1);
							\draw [arrow] (s) -- node [above] {3} (2);
							\draw [arrow] (1) -- node [right] {2} (2);
							\draw [arrow] (1) -- node [above] {5} (3);
							\draw [arrow] (2) -- node [above] {4} (4);
							\draw [arrow] (3) -- node [above] {3} (t);
							\draw [arrow] (4) -- node [above] {1} (t);
						\end{tikzpicture}
					\end{figure}
					Let $S = \{1, 2, 3, s\}$, $\bar{S} = \{4, t\}$\\
					then $\delta^-(S) = \{(2, 4), (3, t)\} \Rightarrow u(S, \bar{S}) = 7$
				\end{example}

				\begin{definition}
					If $(S, \bar{S})$ has minimum capacity of all $(s,t)$-cuts, then it is called \textbf{minimum cut}.
				\end{definition}

				\begin{definition}
					Let $\delta^+(S) = \delta^-(V-S)$
				\end{definition}

				\begin{example}
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1 cm]
							\node (1) [circleNode] {1};
							\node (3) [circleNode, right of=1] {3};
							\node (s) [circleNode, below of=1, xshift = -1 cm] {s};
							\node (2) [circleNode, below of=s, xshift = 1 cm] {2};
							\node (t) [circleNode, below of=3, xshift = 1 cm] {t};
							\node (4) [circleNode, below of=t, xshift = -1 cm] {4};
							\draw [arrow] (s) -- node [above] {5} (1);
							\draw [arrow] (s) -- node [above] {4} (2);
							\draw [arrow] (1) -- node [left] {2} (2);
							\draw [arrow] (1) -- node [above] {1} (3);
							\draw [arrow] (1) -- node [below] {4} (4);
							\draw [arrow] (2) -- node [above] {5} (4);
							\draw [arrow] (4) -- node [right] {4} (3);
							\draw [arrow] (3) -- node [above] {1} (t);
							\draw [arrow] (4) -- node [above] {6} (t);
							\draw [arrow] (t) -- node [right] {1} (1);
						\end{tikzpicture}
					\end{figure}

					Let $S = \{s, 1, 2, 3\}$, $\bar{S} = \{4, t\}$, $u(S, \bar{S}) = u_{14} + u_{24} + u_{3t} = 10$, $\delta^-(S) = \{(1, 4), (2, 4), (3, t)\}$, $\delta^+{S} = \{(4, 3), (t, 1)\}$
				\end{example}

				\begin{lemma}
					If $x$ is a $(s, t)$ flow of value $v$ and $(S, \bar{S})$ is a $(s, t)$-cut, then
					\begin{equation}
						v = \sum_{e\in \delta^-(S)} x_e - \sum_{e\in \delta^+(S)} x_e
					\end{equation}
				\end{lemma}

				\begin{proof}
					Summing the first set of constraints over the vertices of $S$,
					\begin{equation}
						\sum_{i\in S} (\sum_{h(e) = i}x_e - \sum_{t(e) = i}x_e) = -v
					\end{equation}
					Now for an arc $e$ with both ends in $S$, $x_e$ will occur twice once with a positive and once with negative so they cancel and the above sum is reduced to
					\begin{equation}
						\sum_{e\in \delta^+(S)}x_e - \sum_{e \in \delta^-(S)}x_e = -v
					\end{equation}
				\end{proof}

				\notice{Flow is the prime variable, capacity is the dual variable.}

				\begin{corollary}
					If $x$ is a feasible flow of value $v$, and $(S, \bar{S})$ is an $(s, t)$-cut, then
					\begin{equation}
						v \le u(S, \bar{S}) \quad \text{(Weak duality)}
					\end{equation}
				\end{corollary}

				\begin{definition}
					Define an arc $e$ to be \textbf{saturated} if $x_e = u_e$, and to be \textbf{flowless} if $x_e = 0$
				\end{definition}

				\begin{corollary}
					Let $x$ be a feasible flow and $(S, \bar{S})$ be a $(s, t)$-cut, if $\forall e\in \delta^-(S)$ is saturated, and $\forall e\in \delta^+(S)$ is flowless, then $x$ is a maximum flow and $(S, \bar{S})$ is a minimum cut. (Strong duality)
				\end{corollary}

				\begin{proof}
					If every arc of $\delta^-(S)$ is saturated then
					\begin{equation}
						\sum_{e\in \delta^-(S)}x_e = \sum_{e\in \delta^-(S)}u_e
					\end{equation}
					If every arc of $\delta^+(S)$ is flowless then
					\begin{equation}
						\sum_{e\in \delta^+(S)}x_e = 0
					\end{equation}
					$\Rightarrow$ $x$ is as large as it can get when as $u(S, \bar{S})$ is as small as it can get.
				\end{proof}

			\section{Prime and Dual of Maximum Network Flow Problem}
				The LP of maximum flow can be modeled as following, WLOG, we let $s = v_1 \in V, t = v_{|V|} \in V$.
				\begin{align}
					\max \quad & f = \left[\begin{matrix}0 & 0 & \cdots & 0 & 1\end{matrix}\right]\left[\begin{matrix}\mathbf{x} \\ f\end{matrix}\right]\\
					\text{s.t.} \quad & \left[\begin{matrix}\mathbf{A} & \mathbf{F}\end{matrix}\right]\left[\begin{matrix}\mathbf{x}\\ f\end{matrix}\right] = \mathbf{0}\\
					& \mathbf{Ix} \le \mathbf{u}\\
					& \left[\begin{matrix}\mathbf{x}\\ f\end{matrix}\right] \ge 0
				\end{align}

				In which $\mathbf{A}$ is the vertex-arc incident matrix and $\mathbf{F}$ is a column vector where the first row is -1, last row is 1 and all other rows are 0s, which is because we denote the first vertex as source $s$ and the last vertex as the sink $t$. $\mathbf{u}$ is the column vector of upper bound of each arcs.
				\begin{align}
					\mathbf{A} &= \mathbf{A}_{|E|\times |V|} = [a_{ij}], \text{ where } a_{ij} = \begin{cases}
						1, \quad \text{if $v_i = h(e_j)$} \\
						-1, \quad \text{if $v_i = t(e_j)$} \\
						0, \quad \text{otherwise}
					\end{cases}\\
					\mathbf{F} &= \left[\begin{matrix}-1 & \cdots & 0 & \cdots & 1\end{matrix}\right]^\top \\
					\mathbf{u} &= \left[\begin{matrix}u_1 & u_2 & \cdots & u_{|E|}\end{matrix}\right]^\top
				\end{align}

				Then, we take the dual of LP
				\begin{align}
					\min \quad & \mathbf{u}\mathbf{w_E} \\
					\text{s.t.} \quad & \left[\begin{matrix}
						\mathbf{w_V} & \mathbf{w_E}
					\end{matrix}\right]\left[\begin{matrix}
						\mathbf{A} \\ \mathbf{I}
					\end{matrix}\right] \ge 0 \\
					& \left[\begin{matrix}
						\mathbf{w_V} & \mathbf{w_E}
					\end{matrix}\right]\left[\begin{matrix}
						\mathbf{F} \\\mathbf{0}
					\end{matrix}\right] = 1\\
					& \mathbf{w_V} \quad \text{unrestricted} \\
					& \mathbf{w_E} \ge \mathbf{0}
				\end{align}

				In which $\mathbf{w_V}$ is ``whether or not'' vertex $v$ is in $S$ where $(S, \bar{S})$ represents a cut, $\mathbf{w_E}$ is ``whether or not'' an arc in in $\delta^+(S)$. $\mathbf{u}, \mathbf{E}, \mathbf{F}$ have the same meaning as in prime.
				\begin{align}
					\mathbf{w_V} &= \left[\begin{matrix}w_1 & w_2 & \cdots & w_{|V|}\end{matrix}\right]^\top \\
					\mathbf{w_E} &= \left[\begin{matrix}w_{|V| + 1} & w_{|V| + 2} & \cdots & w_{|V| + |E|}\end{matrix}\right]^\top
				\end{align}

				To make it more clear, it can be rewritten as following
				\begin{align}
					\min \quad & \sum_{e \in E} u_ew_e\\
					\text{s.t.} \quad & w_i - w_j + w_{|V| + e} \ge 0, \forall e = (i, j) \in E\\
					& -w_1 + w_{|V|} = 1\\
					& \mathbf{w_V} \quad \text{unrestricted} \\
					& \mathbf{w_E} \ge \mathbf{0}
				\end{align}

				The meaning for the first set of constraint is to decide whether or not an arc is in $\delta^+(S)$ of a $(S, \bar{S})$, which is decided by $w_V$. The $w_1 - w_{|V|} = 1$, which is the second set of constraint means the source $s = v_1$ and the sink $t = v_{|V|}$ has to be in $S$ and $\bar{S}$ respectively.

			\section{Maximum Flow Minimum Cut Theorem}
				\begin{definition}
					Let $P$ be a path, (not necessarily a dipath), $P$ is called \textbf{unsaturated} if every \textbf{forward} arc is unsaturated ($x_e < u_e$) and ever \textbf{reverse} arc has positive flow ($x_e > 0$). If in addition $P$ is an $(s, t)$-path, then $P$ is called an \textbf{x-augmenting path}
				\end{definition}

				\begin{theorem}
					A feasible flow $x$ in a digraph $D$ is maximum iff $D$ has no augmenting paths.
				\end{theorem}

				\begin{proof}
					(Prove by contradiction) 

					($\Rightarrow$) Let $x$ be a maximum flow of value $v$ and suppose $D$ has an augmenting path. Define in $P$ (augmenting path):
					\begin{align}
						& D_1 = \min \{u_e-x_e: e \text{ forward in } P\} \\
						& D_2 = \min \{x_e: e \text{ backward in } P\}\\
						& D = \min \{D_1, D_2\}
					\end{align}
					Since $P$ is augmenting, then $D > 0$, let
					\begin{align}
						\hat{x_e} = \begin{cases}
							x_e + D \quad \text{If $e$ is forward in $P$}\\
							x_e - D \quad \text{If $e$ is backward in $P$}\\
							x_e \quad otherwise
						\end{cases}
					\end{align}
					It is easy to see that $\hat{x}$ is feasible flow and that the value is $V+D$, a contradiction.

					($\Leftarrow$) Suppose $D$ admits no x-augmenting path, Let $S$ be the set of vertices reachable from $s$ by x-unsaturated path clearly $s\in S$ and $t\notin S$ (because otherwise there would be an augmenting path). Thus, $(S, \bar{S})$ is a $(s, t)$-cut.

					Let $e\in \delta^-(S)$ then $e$ must be saturated. For otherwise we could add the $h(e)$ to $S$

					Let $e\in \delta^+(S)$ then $e$ must be flow less. For otherwise we could add the $t(e)$ to $S$.

					According to previous corollary, that $x$ is maximum.
				\end{proof}

				\begin{theorem}(Max-flow = Minimum-cut)
					For any digraph, the value of a maximum $(s, t)$-flow is equal to the capacity of a minimum $(s, t)$-cut
				\end{theorem}

			\section{Ford-Fulkerson Method}
				Finding augmenting paths is the key of max-flow algorithm, we need to describe two functions, labeling and scanning a vertex.

				A vertex is first labeled if we can find x-unsaturated path from $s$, i.e., $(s, v)$-unsaturated path.

				The vertex $v$ is scanned after we attempted to extend the x-unsaturated path.

				\fixme{This algorithm is incomplete/incorrect, needs to be fixed}
				\begin{algorithm}
					\caption{Labeling algorithm}
					\begin{algorithmic}
						\Ensure Max-flow $x$ with value $v$
						\Require Digraph with source $s$ and sink $t$, a capacity function $u$ and a feasible flow (could be $x_e = 0$)
						\State Initialize, $v \gets x$
						\State Designate all vertices as unlabeled and unscanned
						\State Label $s$
						\While {There exists vertex unlabeled or unscanned}
							\State Let $i$ be such a vertex, for each arc $e$ with $t(e) = i, x_e < u_e$ and $h(e)$ unlabeled, label $h(e)$
							\State For each arc $e$ with $h(e) = i, x_e > 0$ and $t(e)$ unlabeled, label $t(e)$, designate $i$ as scanned.
							\State If $t$ is not label
						\EndWhile
						\State $x$ is the maximum.
					\end{algorithmic}
				\end{algorithm}

				\begin{algorithm}
					\caption{Ford-Fulkerson algorithm}
					\begin{algorithmic}
						\Ensure Max-flow $x$ with value $v$
						\Require Digraph with source $s$ and sink $t$, a capacity function $u$ and a feasible flow (could be $x_e = 0$)
						\State Initialize, $v \gets x$
						\State Designate all vertices as unlabeled and unscanned
						\State Label $s$
						\While {There exists vertex unlabeled or unscanned}
							\State Let $i$ be such a vertex, for each arc $e$ with $t(e) = i, x_e < u_e$ and $h(e)$ unlabeled, label $h(e)$
							\State For each arc $e$ with $h(e) = i, x_e > 0$ and $t(e)$ unlabeled, label $t(e)$, designate $i$ as scanned.
							\State If $t$ is not label
						\EndWhile
						\State $x$ is the maximum.
					\end{algorithmic}
				\end{algorithm}

				Labeling algorithm can be exponential, the following is an example
				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 1 cm]
						\node (1) [circleNode] {1};
						\node (s) [circleNode, below of=1, xshift = -1 cm] {s};
						\node (2) [circleNode, below of=s, xshift = 1 cm] {2};
						\node (t) [circleNode, below of=1, xshift = 1 cm] {t};
						\draw [arrow] (s) -- node [left] {M} (1);
						\draw [arrow] (s) -- node [left] {M} (2);
						\draw [arrow] (1) -- node [left] {1} (2);
						\draw [arrow] (1) -- node [left] {M} (t);
						\draw [arrow] (2) -- node [left] {M} (t);
					\end{tikzpicture}
				\end{figure}

			\section{Polynomial Algorithm for max flow}
				Let $(D,s,t,u)$ be a max flow problem and let $x$ be a feasible flow for $D$, the \textbf{x-layers} of $D$ are define be the following algorithm

				Layer algorithm (Dinic 1977)
				Input: A network $(D, s, t, u)$ and a feasible flow $x$
				Output: The \textbf{x-layers} $V_0, V_1, ..., V_l$ where $V_i \cap V_j = \emptyset \forall i \neq j$

				Step 1: Set $V_0 = \{s\}, i \gets 0 and l(x) = 0$
				Step 2: Let $R$ be the set of vertices $w$ such that there is an arc $e$ with either:
					\begin{itemize}
						\item $t(e) \in V_i, h(e) = w, x_e < u_e$ or
						\item $h(e) \in V_j, t(e) = w, x_e > 0$ 
					\end{itemize}
				Step 3: If $t \in R$, set $V_{i+1} = \{t\}$, $l(t) = i + 1$ and stop. Set $V_{i+1} \gets R\setminus \cup_{0 \le j \le i} V_j$, $l \gets i+1, l(x) = i$, goto Step 2.
				If $V_{i+1} = \emptyset$, set $l(x) = i$ and Stop.

				\begin{example}
					For the following graph
					\begin{figure}
						\centering
						\begin{tikzpicture}[node distance = 1 cm]
							\node (1) [circleNode] {1};
							\node (s) [circleNode, below of=1, xshift = -1 cm] {s};
							\node (2) [circleNode, below of=s, xshift = 1 cm] {2};
							\node (t) [circleNode, below of=1, xshift = 1 cm] {t};
							\draw [arrow] (s) -- node [left] {M} (1);
							\draw [arrow] (s) -- node [left] {M} (2);
							\draw [arrow] (1) -- node [left] {1} (2);
							\draw [arrow] (1) -- node [left] {M} (t);
							\draw [arrow] (2) -- node [left] {M} (t);
						\end{tikzpicture}
					\end{figure}
					\begin{align}
						\text{Second iteration}\\
						V_0 = \{s\}, i = 0, l(x) = 0 \\
						R=\{1,2\}\\
						V_1 \gets \{1, 2\}, i = 1, l(x) = 1\\
						R=\{3, 4, 5\}\\
						V_2 \gets \{3, 4\}, i = 2, l(x) = 2\\
						R=\{1, 5, 6, 3\}\\
						V_3 \gets \{5, 6\}, i = 3, l(x) = 3\\
						R=\{4, t\}\\
						V_4 = \{t\}\\
						A_1=\{(s, 1), (s, 2)\}\\
						A_2=\{(1, 3), (2, 4)\}\\
						A_3=\{(3, 5), (4, 6)\}\\
						A_4=\{(5, t), (6, t)\}
					\end{align}
				\end{example}

				The layer network $D_x$ is defined by $V(D_x) = V_0 \cup V_1 \cup V_2 \dots \cup V_{l(x)}$

				Suppose we have computed the layers of $D$ and $t \in V_l(x)$, the last layer (last layer I am goin to $V_e$)

				For each $i, 1 \le i \le l$, define a set of arcs $A_i$ and a function $\hat{u}$ on $A_i$ as following. For each $e\in A(D)$
				\begin{itemize}
					\item If $t(e) \in V_{i-1}, h(e) \in V_i$ and $x_e < u_e$ then add arc $e$ to $A_i$ and define $\hat{u}_e = u_e - x_e$
					\item If $h(e) \gets V_{i-1}, t(e) \in V_i$ and $x_e > 0$ then add arc $e^\prime = (h(e), t(e))$ to $A_i$ with $\hat{u}_e - x_e$
				\end{itemize}

				Let $\hat{u}$ be the capacity function on $D_x$ and let the source and sink of $D_x$ be $s$ and $t$

				We can think of $D_x$ as being make of arc shortest (in terms of numbers of arcs) x-augmenting paths.

				A feasible flow in a network is said to be maximal (does not means maximum) if every $(s, t)$-directed path contains at least one saturated arc.

				For layered algorithm $V_0, V_1, ..., V_L$

				Arcs:
				\begin{itemize}
					\item If $t(e)\in V_{i-1}$, $h(e) \in V_i$ and $x_e < u_e$, then add $e$ to $A_i$ with $\hat{u_e} = u_e - x_e$
					\item If $h(e) \in V_{i-1}$, $t(e)\in V_i$ and $x_e > 0$, then add arc $e^\prime = (h(e), t(e))$ to $A_i$ and define $\hat{u_e} = x_e$
				\end{itemize}

				Maximal Flow: If every directed $(s, t)$-path has at least one saturated arc.

				Computing maximal flow is easier than computing maximum flow, since we never need to consider canceling flows on reverse arcs,

				Let $\hat{x}$ be a maximal flow on the layered network $D_x$, we can define new flows in $D(x^\prime)$ by
				\begin{align}
					x_e^\prime = x_e + \hat{x_e}, \quad \text{If } t(e) \in V_{i-1}, h(e)\in V_i\\
					x_e^\prime = x_e - \hat{x_e}, \quad \text{If } h(e) \in V_{i-1}, t(e)\in V_i
				\end{align}

			\section{Dinic Algorithm}
				Input: A layered network $(D_x, s, t, \hat{u})$ and a feasible flow x
				Output: A maximal flow $\hat{x}$ from $D_x$

				Step 1: Set $H\gets D_x$ and $i\gets S$
				Step 2: If there is no arc $e$ with $t(e) = i$, goto Step 4, otherwise let $e$ be such an arc
				Step 3: Set $T(h(e))\gets i$ and $i \gets h(e)$, if $i= t$ goto Step 5, otherwise goto Step 2.
				Step 4: If $i = s$, Stop, Otherwise delete $i$ and all incident arcs with $H$, set $i \gets T(i)$ and goto Step 2
				Step 5: Construct the directed path, $s = i_0e_1i_1e_2...e_ki_k=t$ where $i_{j-1} = T(i_j), 1\le j \le k$. Set $D=\min\{\hat{u_{e_j}}-\hat{x_{e_j}}:i\le j \le k\}$, set $\hat{x_{e_j}} \gets \hat{x_{e_j}} + D, i \le j \le k$. Delete from $H$ all saturated arcs on this path, set $i \gets 1$ and goto Step 2.

				\begin{algorithm}
					\caption{Dinic Algorithm}
					\begin{algorithmic}
						\Ensure A maximal flow $\hat{x}$ from $D_x$
						\Require A layered network $(D_x, s, t, \hat{u})$ and a feasible flow x

						\State Initialize $H\gets D_x$ and $i\gets S$
					\end{algorithmic}
				\end{algorithm}

				\begin{theorem}
					Dinic algorithm runs in $O(|E||V|^2)$
				\end{theorem}

				\begin{proof}
					Step 1 is $O(|E||V|)$
					Step 2 runs Step 1 for $O(|V|)$ times
				\end{proof}

		\chapter{Minimum Cost Flow Problem}
			\section{Transshipment Problem}
				Transshipment Problem $(D, b, w)$ is a linear program of the form
				\begin{align}
					\min \quad & wx\\
					\text{s.t.} \quad & Nx = b\\
									  & x \ge 0
				\end{align}
				Where $N$ is a vertex-arc incident matrix. For a feasible solution to LP to exist, the sum of all $b$s must be zero. Since the summation of rows of $N$ is zero. The interpretation of the LP is as follows.

				The variables are defined on the edges of the digraph and that $x_e$ denote the amount of flow of some commodity from the tail of $e$ to the head of $e$

				Each constraints
				\begin{equation}
					\sum_{h(e) = i} x_e - \sum_{t(e) = i}x_e = b_i
				\end{equation}
				represents consequential of flow of all edges into $k$ vertex that have a demand of $b_i > 0$, or a supply of $b_i < 0$. If $b_i = 0$ we call that vertex a transshipment vertex.

			\section{Network Simplex Method}
				\begin{lemma}
					Let $C_1$ and $C_2$ be distinct cycles in a graph $G$ and let $e\in C_1 \cup C_2$. Then $(C_1 \cup C_2) \setminus e$ contains a cycle.
				\end{lemma}

				\begin{proof}
					Case 1: $C_1 \cap C_2 = \emptyset$. Trivia.\\
					Case 2: $C_1 \cap C_2 \neq \emptyset$. Let $e\in C_2$ and $f=uv \in C_1 \setminus C_2$. Starting at $v$ traverse $C_1$ in the direction away from $u$ until the first vertex of $C_2$, say $x$. Denote the $(v, x)$-path as $P$. Starting at $u$ traverse $C_1$ in the direction away from $v$ until the first vertex of $C_2$, say $y$. Denote the $(u, y)$-path as $Q$. $C_2$ is a cycle, there are two $(x, y)$-path in $C_2$. Denote the $(x, y)$-path without $e$ as $R$. Then $vPxRyQ^{-1}uf$ is a cycle.
				\end{proof}

				\begin{theorem}
					Let $T$ be a spanning tree of $G$. And let $e\in E\setminus T$ then $T+e$ contains a unique cycle $C$ and for any edge $f\in C$, $T+e-f$ is a spanning tree of $G$
				\end{theorem}

				Let $(D, b, w)$ be a transshipment problem. A feasible solutions $x$ is a \textbf{feasible tree solution} if there is a spanning tree $T$ such that $||x|| = \{e\in A, x_e\neq 0\} \subseteq T$.

				The strategy of network simplex algorithm is to generate negative cycles, if negative cycle exists, it means the solution can be improved.

				For any tree $T$ of $D$ and for $e\in A\setminus T$, it follows from above theorem that $T+e$ contains a unique cycle. Denote that cycle $C(T, e)$ and orient it in the direction of $e$, define 

				\begin{eqnarray}
					w(T, e) = \sum\{w_e: e \text{ forward in } C(T, e)\} \nonumber \\ 
							- \sum\{w_e: e \text{ reverse in } C(T, e)\}
				\end{eqnarray}

				We think of $w(T, e)$ as the weight of $C(T,e)$.

				\subsection{Network Simplex Method}
					\begin{algorithm}
						\caption{Network Simplex Method Algorithm}
						\begin{algorithmic}
							\Ensure An optimal solution or the conclusion that $(D, b, w)$ is unbounded
							\Require A transshipment problem $(D, b, w)$ and a feasible tree solution $x$ containing to a spanning tree $T$
							\While{$\exists e\in A\setminus T, w(T,e) < 0$}
								\State let $e \in A \setminus T$ be such that $w(T, e) < 0$.
								\If {$C(T, e)$ has no reverse arcs}
									\State Return unboundedness
								\Else
									\State Set $\theta = \min\{x_f: f \text{ reverse in } C(T, e)\}$ and set $f = \{f\in C(T, e): f \text{ reverse in } C(T, e), x_f = \theta\}$
									\If {$f$ forward in $C(T, e)$}
										\State $x_f \gets x_f + \theta$
									\Else
										\State $x_f \gets x_f - \theta$
									\EndIf
									\State Let $f \in F$ and $T \gets T+e-f$
								\EndIf
							\EndWhile
							\State Return $x$ as optimal
						\end{algorithmic}
					\end{algorithm}

				\subsection{Example for cycling}
					\notice{Similar to Simplex Method in LP, even though in worst case may be inefficient. In most cases it is simple and empirically efficient. Also, similarily, there will be cycling problems.}

					The following is an example of cycling\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance=2.5 cm]
							\node (1) [circleNode, label=above:{$b = 0$}] {1};
							\node (2) [circleNode, below of=1, xshift=-1.5 cm, label=left:{$b = 0$}] {2};
							\node (3) [circleNode, below of=1, xshift=1.5 cm, label=right:{$b = 0$}] {3};
							\draw [arrow] (1) to [out = 230, in = 70] node [right] {1, a} (2);
							\draw [arrow] (1) to [out = 205, in = 90] node [above] {-1, b} (2);
							\draw [arrow] (1) to [out = 180, in = 110] node [left] {3, c} (2);
							\draw [arrow] (1) to [out = 0, in = 70] node [right] {3, h} (3);
							\draw [arrow] (1) to [out = -25, in = 90] node [above] {-1, g} (3);
							\draw [arrow] (1) to [out = -50, in = 110] node [left] {1, f} (3);
							\draw [arrow] (2) to [out = 15, in = 165] node [above] {3, d} (3);
							\draw [arrow] (2) to [out = 45, in = 135] node [above] {-3, e} (3);
							\draw [arrow] (3) to [out = -165, in = -15] node [below] {3, i} (2);
							\draw [arrow] (3) to [out = -135, in = -45] node [below] {-3, j} (2);
						\end{tikzpicture}
					\end{figure}

					Then for the following steps we can detect cycling:

					$w(T, j) = w_j - w_i = -3 -3 = -6$, therefore $j$ in entering basis, $i$ is leaving basis.\\
					$w(T, h) = w_h + w_j - w_a = 3-3-1=-1$, therefore $h$ is entering basis, $a$ is leaving basis.\\
					$w(T, b) = w_b - w_j - w_h = -1 + 3 - 3 = -1$, therefore $b$ is entering basis, $j$ is leaving basis.\\
					$w(T, d) = w_d - w_h + w_b = 3 - 3 - 1 = -1$, therefore $d$ is entering basis, $h$ is leaving basis.\\
					$w(T, f) = w_f - w_d - w_b = 1 - 3 + 1 = -1$, therefore $f$ is entering basis, $b$ is leaving basis.\\
					$w(T, e) = w_e - w_d = -3 -3 = -6$, therefore $e$ is entering basis, $d$ is leaving basis.\\
					$w(T,c) = w_c + w_e - w_f = 3 -3 - 1 = -1$, therefore $c$ is entering basis, $f$ is leaving basis.\\
					$w(T,g) = w_g - w_e - w_c = -1 + 3 - 3 = -1$, therefore $g$ is entering basis, $e$ is leaving basis.\\
					$w(T,i) = w_i - w_c + w_g =3 - 3 - 1= -1 $, therefore $i$ is entering basis, $c$ is leaving basis.\\
					$w(T,a) = w_a - w_i - w_g = 1 - 3 + 1 = -1$, therefore $a$ is entering basis, $g$ is leaving basis.\\

					\begin{figure*}[!ht]
						\centering
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow] (1) to [out = 230, in = 70] node [left] {1, a} (2);
								\draw [arrow] (3) to [out = -165, in = -15] node [above] {3, i (out)} (2);
								\draw [arrow, dashed] (3) to [out = -135, in = -45] node [below] {-3, j (in)} (2);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow] (1) to [out = 230, in = 70] node [left] {1, a (out)} (2);
								\draw [arrow] (3) to [out = -135, in = -45] node [below] {-3, j} (2);
								\draw [arrow, dashed] (1) to [out = 0, in = 70] node [right] {3, h (in)} (3);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow, dashed] (1) to [out = 205, in = 90] node [above] {-1, b (in)} (2);
								\draw [arrow] (1) to [out = 0, in = 70] node [right] {3, h} (3);
								\draw [arrow] (3) to [out = -135, in = -45] node [below] {-3, j (out)} (2);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow] (1) to [out = 205, in = 90] node [above] {-1, b} (2);
								\draw [arrow] (1) to [out = 0, in = 70] node [right] {3, h (out)} (3);
								\draw [arrow, dashed] (2) to [out = 15, in = 165] node [below] {3, d (in)} (3);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow] (1) to [out = 205, in = 90] node [above] {-1, b (out)} (2);
								\draw [arrow, dashed] (1) to [out = -50, in = 110] node [right] {1, f (in)} (3);
								\draw [arrow] (2) to [out = 15, in = 165] node [below] {3, d} (3);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow] (1) to [out = -50, in = 110] node [right] {1, f} (3);
								\draw [arrow] (2) to [out = 15, in = 165] node [below] {3, d (out)} (3);
								\draw [arrow, dashed] (2) to [out = 45, in = 135] node [above] {-3, e (in)} (3);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow, dashed] (1) to [out = 180, in = 110] node [left] {3, c (in)} (2);
								\draw [arrow] (1) to [out = -50, in = 110] node [right] {1, f (out)} (3);
								\draw [arrow] (2) to [out = 45, in = 135] node [above] {-3, e} (3);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow, dashed] (1) to [out = -25, in = 90] node [right] {-1, g (in)} (3);
								\draw [arrow] (1) to [out = 180, in = 110] node [left] {3, c} (2);
								\draw [arrow] (2) to [out = 45, in = 135] node [above] {-3, e (out)} (3);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow] (1) to [out = -25, in = 90] node [right] {-1, g} (3);
								\draw [arrow] (1) to [out = 180, in = 110] node [left] {3, c (out)} (2);
								\draw [arrow, dashed] (3) to [out = -165, in = -15] node [above] {3, i (in)} (2);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow, dashed] (1) to [out = 230, in = 70] node [left] {1, a (in)} (2);
								\draw [arrow] (1) to [out = -25, in = 90] node [right] {-1, g (out)} (3);
								\draw [arrow] (3) to [out = -165, in = -15] node [above] {3, i} (2);
							\end{tikzpicture}
						\end{subfigure}
						\qquad
						\begin{subfigure}[c]{0.24\textwidth}
							\centering
							\begin{tikzpicture}[node distance=1.5 cm]
								\node (1) [solidNode] {};
								\node (2) [solidNode, below of = 1, xshift=-0.9 cm] {};
								\node (3) [solidNode, below of = 1, xshift=0.9 cm] {};
								\draw [arrow] (1) to [out = 230, in = 70] node [left] {1, a} (2);
								\draw [arrow] (3) to [out = -165, in = -15] node [above] {3, i (out)} (2);
								\draw [arrow, dashed] (3) to [out = -135, in = -45] node [below] {-3, j (in)} (2);
							\end{tikzpicture}
						\end{subfigure}
					\end{figure*}

					The last graph is the same as the first graph, i.e., cycling detected.

				\subsection{Cycling prevention}
				To Avoid cycling we will introduce the Modified Network Simplex Method. Let $T$ be a \textbf{rooted} spanning tree. Let $f$ be an arc in $T$, we say $f$ is \textbf{away} from the root $r$ if $t(f)$ is the component of $T-f$. Otherwise we say $f$ is \textbf{towards} $r$.

					Let $x$ be a feasible tree solution associated with $T$, then we say $T$ is a \textbf{strong feasible tree} if for every arc $f \in T$ with $x_f = 0$ then $f$ is away from $r\in T$.

					Modification to NSM:
					\begin{itemize}
						\item The algorithm is initialed with a strong feasible tree.
						\item $f$ in pivot phase is chosen to be the first reverse arc of $C(T, e)$ having $x_f = \theta$. By ``first'', we mean the first arc encountered in traversing $C(T, e)$ in the direction of $e$, starting at the vertex $i$ of $C(T, e)$ that minimizes the number of arcs in the unique $(r, i)$-path in $T$.
					\end{itemize}

					\notice{In the second rule above, $r$ could also be in the cycle, in that case, $i$ is $r$.}

					Continue the previous example. Now should how we can avoid cycling:

					The first few (four) steps are the same as previous example, starting from\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance=2.5 cm]
							\node (1) [circleNode] {1};
							\node (2) [circleNode, below of = 1, xshift=-1.5 cm] {2};
							\node (3) [circleNode, below of = 1, xshift=1.5 cm] {3};
							\draw [arrow] (1) to [out = 205, in = 90] node [above] {-1, b (can leave)} (2);
							\draw [arrow, dashed] (1) to [out = -50, in = 110] node [left] {1, f (in)} (3);
							\draw [arrow] (2) to [out = 15, in = 165] node [above] {3, d (can leave)} (3);
						\end{tikzpicture}
					\end{figure}
					$w(T,f) = w_f - w_d - w_b = 1 - 3 + 1 = -1$. $f$ is entering basis, both $b$ and $d$ can leave the basis, according to the modified pivot rule, we choose the ``first'' arc encountered in traversing $C(T, e)$, which is $d$ to leave the basis, instead of $b$.\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance=2.5 cm]
							\node (1) [circleNode] {1};
							\node (2) [circleNode, below of = 1, xshift=-1.5 cm] {2};
							\node (3) [circleNode, below of = 1, xshift=1.5 cm] {3};
							\draw [arrow] (1) to [out = 205, in = 90] node [above] {-1, b} (2);
							\draw [arrow] (1) to [out = -50, in = 110] node [left] {1, f (out)} (3);
							\draw [arrow, dashed] (2) to [out = 45, in = 135] node [above] {-3, e (in)} (3);
						\end{tikzpicture}
					\end{figure}
					$w(T, e) = w_e - w_f + w_b = -5$, $e$ is entering basis, $f$ is leaving basis. Now the only arc to enter basis and maintain negative $w$ is $j$.\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance=2.5 cm]
							\node (1) [circleNode] {1};
							\node (2) [circleNode, below of = 1, xshift=-1.5 cm] {2};
							\node (3) [circleNode, below of = 1, xshift=1.5 cm] {3};
							\draw [arrow] (1) to [out = 205, in = 90] node [above] {-1, b} (2);
							\draw [arrow] (2) to [out = 45, in = 135] node [above] {-3, e} (3);
							\draw [arrow, dashed] (3) to [out = -135, in = -45] node [below] {-3, j (in)} (2);
						\end{tikzpicture}
					\end{figure}
					$w(T, j) = w_j + w_e = -6$, but in $C(T, j)$ there is no reversing arc, therefore we detect unboundedness.

				\subsection{Finding Initial Strong Feasible Tree}
					Pick a vertex in $D$ to be root $r$. The tree $T$ has an arc $e$ with the $t(e) = r$ and $h(e) = v$. For each $v\in V\setminus r$ with $b_v \ge 0$ and has an arc $e$ with $h(e) = r$ and $t(e) = v$ for each $v \in V\setminus r$ for which $b_v < 0$.
					Wherever possible the arcs of $T$ are chosen from $A$, where an appropriate arc doesn't exist. We create an \textbf{artificial arc} and give its weight $|V|(\max\{w_e: e\in A\}) + 1$. This is similar to Big-M method and if optimal solution contains artificial arcs ongoing arc problem is infeasible.

					Here is an example after adding artificial arcs:\\
					\begin{figure}[!ht]
						\centering
						\begin{tikzpicture}[node distance = 1.5 cm]
							\node (1) [circleNode, label=above:{$b_1 \ge 0$}] {$v_1$};
							\node (2) [circleNode, label=above:{$b_2 < 0$}, right of = 1] {$v_2$};
							\node (3) [circleNode, label=above:{$b_3 \ge 0$}, right of = 2] {$v_3$};
							\node (4) [circleNode, label=above:{$b_4 \ge 0$}, right of = 3] {$v_4$};
							\node (5) [circleNode, label=above:{$b_5 < 0$}, right of = 4] {$v_5$};
							\node (6) [circleNode, label=above:{$b_6 \ge 0$}, right of = 5] {$v_6$};
							\node (0) [circleNode, label=below:{$b_0$}, below of= 3, yshift=-1 cm] {$r$};
							\draw [arrow] (0) -- node [left] {$e_1$} (1);
							\draw [arrow] (2) -- node [left] {$e_2$} (0);
							\draw [arrow] (0) -- node [left] {$e_3$} (3);
							\draw [arrow] (0) -- node [left] {$e_4$} (4);
							\draw [arrow, dashed] (5) -- node [left] {$e_5$} (0);
							\draw [arrow, dashed] (0) -- node [left] {$e_6$} (6);
						\end{tikzpicture}
					\end{figure}
					Where $e_5$ and $e_6$ are artificial arcs, the weight of those arcs are $|V|(\max\{w_e: e\in \mathcal{A}\}) + 1$. And the above tree is a basic feasible solution.

					We need to prove that such artificial arc has sufficiently large weight to guarantee
					\begin{itemize}
						\item It will leave the basis, and
						\item It will not enter the basis again (for this, just delete the artificial arc after it leaves the basis, then it will never enter the basis again)
					\end{itemize}
					\begin{proof}
						Now prove that such arcs will always leave the basis. Before the prove we give some notation. 
						\begin{itemize}
							\item Define set $E$ as the set of arcs which is not artificial arc, in the above example, $E = \{e_1, e_2, e_3, e_4\}$. 
							\item Define set $A$ as the set of arcs which are artificial arcs, in the above example, $A = \{e_5, e_6\}$. Noticed that $E \cap A = \emptyset$.
							\item Define set $M$ as the vertices in the spanning tree that is reachable from $r$ by $E$, in the above example, $M = \{v_1, v_2, v_3, v_4\}$.
							\item Define $M^\prime = (V\setminus r) \setminus M$ in the tree that can only be reached from $r$ by $A$, i.e., artificial arcs, in the above example, $M^\prime = \{v_5, v_6\}$. 
						\end{itemize}

						Then the initial basic feasible solution is a graph 
						\begin{equation}
							G_0 = <M\cup M^\prime \cup \{r\}, E\cup A>
						\end{equation}
						Denote the origin graph 
						\begin{equation}
							G = <V, \mathcal{A}>
						\end{equation}
						Notice that with the artificial arcs, $G_0$ is not a subgraph of $G$.

						Let $(M\cup\{r\}, M^\prime)$ be a cut in the origin graph $G$. For the vertices in $M^\prime$, one of the following cases will happen:
						\begin{itemize}
							\item case 1: $\sum_{v \in M^\prime} b_v \ge 0$
							\item case 2: $\sum_{v \in M^\prime} b_v < 0$
						\end{itemize}

						For case 1, we claim that at least one of the vertices $v_{M^\prime} \in M^\prime$ with $b_{v_{M^\prime}} \ge 0$ linked by an arc, say $f$, such that $h(f) = v_{M^\prime}$ and $t(f) = v_M \in M$. Otherwise the balance of flow cannot hold in the origin graph $G$. Furthermore, denote the artificial arc from $r$ to $v_{M^\prime}$ by $e_{rv_{M^\prime}}$.

						Notice that for $v_M$ there is not necessarily be an arc between $r$ and $v_M$, but there must exists an $(r, v_M)$-path denoted by $P$, for $M$ is the set of vertices that reachable from $r$ by arcs in $E$.

						Take that arc $f$ as entering arc to the basis. Then 
						\begin{equation}
							C(T, f) = re_{rv_{M^\prime}}v_{M^\prime}fv_MPr
						\end{equation}
						For
						\begin{equation}
							w(T, f) = w_f - w_{e_{rv_{M^\prime}}} + \sum_{e \in P} d_e w_e
						\end{equation}
						where $d_e = 1$ if $w_e$ is forward in $P$ and $d_e = -1$ otherwise.

						Now that $w_{e_{rv_{M^\prime}}} = |V|(\max\{w_e: e\in \mathcal{A}\}) + 1$, it guarantees that
						\begin{align}
							w(T, f) &= w_f + \sum_{e \in P} d_e w_e - w_{e_{rv_{M^\prime}}}\\
							        &\le w_f + \sum_{e \in P}w_e - w_{e_{rv_{M^\prime}}}\\
							        &\le \sum_{e \in \mathcal{A}}w_e - w_{e_{rv_{M^\prime}}}\\
							        &\le |V|(\max\{w_e: e\in \mathcal{A}\}) - w_{e_{rv_{M^\prime}}}\\
							        &\le -1 < 0 
						\end{align}
						So $f$ can enter the basis, and the artificial variable $e_{rv_{M^\prime}}$ will leave the basis, for it is the most violated reverse arc in the $C(T, f)$. When we put $f$ into the basis, update $G_0$, such that $M \leftarrow M \cup \{v_{M^\prime}\}$ and $M^\prime \leftarrow M^\prime \setminus \{v_{M^\prime}\}$.

						For case 2, it is similar. At least one of the vertices $v_{M^\prime} \in M^\prime$ with $b_{v_{M^\prime}} < 0$ linked by an arc, say $f\prime$, such that $t(f^\prime) = v_{M^\prime}$ and $h(f^\prime) = v_M \in M$. Otherwise the balance of flow cannot hold in the origin graph $G$. Furthermore, denote the artificial arc from $v_{M^\prime}$ to $r$ by $e_{v_{M^\prime}r}$.

						Similarly we can find a cycle $C(T, f^\prime) = rP^\prime v_M f^\prime v_{M^\prime}e_{v_{M^\prime}r}r$. $w(T, f^\prime) = w_{f^\prime} - w_{e_{rv_{M^\prime}}} + \sum_{e \in P^\prime} d_e w_e$, where $d_e = 1$ if $w_e$ is forward in $P\prime$ and $d_e = -1$. We can prove $w(T, f^\prime) \le -1 < 0$. That that $f^\prime$ as entering arc to the basis, similarly move $v_{M^\prime}$ form set $M^\prime$ to $M$.

						The above case can be dealt with iteratively until set $M^\prime$ become $\emptyset$, at which stage there is no artificial arc in the basic feasible solution. Which means all the artificial variable can leave the basis.
					\end{proof}

					\notice{This algorithm can be really bad, its mimic of Simplex Method of LP, which means we can run into exponential operations}

			\section{Transshipment Problem and Circulation Problem}
				\begin{definition}
					The minimum weight circulation problem is defined as follows:
					\begin{align}
						\min \quad & wx\\
						\text{s.t.} \quad & Nx = 0\\
							&l \le x \le u
					\end{align}
					It turns out that the circulation problem is equivalent with transshipment problem.
				\end{definition}

				We will show how to transform any transshipment into circulation.

				Let $(D, b, w)$ be a transshipment problem and define two new vertices $s$ and $t$. 
				\begin{itemize}
					\item For each supply vertex $x$ add the arc $(s, x)$ to $D$ with $l_x = 0, u_x = -b_x, w_x = 0$. 
					\item Similarly, for each demand vertex $x$, add the arc $(x, t)$ to $D$ with $l_x = 0, u_x = b_x, w_x = 0$. 
					\item Finally, add an arc $(t, s)$ having $w_{ts} = 0, l_{ts} = u_{ts} = \sum\{b_x: \forall x, x \text{ is demand vertex}\}$. 
					\item Each original arc is given a $l_x = 0, u_x = \sum\{b_x : \forall x, x \text{ is a demand vertex}\}$, $w_x$ remains unchanged.
				\end{itemize}

				The following is a graph for transshipment problem.

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 2 cm]
						\node (2) [circleNode, label=above:{-2}] {2};
						\node (1) [circleNode, below of=2, label=above:{-1}] {1};
						\node (3) [circleNode, right of=2, label=above:{2}] {3};
						\node (4) [circleNode, below of=3, label=above:{1}] {4};
						\draw (2) [arrow] -- node [above] {5} (3);
						\draw (2) [arrow] -- node [below, xshift=-0.5 cm] {11} (4);
						\draw (1) [arrow] -- node [below, xshift=0.5 cm] {7} (3);
						\draw (1) [arrow] -- node [above] {12} (4);
					\end{tikzpicture}
				\end{figure}

				After the above procedures, it is now transformed into a circulation problem.

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 2 cm]
						\node (2) [circleNode] {2};
						\node (1) [circleNode, below of=2] {1};
						\node (3) [circleNode, right of=2] {3};
						\node (4) [circleNode, below of=3] {4};
						\node (s) [circleNode, below of=1] {s};
						\node (t) [circleNode, below of=4] {t};
						\draw (2) [arrow] -- node [above] {0, 3, 5} (3);
						\draw (2) [arrow] -- node [below, yshift=0.5 cm, xshift=-1 cm] {0, 3, 11} (4);
						\draw (1) [arrow] -- node [below, yshift=0.5 cm, xshift=1 cm] {0, 3, 7} (3);
						\draw (1) [arrow] -- node [above] {0, 3, 12} (4);
						\draw (s) [arrow] to [out = 180, in = 180] node [right] {0, 1, 0} (1);
						\draw (s) [arrow] to [out = 180, in = 180] node [left] {0, 2, 0} (2);
						\draw (3) [arrow] to [out = 0, in = 0] node [right] {0, 2, 0} (t);
						\draw (4) [arrow] to [out = 0, in = 0] node [left] {0, 1, 0} (t);
						\draw (t) [arrow] -- node [below] {3, 3, 0} (s);
					\end{tikzpicture}
				\end{figure}

				Then, we will show how to transform any circulation problem into transshipment problem.

				If the lower bound of the arc is zero, i.e., $l_e = 0$, then for each such arc $(u, v)$, introduce a vertex in between $u$ and $v$, replace the arc $e = (u, v)$ by $e_1 = (u, x)$ and $e_2 = (x, v)$. Both arcs are uncapacitated. Let $w_1(u, x) = w(u, v)$ and $w_2(x, v) = 0$ be the new weights for the arcs. Let $u_e$ be the demands of newly added vertex $x$ and add $u_e$ to the supplies of vertex $v$ (in $v$ the supplies is the summation from all arcs that go to $v$).

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 2 cm]
						\node (u) [circleNode] {$u$};
						\node (v) [circleNode, right of=u] {$v$};
						\draw (u) [arrow] -- node [above] {0, $u_e$, $w_e$} (v);
					\end{tikzpicture}
				\end{figure}

				Will be transform into

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 2 cm]
						\node (u) [circleNode, label=above:{0}] {$u$};
						\node (x) [circleNode, label=above:{$u_e$}, right of=u] {$x$};
						\node (v) [circleNode, label=above:{$u_e$}, right of=x] {$v$};
						\draw (u) [arrow] -- node [above] {$w_e$} (x);
						\draw (v) [arrow] -- node [above] {0} (x);
					\end{tikzpicture}
				\end{figure}

				If the lower bound of the arc is not zero, i.e., $l_e \neq 0$, then for each such arc $(u, v)$, introduce two new vertices, one vertex is in between $u$ and $v$, similar to the previous case, the difference is the demands of this new vertex is $u_e - l_e$, the others stays the same. Then add the other vertex, this vertex, denoted as $x^\prime$ is added along with an arc between $u$ and $x^\prime$, the weight of this arc is $w(u, v)$, the demands on this new vertex is $l_e$.

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 2 cm]
						\node (u) [circleNode] {$u$};
						\node (v) [circleNode, right of=u] {$v$};
						\draw (u) [arrow] -- node [above] {$l_e$, $u_e$, $w_e$} (v);
					\end{tikzpicture}
				\end{figure}

				Will be transform into\\

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}[node distance = 2 cm]
						\node (u) [circleNode, label=above:{0}] {$u$};
						\node (x) [circleNode, label=above:{$u_e - l_e$}, right of=u] {$x$};
						\node (v) [circleNode, label=above:{$u_e$}, right of=x] {$v$};
						\node (xprime) [circleNode, label=below:{$l_e$}, below of=u] {$x^\prime$};
						\draw (u) [arrow] -- node [above] {$w_e$} (x);
						\draw (v) [arrow] -- node [above] {0} (x);
						\draw (u) [arrow] -- node [left] {$w_e$} (xprime);
					\end{tikzpicture}
				\end{figure}

				Perform the above procedures to all the arcs in a circulation problem. In $O(E)$ (polynomially times) transformation, such problem can be transformed into transshipment problem.

			\section{Out-of-Kilter algorithm}
				This algorithm is a Primal-dual method and is applied to the minimum weight circulation problem.

				For LP optimality conditions we need primal feasibility, dual feasibility and complementary slackness, i.e., KKT conditions. Primal and dual feasibility are obvious so we need to show complementary slackness through following theorem.

				\begin{theorem}
					Let $x$ be a feasible circulation flow for $(D, l, u, w)$. And suppose there exists a real value vector $\{y_i: i \in V\}$ which we called \textbf{vertex-numbers}. For all edges $e\in A$
					\begin{align}
						y_{h(e)} - y_{t(e)} &> w_e \text{ implies } x_e = u_e\\
						y_{h(e)} - y_{t(e)} &< w_e \text{ implies } x_e = l_e
					\end{align}
					Then $x$ is optimal to the circulation problem.
				\end{theorem}

				\begin{proof}
					For each $e \in A$ define
					\begin{align}
						\gamma_e &= \max\{y_{h(e)} - y_{t(e)} - w_e, 0\} \\
						\mu_e &= \max\{w_e - y_{h(e)} + y_{t(e)}, 0\}
					\end{align}
					Then
					\begin{equation}
						\gamma_e - \mu_e = y_{h(e)} - y_{t(e)} - w_e
					\end{equation}
					Furthermore
					\begin{align}
						&\sum_{e\in A} (\mu_el_e - \gamma_eu_e) \\
						&= \sum_{e\in A} (\mu_el_e - \gamma_eu_e) + \sum_{i \in V}y_i(\sum_{h(e) = i} x_e - \sum_{t(e) = i} x_e)\\
						&= \sum_{e\in A} (\mu_el_e - \gamma_eu_e + x_e(y_{h(e)} - y_{t(e)}))\\
						&= \sum_{e\in A} (\mu_el_e - \gamma_eu_e + x_e(\gamma_e - \mu_e + w_e))\\
						&= \sum_{e\in A} (\gamma_e(x_e - u_e) + \mu_e(l_e - x_e) + x_ew_e)\\
						&\le \sum_{e\in A} x_ew_e 
					\end{align}
					The last inequality will be satisfied as equality iff the first two hold.
				\end{proof}

				The following is the formulation of circulation problem
				\begin{align}
					\text{(P)} \quad \min \quad & wx\\
					\text{s.t.} \quad & Nx = 0 \quad y \\
									  & x \ge l \quad z^l \\
									  & -x \le -u \quad z^u\\
					\text{(D)} \quad \max \quad & lz^l - uz^u \\
					\text{(s.t.)} \quad & yN^{-1} + z^l-z^u \le w\\
										& y \quad free\\
										&z^l, z^u \ge 0 \\
					\text{(CS)} \quad & y_{h(e)} - y_{t(e)} > w_e \Rightarrow x_e = u_e\\
									  & y_{h(e)} - y_{t(e)} < w_e \Rightarrow x_e = l_e
				\end{align}

				There is an alternative way of circulation optimality for a circulation problem. We define a \textbf{kilter-diagram} as follows.

				For every edge construct the following:
				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}
						\draw [arrow] (0, 0) -- (0, 3);
						\draw [arrow] (0, 0) -- (3, 0);
						\draw [link, line width=0.6 mm] (1, -1) -- (1, 1) -- (2, 1) -- (2, 3);
						\draw node at (0, 3) [above] {$y_{h(e)} - y_{t(e)}$};
						\draw node at (3, 0) [above] {$x_e$};
						\draw node at (1, -0.5) [left] {$l_e$};
						\draw node at (2, 0) [above] {$u_e$};
						\draw node at (0, 1) [left] {$w_e$};
					\end{tikzpicture}
				\end{figure}

				For each point $(x_e, y_{h(e)} - y_{t(e)})$ we define a \textbf{kilter-number} $k_e$, be the minimum positive distance change in $x_e$ required to put in on the kilter line.
				\begin{example}
					For edge $e: w_e = 2, l_e = 0, u_e = 3$, assume $x_e = 2, y_{h(e)} - y_{t(e)} = 3$, then $k_e = 1$
				\end{example}

				\begin{figure}[!ht]
					\centering
					\begin{tikzpicture}
						\draw [arrow] (0, 0) -- (0, 4);
						\draw [arrow] (0, 0) -- (4, 0);
						\draw [link, line width=0.6 mm] (0, -1) -- (0, 2) -- (3, 2) -- (3, 4);
						\draw node at (0, 4) [above] {$y_{h(e)} - y_{t(e)}$};
						\draw node at (4, 0) [above] {$x_e$};
						\draw node at (0, -0.5) [left] {0};
						\draw node at (3, 0) [above] {3};
						\draw node at (0, 2) [left] {2};
						\fill (2, 3) circle [radius=2 pt];
						\draw node at (2, 3) [above, xshift=-0.6 cm] {$(x_e, y_{h(e)} - y_{t(e)})$};
						\draw node at (2, 3) [below] {(2, 3)};
						\draw [link, dashed] (2, 3) -- (3, 3);
					\end{tikzpicture}
				\end{figure}

				\begin{lemma}
					If for every circulation $x$ and vertex number $y$ we have $\sum_{e \in A} k_e = 0$, then $x$ is optimal.
				\end{lemma}

				\begin{proof}
					Since $k_e$ is a nonnegative number, then the only way that $\sum_{e \in A} k_e = 0$ is $k_e = 0, \forall e\in A$, which means $\forall e\in A, l_e \le x_e \le u_e$. Furthermore, the complementary slackness are satisfied.
				\end{proof}

				General idea of algorithm follows. Suppose we are given a circulation $x$ and vertex-numbers $y$ (we do not require feasibility). Usually we pick $x=0, y=0$. If every edge is in kilter-line then we are optimal.

				Otherwise there is at least one edge $e^*$ that is out-of kilter. The algorithm consist of two phases, one called \textbf{flow-change} phase (horizontally), then other \textbf{number-change} phase (vertically).

				In the flow-change phase, we want to find a new circulation for an out-of-kilter edge $e^*$ say $\hat{e}$ such that we reduce the kilter number $k_{e^*}$, without increasing any other kilter number for other edges.

				To do this, denote the edges of $e^*$ to be $s$ and $t$, where such that $k_{e^*}$ will be decreased by increasing the flow from $s$ to $t$ on $e^*$.

				If $e^*=(s, t)$ this will accomplished by increasing $x_{e^*}$ and if $e=(t, s)$ it is accomplished by decreasing $x_{e^*}$. 

				To do this we look for an $(s, t)$-path $p$ of the following edges.

				\begin{itemize}
					\item If $e$ is forward in $p$, then increasing $x_e$ does not increase $k_e$ and
					\item If $e$ is reversed in $p$, then decreasing $x_e$ dose not increase $k_e$
				\end{itemize}

				In terms of kilter diagram, an arc satisfies ``forward'' if it is forward and in left side of kilter line, and it satisfies ``reversed'' if it is reverse and in right side of kilter line.

				Suppose we can not find such a path. From $s$ to $t$, let $x$ be the vertices that can decrease by an augmenting path. Then either we can change the vertex numbers $y$ so that $\sum_{e\in A} k_e$ does not increase but $x$ does, or we can show that problem is infeasible.

				INPUT a minimum circulation problem $(D, l, u, w)$ a circulation $x$ and vertex-numbers $y$\\
				OUTPUT conclusion that $(D, l, u, w)$ is infeasible or an minimum weighted flow.

				Step 1: If every arc is in kilter $(k_e = 0, \forall e\in A)$. Stop with $x$ is optimal. Otherwise let $e^*$ be an out-of-kilter arc. If increasing $x_{e^*}$ decreases $k_{e^*}$ set $s=h(e^*)$ and $t(e^*)$ otherwise set $s=t(e^*)$ and $t=h(e^*)$

				Step 2: If there exists an $(s, t)$ augmenting path $p$ then goto Step 3, otherwise goto Step 4.

				STEP 3: Set $y_e = y_{h(e)} - y_{t(e)}, e\in A$
				Set $\Delta_1 = \min\{u_e - x_e: e \text{ is forward and } y_e \ge w_e\}$
				Set $\Delta_2 = \min\{l_e - x_e: e \text{ is forward and } y_e < w_e\}$
				Set $\Delta_3 = \min\{x_e - l_e: e \text{ is reverse and } y_e \le w_e\}$
				Set $\Delta_4 = \min\{x_e - u_e: e \text{ is reverse and } y_e > w_e\}$
				$\Delta = \min\{\Delta_i, i = 1, 2, 3, 4\}$

				Increase $x_e$ by $\Delta$ on each forward arc in $p$, decrease $x_e$ by $\Delta$ on each reverse arc in $p$.

				If $e^* = (s, t)$ decrease $x_{e^*}$ by $\Delta$, otherwise increase $x_{e^*}$ by $\Delta$

				If $k_{e^*} > 0$ goto Step 2. otherwise goto Step 1.

				Step 4: Let $X$ be the set of vertices reachable from $s$ by augmenting paths, then $t \notin X$, if every arc $e$ with $h(e) \in X$ has $x_e \le l_e$ and every arc $e$ with $t(e) \in X$ has $x_e \ge u_e$, and at least one of the above inequality is strict, then Stop with problem infeasible

				Otherwise set
				$\delta_1 = \min\{w_e - y_e: t(e) \in X, y_e < w_e, x_e \le u_e \neq l_e\}$
				$\delta_2 = \min\{y_e - w_e: h(e) \in X, y_e > w_e, x_e \ge u_e \neq l_e\}$
				$\delta = \min\{\delta_1, \delta_2\}$

				Set $y_i = y_i + \delta$ for $i \notin X$

				If $k_{e^*} > 0$, goto Step 2, otherwise goto Step 1.

				Out-of-kilter takes $O(|E||V|K)$ where $K = \sum_{e\in A} k_e$. However, there is an algorithm called \textbf{scaling algorithm} that uses out-of-kilter as subroutine that runs in $O(R|E|^2|V|)$ where $R = \lceil \max\{\log_2 u_e: e\in A\}\rceil$

			\section{Complexity of Different Minimum Weighted Flow Algorithms}
				Let arc capacities between 1 and $U$, costs between $-C$ and $C$
				\begin{table*}[ht]
					\centering
					\begin{tabular} {|c|c|c|c|}
						\hline
						Year & Discoverer & Method & Big $O$ \\
						\hline
						1951 & Dantzig & Network Simplex & $O(E^2V^2U)$ \\
						1960 & Minty, Fulkerson & Out-of-Kilter & $O(EVU)$\\
						1958 & Jewell & Successive Shortest Path & $O(EVU)$ \\
						1962 & Ford-Fulkerson & Primal Dual & $O(EV^2U)$ \\
						1967 & Klein & Cycle Canceling & $O(E^2CU)$ \\
						1972 & Edmonds-Karp, Dinitz & Capacity Scaling & $O(E^2 \log U)$\\
						1973 & Dinitz-Gabow & Improved Capacity Scaling & $O(EV\log U)$ \\
						1980 & Rock, Bland-Jensen & Cost Scaling & $O(EV^2\log C)$\\
						1985 & Tardos & $\epsilon$-optimality & poly($E,V$)\\
						1988 & Orlin & Enhanced Capacity Scaling & $O(E^2)$ \\
						\hline
					\end{tabular}
				\end{table*}

		\chapter{Social Network Analysis}

	\part{Heuristic Optimization}\label{Heur}
		\chapter{}

	\part*{Special Topic: Vehicle Routing Problem}\label{VRP}
		\addcontentsline{toc}{part}{Special Topic: Vehicle Routing Problem}
		\chapter{The Traveling Salesman Problem}
			\section{Formulation}
				Let $G = (V, A)$ be a graph where $V$ is a set of $n$ vertices, and $A$ is a set of arcs (or edges). Let $C = c_{ij}$ be a cost (distance) matrix associated with $A$. The TSP consists of determining a minimum cost (distance) Hamiltonian circle (or cycle) that visits each vertex once and only once. If for all $i, j \in V, c_{ij} = c_{ji}$, then the TSP is symmetrical, otherwise is asymmetrical.

				Define the decision variable $x_{ij}$ as the following
				\begin{equation}
					x_{ij} = \begin{cases}1, &\text{if goes from } i \text{ to } j\\ 0, & \text{otherwise}\end{cases}
				\end{equation}
				There are two famous formulations for TSP, the Dantzig-Fulkerson-Johnson (DFJ) formulation and the Miller-Tucker-Zemlin (MTZ) formulation. The DFJ formulation is
				\begin{align}
					\min \quad &\sum_{(i, j)\in A} c_{ij}x_{ij} \\
					& \sum_{j \in V, (i,j)\in A} x_{ij} = 1, \quad \forall i \in V \label{TSP:formulation:con:degree1}\\
					& \sum_{i \in V, (i,j)\in A} x_{ij} = 1, \quad \forall j \in V \label{TSP:formulation:con:degree2}\\
					& \sum_{j\notin S, i\in S, (i,j)\in A} x_{ij} \ge 1 \text{ or } \sum_{i, j \in S, (i, j) \in A} x_{ij} \le |S| - 1 \nonumber \\
					& \quad \forall S \subset V, 2\le |S| \le n-1 \label{TSP:formulation:con:DFJSubtour}\\
					& x_{ij} \in \{0, 1\}, \forall (i, j) \in A 
				\end{align}

				In the formulation, constraints (\ref{TSP:formulation:con:degree1}) and constraints (\ref{TSP:formulation:con:degree2}) are degree constraints, which specify that every vertex is entered exactly once. Constraints (\ref{TSP:formulation:con:DFJSubtour}) is the sub-tour constraints, they prohibit the formation of sub-tours. $S$ is a non-empty subset of $V$, and has at least 2 vertices. 

				In the MTZ formulation, the sub-tour constraints (\ref{TSP:formulation:con:DFJSubtour}) are replaced by the following:
				\begin{align}
					&u_i - u_j + (n - 1) x_{ij} \le n - 2, \quad i, j = 2, \cdots, n \in V, (i, j) \in A\\
					&1 \le u_i \le n - 1, i \in 2, \cdots, n \in V
				\end{align}

				\notice{The MTZ formulation is weaker than the DFJ formulation in spite of its relative compactness.}

			\section{Sub-tour Searching Algorithm}
				Notice that the number of sub-tour constraints increase exponentially as the number of vertices increase, therefore it is impractical to list all sub-tour constraints when we are solving the TSP. An alternative method is to use Lazy cuts to speed up the solving process, i.e., if there are sub-tours detected in the solution, add corresponded cuts to eliminate those sub-tours. Finding sub-tours is essentially finding components in a graph. In the graph $G=(V, A)$, let $\bar{G}=(V, \bar{A})$ be the connected components of graph, where
				\begin{equation}
					\bar{G}=(G, \bar{A}), \bar{A}=\{(i, j) \in A | \bar{x_{ij}}=1\}
				\end{equation}
				denote
				\begin{equation}
					\bar{FS}(i) = \{(i,j)\in \bar{A}\}
				\end{equation}
				Then the algorithm to find all sub-tour is the following:
				\begin{algorithm}[!ht]
					\caption{Sub-tour Searching Algorithm}
					\begin{algorithmic}[1]
						\State $K = \emptyset$
						\State $d_i = 0, \forall i \in V$
						\For {$i\in V$}
							\State $C = \emptyset$
							\State $Q = \emptyset$
							\If {$d_i == 0$}
								\State $d_i = 1$
								\State $C = C\cup \{i\}$
								\State Q.append(i)
								\While {$Q\ne \emptyset$}
									\State v = Q.pop()
									\For {$u \in \bar{FS}(v)$}
										\If {$d_u == 0$}
											\State $d_u = 1$
											\State $C = C \cup \{u\}$
											\State Q.append(u)
										\EndIf
									\EndFor
								\EndWhile
							\EndIf
							\State $K=K\cup C$
						\EndFor
					\end{algorithmic}
				\end{algorithm}

			\section{Polynomially Solvable Special Cases of TSP}

			\section{Lower Bounds of TSP}
				\subsection{The Assignment Lower Bound}


				\subsection{The Shortest Spanning Tree (Arborescence) Bound}

		\chapter{The Vehicle Routing Problem}

		\chapter{The Capacitate Vehicle Routing Problem}
			\section{Exact Algorithms for VRP}

			\section{Heuristics for VRP}

		\chapter{The Vehicle Routing Problem with Time Windows}

		\chapter{The Vehicle Routing Problem with Pickup-and-Delivery}

		\chapter{Stochastic Vehicle Routing Problem}

		\chapter{Dynamic Vehicle Routing Problem}

	\part{Nonlinear Programming}\label{Nonlinear}
		\chapter{Optimality Conditions and Duality}
			\section{The Fritz John Optimality Conditions}

			\section{The Karush-Kuhn-Tucker Optimality Conditions}

			\section{Constraint Qualification}

			\section{Lagrangian Duality and Saddle Point Optimality Condition}

		\chapter{Unconstrained Optimization}

		\chapter{Quadratic Programming}

		\chapter{Penalty and Barrier Functions}

	\part{Algorithms}\label{Algo}
		\chapter{Computational Complexity}
			\section{Asymptotic Notation}
				\subsection{Asymptotic Analysis}
					\begin{definition}[asymptotically positive function]
						$f: \mathbb{N} \rightarrow \mathbb{R}$ is an asymptotically positive function if $\exists n_0 > 0$ such that $\forall n > n_0$ we have $f(n) > 0$
					\end{definition}

				\subsection{\texorpdfstring{$O$}{O}-Notation, \texorpdfstring{$\Omega$}{Omega}-Notation and \texorpdfstring{$\Theta$}{Theta}-Notation}
					\begin{definition}[$O$-Notation]
						For a function $g(n)$, 
						\begin{equation*}
							O(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that } f(n) \le cg(n), \forall n\ge n_0\}
						\end{equation*}
						$O$-Notation also known as asymptotic upper bound. 
					\end{definition}				

					\begin{definition}[$\Omega$-Notation]
						For a function $g(n)$, 
						\begin{equation*}
							\Omega(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that } f(n) \ge cg(n), \forall n\ge n_0\}
						\end{equation*}
						$\Omega$-Notation also known as asymptotic lower bound.
					\end{definition}
					
					\begin{definition}[$\Theta$-Notation]
						For a function $g(n)$, 
						\begin{equation*}
							\Theta(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that }  c_1g(n) \le f(n) \le c_2g(n), \forall n\ge n_0\}
						\end{equation*}
						$\Omega$-Notation and $\Theta$-Notation are not used very often when we talk about running times.
					\end{definition}
					
					\begin{definition}[$o$-Notation]
						For a function $g(n)$, 
						\begin{equation*}
							o(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that } f(n) < cg(n), \forall n\ge n_0\}
						\end{equation*}					
					\end{definition}

					\begin{definition}[$\omega$-Notation]
						For a function $g(n)$, 
						\begin{equation*}
							\omega(g(n)) = \{f: \exists c > 0, n_0 > 0 \text{ such that } f(n) > cg(n), \forall n\ge n_0\}
						\end{equation*}					
					\end{definition}

					\notice{$O(g)$, $\Omega(g)$ and $\Theta(g)$ are sets, we use ``$=$'' to represent ``$\in$''. In here ``$=$'' is asymmetric. Equality such as $O(n^3) = n^3 + n$ is incorrect.}

					\begin{example}
						The following are some examples
						\begin{align}
							\centering
							\begin{tabular}{|c|c||c|c|c|}
								\hline
								$f(n)$ & $g(n)$ & $O$ & $\Omega$ & $\Theta$\\
								\hline
								$4n^2 + 3n$ & $n^3 - 2n + 3$ & Yes & No & No \\
								\hline
								$\lg^{10} n$ & $n^{0.1}$ & Yes & No & No \\
								\hline
								$\log_{10} n$ & $\lg(n^3)$ & Yes & Yes & Yes \\
								\hline
								$\lceil \sqrt{10n + 100} \rceil$ & $n$ & Yes & No & No \\
								\hline
								$n^3 -100n$ & $10n^2\lg n$ & No & Yes & No\\
								\hline
								$2^n$ & $2^{\frac n2}$ & No & Yes & No \\
								\hline
								$\sqrt{n}$ & $n^{\sin n}$ & No & No & No\\
								\hline
							\end{tabular}
						\end{align}
					\end{example}

					\begin{theorem}
						Let $f$ and $g$ be two functions that
						\begin{equation*}
							\lim_{n\rightarrow \infty} \frac{f(n)}{g(n)} = c > 0
						\end{equation*}
						Then $f(n) = \Theta(g(n))$
					\end{theorem}

					\begin{proof}
						Since $\lim_{n\rightarrow \infty} \frac{f(n)}{g(n)}$ exists and positive, there is some $n_0$ beyond which the ratio is always between $\frac12 c$ and 2c. Thus,
						\begin{align}
							& \forall n > n_0, f(n) \le 2cg(n) \Rightarrow f(n) = O(g(n)) \\
							& \forall n > n_0, f(n) \ge \frac12cg(n) \Rightarrow f(n) = \Omega(g(n))
						\end{align}
					\end{proof}

					A set of properties:
					\begin{align}
						& f(n) = O(g(n)) \iff g(n) = \Omega(f(n))\\
						& f(n) = \Theta(g(n)) \iff f(n) = O(g(n)) \text{ and } g(n) = O(f(n))
					\end{align}

					Another set of properties:
					\begin{align}
						& f = O(g), g = O(h) \Rightarrow f=O(h) \\
						& f = \Omega(g), g = \Omega(h) \Rightarrow f=\Omega(h) \\
						& f = \Theta(g), g = \Theta(h) \Rightarrow f=\Theta(h)
					\end{align}

					\begin{theorem}
						If $f_i = O(h)$, for finite number of $i \in K$, then $\sum_{i\in K} f_i = O(h)$
					\end{theorem}

					Proof is trivia.

					\notice{Function $f$ and $g$ not necessarily have relation $f=O(g)$ or $g=O(f)$. E.g., for $f = \sqrt{n}$ and $g = n^{\sin n}$, $f \notin O(g)$ and $g \notin O(f)$ }

			\section{Common Running Times}
				The following are examples of common running times.
				\begin{align*}
					\centering
					\begin{tabular}{|c|p{10cm}|}
						\hline
						Running Time & Examples\\
						\hline
						$O(n)$ & Scan through a list to find a element matching with input\\
						\hline
						$O(\lg n)$ & Binary search\\
						\hline
						$O(n^2)$ & Scan through every pair of elements, $\binom{n}{2}$\\
						\hline
						$O(n^3)$ & Matrix multiplication by definition\\
						\hline
						$O(n\lg n)$ & Many divide and conquer algorithm which in each step iteratively divide the problem into two part and solve the subproblem, for example mergesort.\\
						\hline
						$O(n!)$ & Enumerate all permutation, for example Hamiltonian Cycle Problem\\
						\hline
						$O(c^n)$ & Enumerate all elements in power set. ($O(2^n)$)\\
						\hline
						$O(n^n)$ & Enumerate all combinations. (Can't find good example yet)\\
						\hline
					\end{tabular}
				\end{align*}
				Here is a comparison between running times: $\lg n < \sqrt{n} < n < n\lg n < n^2 < n^{\sqrt{n}} < 2^n < e^n < n! < n^n$

		\chapter{General Paradigms}
			This chapter we are going to discussion three types of strategies for algorithm design, greedy algorithm, divide-and-conquer, and dynamic programming.
			\begin{itemize}
				\item Greedy algorithm
				\begin{itemize}
					\item Make a greedy choice
					\item Prove that the greedy choice is safe
					\item Reduce the problem to a sub-problem and solve it iteratively
					\item Usually for optimization problems.
				\end{itemize}
				\item Divide-and-Conquer
				\begin{itemize}
					\item Break a problem into many independent sub-problems
					\item Solve each sub-problem separately
					\item Combine solutions for sub-problems to form a solution for the origin one
					\item Usually used to design more efficient algorithm
				\end{itemize}
				\item Dynamic Programming
				\begin{itemize}
					\item Break up a problem into many overlapping sub-problems
					\item Build solutions for larger and larger sub-problems
					\item Use a table to store solutions for sub-problems for reuse
				\end{itemize}
			\end{itemize}

			\section{Greedy Algorithms}
				\subsection{Introduction}
					Greedy algorithm solve the problem incrementally. Its often for optimization problems. Solving optimization problem typically requires a sequences of steps, at each step, an irrevocable decision will be made, and makes the choice looks the best at the moment. Based on that, a small instance will be added to the problem and we solve it again.

					Greedy algorithm do not always yield optimal solution, but it usually can provide a relatively acceptable computational complexity. They often run in polynomial time due to the incrementally of instances. Sometimes even if we cannot guarantee the solution is optimal, we still use it in optimization, because of its cheap computation burden. One of the examples will be the constructive heuristic of VRP problems.

					Greedy algorithm usually gives polynomial time complexity, but that is not all the cases. In simplex method, each pivot is greedily searching through for the extreme point. Although simplex method usually gives us traceable computation running time, however, it is not a polynomial algorithm.

					The following is a very rough sketch of generic greedy algorithm
					\begin{algorithm}[!ht]
						\caption{Generic Greedy Algorithm}
						\begin{algorithmic}[1]
							\While {The instance is non-trivial}
								\State Make the choice using the greedy strategy
								\State Reduce the instance
							\EndWhile
						\end{algorithmic}
					\end{algorithm}

					If we can prove the following, we can claim the greedy algorithm. First, it need to prove that for ``current moment'', the strategy is safe, i.e., there is always an optimum solution that agrees with the decision made according to the strategy, this is usually difficult. Then, it need to show that the remaining task after applying the strategy is to solve a/many smaller instance(s) of the same problem.

					Although greedy algorithm is intuitive and usually leads to satisfactory complexity, however, for most of the problems there is \textbf{no} natural greedy algorithm that works.
				\subsection{Examples}
					\subsubsection{Interval Scheduling}
						For $n$ jobs, job $i$ starts at time $s_i$, and finishes at time $f_i$. $i$ and $j$ are compatible if $[s_i, f_i)$ and $[s_j, f_j)$ are disjoint. Find the maximum-size of mutually compatible jobs.

						The following is the greedy algorithm to solve this problem
						\begin{algorithm}[!ht]
							\caption{Interval Scheduling, $S(s, f, n)$}
							\begin{algorithmic}[1]
								\State Sort jobs by $f$
								\State $t \gets 0$, $S\gets \emptyset$
								\For {every $j\in[n]$ according to non-decreasing order of $f_j$}
									\If {$s_j \ge t$}
										\State $S \gets S \cup \{j\}$
										\State $t \gets f_j$
									\EndIf
								\EndFor
								\Return S
							\end{algorithmic}
						\end{algorithm}

						Now we proof the that it is safe to schedule the job $j$ with the earliest finish time, i.e., there is an optimum solution where the job $j$ with the earliest finish time is scheduled.

						\begin{proof}
							For arbitary optimum solution $S$, one of the following cases will happen:\\
							\textbf{Case 1: } $S$ contains $j$, done\\
							\textbf{Case 2: } $S$ does not contain $j$. Then the first job in $S$ can be replaced by $j$ to obtain another optimum schedule $S^\prime$.
						\end{proof}

					\subsubsection{Unit-length interval covering}
						Given a set of $n$ points $X = \{x_1, x_2, \cdots, x_n\}$ on the real line, WLOG, assuming the points have already been sorted. We want to use the smallest number of unit-length closed intervals to cover all the points in $X$. 

						The following is a greedy algorithm to find the set of unit-length intervals that cover all the points in real line.
						\begin{algorithm}[!ht]
							\caption{Cover points with unit-length intervals}
							\begin{algorithmic}[1]
								\State Initial, $S \gets \emptyset$
								\For {$i = 1, 2, ..., n$}
									\If {$x_i$ is not covered by any unit-length intervals}
										\State Add one unit-length interval starting from $x_i$, i.e., $S \gets S \cup \{x_i\}$
									\EndIf
								\EndFor
							\end{algorithmic}
						\end{algorithm}

						This strategy (algorithm) is a greedy algorithm because it build up the solution in steps, in each iteration it considers one more point to be covered, i.e., it is optimal for each step in the iteration. Now we prove this algorithm is ``safe''.

						\begin{proof}
							First we consider the case where there is only one point on the real line. Then the optimal number of unit-length interval will be 1, according to the algorithm, that interval will be started at that point.

							Then assuming for the case where there are $k$ points from left to right, i.e., $X = \{x_1, x_2, \cdots, x_k\}$, and $p$ unit-intervals is already the minimal number and placed by the algorithm, then the $(k+1)^{th}$ point can only be one of the following cases:

							\textbf{Case 1:} The $(k+1)^{th}$ point is covered by the $p^{th}$ unit-length interval. According to the strategy, no new unit-length interval will be needed, the number of unit-length interval for $k+1$ points will be the same as when there are $k$ points. Therefore in this case for $k+1$ points, $p$ is the minimal (optimal) number. So the strategy is ``safe'' in this case.

							\textbf{Case 2:} The $(k+1)^{th}$ point is not covered by the $p^{th}$ unit-length interval. According to the strategy, there will be one new unit-length interval added. Notice that $p$ unit-length intervals will not be feasible to cover $k+1$ points in this case, because if we move the $p^{th}$ unit-length interval to the right, it will not be able to cover at least one point which overlapped with the starting point of that unit-length interval. Since $p$ is infeasible and $p+1$ unit-length interval is feasible, then $p+1$ is the minimal (optimal) number. So the strategy is ``safe'' in this case.

							Notice that for the $k$ we have mentioned above, $k$ can start from 1 to infinite number of integer. So this strategy is ``safe'' in every cases.
						\end{proof}

			\section{Divide and Conquer}
				\subsection{Introduction}
					The divide-and-conquer algorithm contains three steps: divide, conquer and combine. Step one, divide instances into many smaller instances. Step two, conquer small instance by solving each of smaller instances recursively and separately. Step three, combine solutions to small instances to obtain a solution for the origin large instance.

					Divide and conquer can sometimes solve the problems that greedy algorithm cannot solve, but they often not strong enough to reduce exponential brute-force search down to polynomial time. What usually happen is that they reduce a running time that unnecessarily large, but already polynomial, down to a faster running time.

				\subsection{Master Theorem}
					The Master Theorem is useful in analyzing the running time of divide and conquer algorithm. Assume at each step we divide the origin problem of size $n$ into subproblems of size $n / b$, run for $a$ times of ``itself'' to conquer those subproblems with a combine operation of $O(n^c)$, then the total running time can be derived by the Master Theorem as following:

					\begin{theorem}[Master Theorem]
						For running time in forms of $T(n) = aT(n/b) + O(n^c)$, where $a \ge 1$, $b > 1$, $c \ge 0$ are constants. Then
						\begin{equation}
							T(n) = \begin{cases}
								O(n^{\lg_b a}), \quad & c < \lg_b a\\
								O(n^c\lg n), \quad & c = \lg_b a\\
								O(n^c), \quad & c > \lg_b a
							\end{cases}
						\end{equation}
					\end{theorem}

					Proof of Master theorem using recursion tree
					\begin{figure}[h]
						\centering
						\begin{tikzpicture}[iv/.style={draw,rectangle,minimum size=20pt,inner sep=0pt,text=black}]
							\node[iv]{$n^c$}
								child{
									node[iv]{$(n/b)^c$}
									child{
										node[iv]{$(n/b^2)^c$}
									}
									child{
										node[iv]{$(n/b^2)^c$}
									}
								}
								child [missing]
								child{
									node[iv]{$(n/b)^c$}
									child{
										node[iv]{$(n/b^2)^c$}
									}
									child{
										node[iv]{$(n/b^2)^c$}
									}
								};
						\end{tikzpicture}
					\end{figure}
					\begin{proof}
						The $i^{th}$ level has $a^{i - 1}$ nodes. For the following cases, we can derive the time complexity:\\
						\textbf{Case 1: } $c < \lg_b a$ bottom-level dominates $(\frac{a}{b^c}^{\lg_b n})n^c = O(n^{\lg_b a})$\\
						\textbf{Case 2: } $c = \lg_b a$ all levels have same time $n^c \lg_b n = O(n^c \lg n)$\\
						\textbf{Case 3: } $c > \lg_b a$ top-level dominates $O(n^c)$
					\end{proof}
				\subsection{Examples}
					\subsubsection{Counting Inversions}

			\section{Dynamic Programming}
				\subsection{Introduction}
					The principal of dynamic programming is essentially opposite of the greedy algorithm. Dynamic programming implicitly explores the space of all possible solutions, by carefully decomposing the origin problem into subproblems, and store the solution of those subproblems. Base on those subproblems, then build up larger and larger problem until the origin problem is solved.

				\subsection{Examples}
					\subsubsection{Weighted Interval Scheduling}
						Consider $n$ jobs, job $i$ starts at time $s_i$ and finishes at time $f_i$, each job has a weight of $v_i > 0$. Job $i$ and job $j$ are compatible if $[s_i, f_i)$ and $[s_j, f_j)$ are disjoint. Find the maximum-size subset of mutually compatible jobs. A special case of this problem is when the values of all jobs are equal, which will be the interval scheduling problem as discussed in the greedy algorithm examples.

						Define $p(j)$ as for a job $j$, the largest index $i < j$ such that job $j$ is disjoint with job $i$. Define $opt(i)$ as the optimal value for instance only containing jobs $\{1, 2, \cdots, i\}$.

						Before the algorithm for weighted interval scheduling, sort the jobs by non-decreasing order of finishing time first, in $O(n \lg n)$ The dynamic programming algorithm is as following:
						\begin{algorithm}[h]
							\caption{ComputeOpt(i)}
							\begin{algorithmic}[1]
								\If {$i == 0$}
									\State \Return 0
								\Else
									\State \Return $\max\{v_i + ComputeOpt(p(i)), ComputeOpt(i - 1)\}$
								\EndIf
							\end{algorithmic}
						\end{algorithm}

						For finding $p(i)$ for one job, it takes $O(\lg n)$ by binary search. For $n$ jobs the complexity will be $O(n\lg n)$.

						The running time of this algorithm can be exponential in $n$, if each time $ComputeOpt(i)$ is computed repeatedly. However, if we store the value of each $ComputeOpt(i)$ and reuse it, we can reduce the running time to $O(n)$.

						We can recover the set of jobs for given (valid) $opt(i)$ by the following algorithm, assuming the jobs has been sorted by non-decreasing order of finishing time.
						\begin{algorithm}[h]
							\caption{RecoverJobs()}
							\begin{algorithmic}[1]
								\State Compute $p_1, p_2, \cdots, p_n$
								\State $opt(0) \gets 0$
								\For {$i \gets 1$ to $n$}
									\If {$opt(i - 1) \ge v_i + opt(p_i)$}
										\State $opt(i) \gets opt(i - 1)$
										\State $b[i] \gets N$								
									\Else
										\State $opt(i) \gets v_i + opt(p_i)$
										\State $b[i] \gets Y$
									\EndIf
								\EndFor
								\State $i \gets n, S\gets \emptyset$
								\While {$i \ne 0$}
									\If {$b[i] == N$}
										\State $i \gets i - 1$
									\Else
										\State $S \gets S \cup \{i\}$
										\State $i \gets p_i$
									\EndIf
								\EndWhile
								\State \Return S
							\end{algorithmic}
						\end{algorithm}

						The above algorithm is using memorized recursion to solve the problem, there is a second efficient algorithm to solve the Weighted Interval Scheduling Problem.

					\subsubsection{Subset Sum Problem}
						Given an integer bound $W > 0$ and a set of $n$ items, each with an integer weight $w_i > 0$, find a subset $S$ of items that
						\begin{align}
							\max \quad &\sum_{i \in S} w_i\\
							\text{s.t.} \quad & \sum_{i \in S} w_i \le W
						\end{align}

						Consider this instance, $i$ items, $(w_1, w_2, \cdots, w_i)$, budget is $W^\prime$. For $opt[i, W^\prime]$ there can only be one of the following cases:

						\textbf{Case 1: }The value of optimum solution does not contain $w_i$, then $opt[i, W^\prime] = opt[i - 1, W^\prime]$, else

						\textbf{Case 2: }The value of optimum solution contains $w_i$, then $opt[i, W^\prime] = opt[i - 1, W^\prime - w_i] + w_i$

						The algorithm is as following
						\begin{algorithm}[h]
							\caption{Optimum Subset}
							\begin{algorithmic}[1]
								\For {$W^\prime \gets 0$ to $W$}
									\State $opt[0, W^\prime] \gets 0$
								\EndFor
								\For {$i \gets 1$ to $n$}
									\For {$W^\prime \gets 0$ to $W$}
										\State $opt[i, W^\prime] \gets opt[i - 1, W^\prime]$
										\State $b[i, W^\prime] \gets N$
										\If {$w_i \le W^\prime$ and $opt[i - 1, W^\prime - w_i] + w_i \ge opt[i, W^\prime]$}
											\State $opt[i, W^\prime] \gets opt[i - 1, W^\prime - w_i] + w_i$
											\State $b[i, W^\prime] \gets Y$
										\EndIf
									\EndFor
								\EndFor
								\State \Return $opt[n, W]$
							\end{algorithmic}
						\end{algorithm}

						\begin{algorithm}[h]
							\caption{Recover the Optimum Set}
							\begin{algorithmic}[1]
								\State $i \gets n, W^\prime \gets W, S \gets 0$
								\While {$i > 0$}
									\If {$b[i, W^\prime] == Y$}
										\State $W^\prime \gets W^\prime - w_i$
										\State $S \gets \cup \{i\}$
									\EndIf
									\State $i \gets i - 1$
								\EndWhile
								\State \Return S
							\end{algorithmic}
						\end{algorithm}

					\subsubsection{Optimum Binary Search Tree}
						Given $n$ elements $e_1 < e_2 < \cdots < e_n$, $e_i$ has frequency $f_i$, the goal is to build a binary search tree for $\{e_1, e_2, \cdots, e_n\}$ with the minimum accessing cost
						\begin{equation}
							\sum_{i = 1}^n f_i d_i
						\end{equation}
						Where $d_i$ is the depth of $e_i$ in the tree.

						Suppose we choose $e_k$ to be the root, than $e_1, e_2, \cdots, e_{k-1}$ are in left-sub tree, and $e_{k+1}, \cdots, e_n$ will be in right-sub tree, then, denote the cost for the tree and subtrees to be $C, C_L, C_R$ respectively
						\begin{align}
							C &= \sum_{j = 1}^n f_j d_j\\
							&= \sum_{j = 1}^n f_j + \sum_{j = 1}^n f_j(d_j - 1)\\
							&= \sum_{j = 1}^n f_j + \sum_{j = 1}^{k - 1} f_j(d_j - 1) + \sum_{j = k + 1}^n f_j(d_j - 1)\\
							&= \sum_{j = 1}^n f_j + C_L + C_R
						\end{align}

						Denote $opt(i, j)$ to be the optimal value for the instance of $(e_i, e_{i + 1}, \cdots, e_j)$, then, for every $i, j$ such that $1 \le i \le j \le n$
						\begin{equation}
							opt(i, j) = \sum_{k = i}^j f_k + \min_{k:i \le k \le j}\{opt(i, k - 1) + opt(k + 1, j)\}
						\end{equation}

						Here is an example
						\begin{example}
							Consider the following optimum binary search tree instance. We have 5 elements $e_1, e_2, e_3, e_4$ and $e_5$ with $e_1 < e_2 < e_3 < e_4 < e_5$ and their frequencies are $f_1 = 5, f_2 = 25, f_3 = 15, f_4 = 10$ and $f_5 = 30$.  Recall that the goal is to find a binary search tree for the 5 elements so as to minimize $\sum_{i=1}^5 \textrm{depth}(e_i) f_i$, where $\textrm{depth}(e_i)$ is the depth of the element $e_i$ in the tree.  You need to output the best tree as well as its cost. You can try to complete the following tables and show the steps.  In the two tables, $opt(i,j)$ is the cost of the best tree for the instance containing $e_i, e_{i+1}, ..., e_j$ and $\pi(i, j)$ is the root of the best tree.
							\begin{table}[H]
								\centering
								\begin{tabular}{|c|c|c|c|c|c|}
									\hline
									\backslashbox{$i$}{$opt(i, j)$}{$j$} &  1 & 2  & 3 & 4 & 5 \\ \hline
									1 & 5 & 35 & 65 & 95 & 170\\\hline
									2&  & 25 & 55 & 85 & 155 \\\cline{1-1}\cline{3-6}
									3& \multicolumn{2}{c|}{} & 15 & 35 & 90\\\cline{1-1}\cline{4-6}
									4& \multicolumn{3}{c|}{} & 10 & 50 \\\cline{1-1}\cline{5-6}
									5& \multicolumn{4}{c|}{} & 30 \\\cline{1-1}\cline{6-6}
								\end{tabular}\qquad\qquad
								\begin{tabular}{|c|c|c|c|c|c|}
									\hline
									\backslashbox{$i$}{$\pi(i, j)$}{$j$} &  1 & 2  & 3 & 4 & 5 \\ \hline
									1 & 1 & 2 & 2 & 2 & 3\\\hline
									2&  & 2 & 2 & 2 (or 3)& 3 \\\cline{1-1}\cline{3-6}
									3& \multicolumn{2}{c|}{} & 3 & 3& 5\\\cline{1-1}\cline{4-6}
									4& \multicolumn{3}{c|}{} & 4 & 5\\\cline{1-1}\cline{5-6}
									5& \multicolumn{4}{c|}{} & 5 \\\cline{1-1}\cline{6-6}
								\end{tabular}
								\caption{$opt$ and $\pi$ tables for the optimum binary search tree instance. For cleanness of the table, we assume $opt(i, j) = 0$ if $j < i$ and there are not shown in the left table.}
							\end{table}
							\vspace*{-30pt}
							\begin{align*}
								opt(1, 2) &= \min\{0 + opt(2, 2), opt(1, 1) + 0\} + (f_1 + f_2) = \min\{25, 5\} + 5 + 25 = 35\\
								opt(2, 3) &= \min\{0 + opt(3, 3), opt(2, 2) + 0\} + (f_2 + f_3) = \min\{15, 25\} + 25 + 15 = 55\\
								opt(3, 4) &= \min\{0 + opt(4, 4), opt(3, 3) + 0\} + (f_3 + f_4) = \min\{10, 15\} + 15 + 10 = 35\\
								opt(4, 5) &= \min\{0 + opt(5, 5), opt(4, 4) + 0\} + (f_4 + f_5) = \min\{30, 10\} + 10 + 30 = 50\\
								opt(1, 3) &= \min\{0 + f(2, 3), f(1, 1) + f(3, 3), f(1, 2) + 0\} + (f_1 + f_2 + f_3) \\
								&= \min\{55, 20, 35\} + 5 + 25 + 15 = 65 \\
							    opt(2, 4) &= \min\{0 + f(3, 4), f(2, 2) + f(4, 4), f(2, 3) + 0\} + (f_2 + f_3 + f_4) \\ 
							    &= \min\{35, 35, 55\} + 25 + 15 + 10 = 85 \\
								opt(3, 5) &= \min\{0 + f(4, 5), f(3, 3) + f(5, 5), f(3, 4) + 0\} + (f_3 + f_4 + f_5) \\
								&= \min\{50, 45, 35\} + 15 + 10 + 30 = 90 \\
								opt(1, 4) &= \min\{0 + f(2, 4), f(1, 1) + f(3, 4), f(1, 2) + f(4, 4), f(1, 3) + 0\} \\ & \qquad + (f_1 + f_2 + f_3 + f_4) = 95\\
								&= \min\{85, 40, 45, 65\} + 5 + 25 + 15 + 10\\
								opt(2, 5) &= \min\{0 + f(3, 5), f(2, 2) + f(4, 5), f(2, 3) + f(5, 5), f(2, 4) + 0\} \\ & \qquad + (f_2 + f_3 + f_4 + f_5) = 155\\
								&= \min\{90, 75, 85, 85\} + 25 + 15 + 10 + 30 \\
								opt(1, 5) &= \min\{0 + f(2, 5), f(1, 1) + f(3, 5), f(1, 2) + f(4, 5), f(1, 3) + f(5, 5) + f(1, 4) + 0\} \\ & \qquad + (f_1 + f_2 + f_3 + f_4 + f_5) \\
								&= \min\{155, 95, 85, 95, 95\} + 5 + 25 + 15 + 10 + 30 = 170
							\end{align*}
						\end{example}

					\subsubsection{Matrix Chain Multiplication}
						Given $n$ matrices $A_1, A_2, \cdots, A_n$ of sizes $r_1 \times c_1, r_2 \times c_2, \cdots, r_n \times c_n$ where $c_i = r_{i + 1}$ for every $i = 1, 2, \cdots, n - 1$. The Matrix Chain Multiplication finds the order of computing $A_1A_1\cdots A_n$ with the minimum number of multiplications.

						The idea is as following. Assume the last step in the multiplication is $(A_1A_2\cdots A_i)(A_{i + 1}A_{i + 2}\cdots A_n)$. The cost of this step will be $r_i \times c_i \times c_n$. So we need to optimally solve two sub-instances, i.e., $(A_1A_2\cdots A_i)$ and $(A_{i + 1}A_{i + 2}\cdots A_n)$.

						Denote $opt[i, j]$ as the minimum cost of computing $A_i A_{i+1} \cdots A_j$, then
						\begin{equation}
							opt[i, j] = \begin{cases}
								0, & \quad i = j\\
								\min_{k: i \le k < j} (opt[i, k] + opt[k + 1, j] + r_i \times c_k \times c_j), & \quad j < j
							\end{cases}
						\end{equation}

						The algorithm is as following
						\begin{algorithm}[h]
							\caption{MatrixChainMultiplication}
							\begin{algorithmic}[1]
								\State $opt[i, i] \gets 0 \quad \forall ~ i = 1, 2, \cdots, n$
								\For {$l \gets  2$ to $n$}
									\For {$i \gets 1$ to $n - l + 1$}
										\State $j \gets i + l - 1$
										\State $opt[i, j] \gets \infty$
										\For {$k \gets i$ to $j - 1$}
											\If {$opt[i, k] + opt[k + 1, j + r_ic_kc_j < opt[i, j]$}
												\State $opt[i, j] \gets opt[i, k] + opt[k + 1, j] + r_i c_k c_j$
												\State $\pi[i, j] \gets k$
											\EndIf
										\EndFor
									\EndFor
								\EndFor
								\State \Return $opt[1, n]$
							\end{algorithmic}
						\end{algorithm}

						With above algorithm, to construct the optimal solution, the follow algorithm is needed
						\begin{algorithm}[h]
							\caption{PrintOptimalOrder(i, j)}
							\begin{algorithmic}[1]
								\If {$i == j$}
									\State Print($A_i$)
								\Else
									\State Print("(")
									\State \texttt{PrintOptimalOrder($i, \pi[i, j]$)}
									\State \texttt{PrintOptimalOrder($\pi[i, j] + 1, j$)}
									\State Print(")")
								\EndIf
							\end{algorithmic}
						\end{algorithm}
					
			\section{Compare between three paradigms}

		\chapter{Sorting}
			\section{Exchange Sorts}
				\subsection{Bubble Sort}

				\subsection{Cocktail Shaker Sort}

				\subsection{Odd-even Sort}

				\subsection{Comb Sort}

				\subsection{Gnome Sort}

				\subsection{Quicksort}
					The idea of quicksort is to recursively divide an array into two subarrays, one with smaller number and one with large number, and concatenate  the subarrays after all subarrays are singletons.

					The most ideal way is to divide the subarrays equally, which requires an algorithm to find the median of an array of size $n$ in $O(n)$ time.

					The quicksort algorithm is as following
					\begin{algorithm}[h]
						\caption{Quicksort(A, n)}
						\begin{algorithmic}[1]
							\If {$n = 1$}
								\Return A
							\EndIf
							\State Initial, $A_L \gets \emptyset, A_H \gets \emptyset$
							\State $x \gets$ \texttt{Median(A)}
							\For {$element \in A$}
								\If {$element \le x$}
									\State $A_L \gets A_L \cup \{element\}$
								\Else
									\State $A_H \gets A_H \cup \{element\}$
								\EndIf
							\EndFor
							\State $B_L \gets$ \texttt{Quicksort($A_L$, $A_L$.size)}
							\State $B_H \gets$ \texttt{Quicksort($A_H$, $A_H$.size)}
							\State $t \gets$ number of times $element$ appear in $A$
							\Return $B_L + element^t + B_H$
						\end{algorithmic}
					\end{algorithm}

					Running time $T(n) = 2T(n/2) + O(n)$, $T(n) = O(n\lg n)$

					For the median finding algorithm
					\begin{algorithm}[h]
						\caption{Median(A)}
						\begin{algorithmic}[1]
							\State (To be finished)
						\end{algorithmic}
					\end{algorithm}

					If we don't use the median finding algorithm, we can modify the \texttt{Quicksort(A, n)} to be a random algorithm by replacing line 3 by $x \gets$ \texttt{RandomElement(A)}. This modified algorithm has an expected running time of $O(n\lg n)$. The worse case running time is $O(n^2)$.

					Based on Quicksort algorithm, we can define an $O(n)$ algorithm to find the $i$th smallest number in $A$, given that we have an $O(n)$ algorithm to find median of array.

					The selection algorithm is as follows
					\begin{algorithm}[h]
						\caption{Selection(A, n, i)}
						\begin{algorithmic}[1]
							\If {$n = 1$}
								\State \Return A
							\Else
								\State $x \gets$ \texttt{Median(A)}
								\For {$element \in A$}
									\If {$element \le x$}
										\State $A_L \gets A_L \cup \{element\}$
									\Else
										\State $A_H \gets A_H \cup \{element\}$
									\EndIf
								\EndFor
								\If {$i \le A_L.size$}
									\State \Return \texttt{Selection($A_L, A_L.size, i$)}
								\ElsIf {$i > n - A_R.size$}
									\State \Return \texttt{Selection($A_R, A_R.size, i - (n - A_R.size)$)}
								\Else
									\State \Return $x$
								\EndIf
							\EndIf
						\end{algorithmic}
					\end{algorithm}

					Similarly, without \texttt{Median(A)}, we can replace line 4 by $x \gets$ \texttt{RandomElement(A)}. Then the expected running time will be $O(n)$

			\section{Selection Sorts}
				\subsection{Selection Sort}

				\subsection{Heapsort}

				\subsection{Smoothsort}

				\subsection{Cartesian Tree Sort}

				\subsection{Tournament Sort}

				\subsection{Cycle Sort}

				\subsection{Weak-heap Sort}

			\section{Insertion Sorts}
				\subsection{Insertion Sort}

				\subsection{Shell Sort}

				\subsection{Splaysort}

				\subsection{Tree Sort}

				\subsection{Library Sort}

				\subsection{Patience Sorting}

			\section{Merge Sorts}
				\subsection{Merge Sort}
					Merge sort is a typical divide and conquer algorithm. It recursively separate an array into two subarrays, and sort while merging them. The algorithm is as following

					\begin{algorithm}[h]
						\caption{MergeSort(A, n)}
						\begin{algorithmic}[1]
							\If {$n = 1$}
								\Return A
							\Else
								\State $B \gets$ \texttt{MergeSort($A[0 .. \lfloor n/2 \rfloor], \lfloor n/2 \rfloor$)}
								\State $C \gets$ \texttt{MergeSort($A[\lceil n/2 \rceil .. n], \lceil n/2 \rceil$)}
							\EndIf
							\Return \texttt{Merge($B, C, \lfloor n/2 \rfloor, \lceil n/2 \rceil$}
						\end{algorithmic}
					\end{algorithm}

				\subsection{Cascade Merge Sort}

				\subsection{Oscillating Merge Sort}

				\subsection{Polyphase Merge Sort}

			\section{Distribution Sorts}
				\subsection{American Flag Sort}

				\subsection{Bead Sort}

				\subsection{Bucket Sort}

				\subsection{Burstsort}

				\subsection{Counting Sort}

				\subsection{interpolation Sort}

				\subsection{Pigenhole Sort}

				\subsection{Proxmap Sort}

				\subsection{Radix Sort}

				\subsection{Flashsort}

			\section{Concurrent Sorts}
				\subsection{Bitonic Sorter}

				\subsection{Batcher Odd-even Mergesort}

				\subsection{Pairwise Sorting Network}

				\subsection{Samplesort}

			\section{Hybird Sorts}
				\subsection{Block Merge Sort}

				\subsection{Timsort}

				\subsection{Spreadsort}

				\subsection{Merge-insertion Sort}

			\section{Please Don't Do that Sorts}
				\subsection{Slowsort}

				\subsection{Bogosort}

				\subsection{Stooge Sort}

		\chapter{Mathematical Algorithm}
			\section{Polynomial Multiplication}
				For given two polynomials of degree $n - 1$, the algorithm outputs the product of two polynomials.
				\begin{example}
					\begin{align}
						&(3x^3 + 2x^2 - 6x + 9) \times (-2x^3 + 7x^2 - 8x + 4)\\
						=& -6x^6 + 17x^5 + 24x^4 -60x^3 +119x^2 -96x +36
					\end{align}
					Then for input as (3, 2, -6, 9) and (-2, 7, -8, 4), the output will be (-6, 17, 2, -60, 119, -96, 36)
				\end{example}

				A naive algorithm to solve this problem will be $O(n^2)$
				\begin{algorithm}[h]
					\caption{PolyMultNaive(A, B, n)}
					\begin{algorithmic}[1]
						\State Let $C[k] = 0$ for every $k = 0, 1, \cdots, 2n - 2$
						\For {$i \gets 0$ to $n - 1$}
							\For {$j \gets 0$ to $n - 1$}
								\State $C[i + j] \gets C[i + j] + A[i] \times B[j]$
							\EndFor
						\EndFor
						\Return $C$
					\end{algorithmic}
				\end{algorithm}

				Use divide and conquer can reduce the running time. The idea is to divide the polynomial with degree of $n - 1$ (WLOG, let $n$ be even number) by two polynomials, i.e.
				\begin{equation}
					p(x) = p_H(x)x^{\frac{n}{2}} + p_L(x)
				\end{equation}
				Both $p_H$ and $p_L$ are polynomials with degree of $\frac{n}{2} - 1$, then
				\begin{align}
					p(x)q(x) &= (p_H(x)x^{\frac{n}{2}} + p_L(x)) \times (q_H(x)x^{\frac{n}{2}} + q_L(x))\\
							 &= p_Hq_H x^n + (p_Hq_L + p_Lq_H) x^{\frac{n}{2}} + p_Lq_L
				\end{align}
				Therefore
				\begin{align}
					multiply(p, q) &= multiply(p_H, q_H) x^n\\
								   &+ (multiply(p_H, q_L) + multiply(p_L, q_H)) x^{\frac{n}{2}} + multiply(p_L, q_L)\\
								   &= multiply(p_H, q_H) x^n\\
								   &+ (multiply(p_H + p_L, q_H + q_L) - multiply(p_H, q_H) - multiply(p_L, q_L)) x^{\frac{n}{2}} \\
								   &+ multiply(p_L, q_L)\\
				\end{align}

				The algorithm is as following
				\begin{algorithm}[h]
					\caption{PolyMultiDC(A, B, n)}
					\begin{algorithmic}[1]
						\If {$n = 1$}
							\Return $A[0]B[0]$
						\EndIf
						\State $A_L \gets A[0 .. n/2 - 1]$, $A_H \gets A[n/2 .. n - 1]$
						\State $B_L \gets B[0 .. n/2 - 1]$, $B_H \gets B[n/2 .. n - 1]$
						\State $C_L \gets $ \texttt{PolyMultiDC($A_L$, $B_L$, n/2)}
						\State $C_H \gets $ \texttt{PolyMultiDC($A_H$, $B_H$, n/2)}\
						\State $C_M \gets $ \texttt{PolyMultiDC($A_H$ + $A_L$, $B_H$ + $B_L$, n/2)}
						\State $C \gets $ 0 array of length $2n-1$
						\For {$i \gets 0$ to $n - 2$}
							\State $C[i] \gets C[i] + C_L[i]$
							\State $C[i + n] \gets C[i + n] + C_H[i]$
							\State $C[i + n/2] \gets C[i + n/2] + C_M[i] - C_L[i] - C_H[i]$
						\EndFor
						\Return $C$
					\end{algorithmic}
				\end{algorithm}

				The running time $T(n) = 3T(n/2) + O(n)$, $T(n) = O(n^{\lg_2 3})$

			\section{Matrices Multiplication}

			\section{Gaussian Elimination}

			\section{Curve Fitting}

			\section{Integration}

		\chapter{Searching}

		\chapter{String}
			\section{String Searching}

			\section{Pattern Matching}

			\section{Longest Common Subsequence}

			\section{Parse}

			\section{Optimal Caching}

			\section{File Compression}

			\section{Cryptology}

		\chapter{Data Structures}
			\section{Elementary Data Structures}

			\section{Hash Tables}

			\section{Binary Search Trees}

			\section{Red-Black Trees}

			\section{B-Trees}

			\section{Fibonacci Heaps}

			\section{van Emde Boas Trees}

		\chapter{NP and Computational Intractability}
			\section{P, NP and Co-NP}
				\begin{definition}[Decision Problem]
					A problem $X$ is a \textbf{decision problem} if the output is either 0 or 1 (yes/no). Further, a 
				\end{definition}

				The input of a problem can always be encoded into a binary string. The size of an input is the length of the encoded string $s$ for the input.

				\notice{For optimization problem $X$, we can always define a decision version $X^\prime$, by giving a threshold and ask if the objective function can satisfy that threshold or not. If that decision version $X^\prime$ can be solved in polynomial time, we can solve the original problem $X$ in polynomial time.}

				\begin{definition}[Polynomial running time]
					An algorithm $A$ has a \textbf{polynomial running time} if there is a polynomial function $p(\cdot)$ such that $\forall s$, the algorithm $A$ terminates on $s$ in at most $p(|\cdot|)$ steps.
				\end{definition}

				\begin{definition}[P]
					The complexity class $P$ is the set of decision problems $X$ that can be solved in polynomial time.
				\end{definition}

				\begin{example}
					Shortest path, minimum spanning tree, determine if an integer is prime number, those are in P.
				\end{example}

				\begin{definition}[Certificate, Certifier]
					In order to check the an algorithm $A$, for a binary string $s$, such that $s \in A$, there is another separated algorithm $B$ uses $s$ and $t$, a separated string that contains the evidence that $s$ is a ``yes'' instance of $X$, as input string and output (another) ``yes''. Then this string $t$ is called \textbf{certificate}, this separated algorithm $B(s, t)$ is called \textbf{certifier}. 
				\end{definition}

				\notice{Certificate is like a solution, and certifier is the algorithm to prove solution is correct. But both with fancy terminology}

				\begin{example}
					For Independent set problem. The input ($s$) is a graph, the certificate ($t$) will be a set of size $k$, and the certifier will be an algorithm to check the given set is an independent set.
				\end{example}

				\begin{example}
					For 3-SAT problem. The input $s$ will be the 3-CNF (conjunctive normal form), the certificate $t$ will be the assignment of true values for each terms in the 3-CNF. The certifier will be the algorithm calculates the true value of the clause given the true values in $t$.
				\end{example}

				\begin{definition}[Efficient Certifier]
					$B$ is an efficient certifier for problem $X$ if
					\begin{itemize}
						\item $B$ runs in polynomial time with input $s$ and $t$. ($s$ is the input of origin algorithm and $t$ is the certificate)
						\item There is a polynomial function $p(|\cdot|)$ such that for every string $s$, we have $s \in X$ ($s$ is a ``yes'' solution for $X$) and $B(s, t) = yes$ (the certifier returns ``yes'')
					\end{itemize}
				\end{definition}

				\begin{definition}[NP]
					The complexity class $NP$ is the set of all problems for which there exists an efficient certifier.
				\end{definition}

				From the definition of NP we can immediately know that $P \subseteq NP$. Because all the problems in P can satisfy the definition of NP. But is there a problem that $X \in NP$ and $X \notin P$? Solve that problem and you will win 100 million dollars and the chance to be remember forever.

				\notice{NP stands for \textbf{Non-deterministic Polynomial} time}

				To prove if a problem is NP, we need to have a certifier that can output ``yes'' given the certificate in polynomial time. Otherwise it is not in NP

				\begin{example}
					Given a graph $G = (V, E)$ and an integer $t > 0$, whether the minimum vertex cover of $G$ has size at least t. This problem is \textbf{unlikely} in NP. Reason is, the ``yes'' instance is ``all minimum vertex cover has size at least $t$'', that is not \textbf{likely to be} solvable in polynomial time for it need to find all minimum vertex covers.
				\end{example}

				\begin{definition}[Co-NP]
					For a problem $X$, the problem $\bar{X}$ is the problem such that $s \in \bar{X} \iff s \notin X$. Then \textbf{Co-NP} is the set of decision problems $X$ such that $\bar{X} \in NP$.
				\end{definition}

				\begin{example}[Tautology Problem]
					Given a boolean formula, determine wither the formula is always evaluates to 1. This is a problem in Co-NP. Because we can have a polynomial time certifier to confirm that an instance is not a tautology.
				\end{example}

				\begin{example}
					Given two boolean formulas, to determine whether or not they are equivalent. This is a problem in Co-NP. Because if we have one instance such that the output is ``no'', then we can easily prove there are counter examples in origin algorithm.
				\end{example}

				\notice{If the instance is ``easy'' to prove to be true, then it is in NP, if the counter instance is ``easy'' to prove to be the algorithm it be false, then it is in Co-NP}

				Relation between $P$, $NP$ and $Co-NP$ is as following
				\begin{itemize}
					\item $P \subseteq NP$
					\item $P \subseteq Co-NP$
					\item $P = NP?$ is not known
					\item $P = Co-NP?$ is not known
					\item $NP = Co-NP?$ is not known
					\item If $P = NP$ then $P = Co-NP$
				\end{itemize}

			\section{Polynomial-Time Reductions}
				\begin{definition}[Polynomial-time reducible]
					Given an algorithm $A$ that solves problem $Y$, if any instance of problem $X$ can be solved using a polynomial number of standard computational steps, plus a polynomial number of calls to algorithm $A$, then we say $X$ is \textbf{polynomial-time reducible} to $Y$, denoted as 
					\begin{equation}
						X \le_P Y
					\end{equation}
				\end{definition}

				In a more intuitive way, $X \le_P Y$ means $X$ can't be more difficult than $Y$. Solving $X$ can be ``transforming'' $X$ into an equivalent $Y$ in polynomial number of steps and then solve it by calling $Y$ polynomial number of times, usually one time. Thus, if $X \le_P Y$,
				\begin{itemize}
					\item If $Y$ can be efficiently solved, $X$ can be efficiently solved.
					\item If $X$ cannot be efficiently solve, $Y$ cannot be efficiently solved.
				\end{itemize}

				\notice{To prove $X \le_P Y$, usually we already have an algorithm for $Y$, could be polynomial or not.}

				\begin{lemma}
					Hamiltonian-Path $\le_P$ Hamiltonian-Cycle
				\end{lemma}
				\begin{proof}
					For graph $G = (V, E)$, $s$ and $t$ are two vertices that in $V$, define a new graph $G^\prime = (V \cup \{v\}, E \cup \{(u, s)\} \cup \{(t, u)\}$. To solve the Hamilton-Path from $s$ to $t$ in graph $G$, say $p_{st}$, is equivalent to solving Hamilton-Cycle problem in $G^\prime$, i.e., find $sp_{st}te_{tu}ue_{us}$.
				\end{proof}

				\begin{lemma}
					Hamiltonian-Cycle $\le_P$ Hamiltonian-Path
				\end{lemma}

				\begin{proof}
					For vertex $s$, make a copy and denote it as $s^\prime$, $s^\prime$ is connected to all the vertices that $s$ connected. Solving the Hamiltonian-Cycle problem is equivalent to solving the Hamiltonian-Path problem from $s$ to $s^\prime$
				\end{proof}

				\begin{lemma}
					Hamiltonian-Path $\le_P$ degree-3 spanning tree
				\end{lemma}

				\begin{proof}
					In graph $G$, for vertex $s$ and $t$, add vertices $s^\prime$, $s^{\prime\prime}$, $t^\prime$, $t^{\prime\prime}$ and edges $(s, s^\prime)$, $(s, s^{\prime\prime})$, $(t, t^\prime)$, $(t, t^{\prime\prime})$. For all the other vertex $u \in V\setminus \{s\} \setminus \{t\}$, add vertices $u^\prime$ and edge $(u, u^\prime)$ to the graph. Then solving Hamiltonian-Path problem is equivalent to solve the degree spanning tree problem in this new $G$.
				\end{proof}

				\begin{lemma}
					Vertex Cover $\le_P$ Set Cover
				\end{lemma}

				\begin{proof}
					
				\end{proof}

				\begin{lemma}
					Set Cover $\le_P$ Vertex Cover
				\end{lemma}

				\begin{proof}
					
				\end{proof}

				\begin{lemma}
					Clique $\le_P$ Independent Set
				\end{lemma}

				\begin{proof}
					$S$ is a clique in $G = (V, E)$ iff $S$ is an independent set in $\bar{G} = (V, \bar{E})$
				\end{proof}

				\begin{lemma}
					Independent Set $\le_P$ Clique
				\end{lemma}

				\begin{proof}
					$S$ is an independent set in $G = (V, E)$ iff $S$ is a clique in $\bar{G} = (V, \bar{E})$
				\end{proof}

				\begin{lemma}
					Vertex-Cover $\le_P$ Independent Set
				\end{lemma}

				\begin{proof}
					$S$ is a vertex-cover of $G = (V, E)$ iff $V \setminus S$ is an independent set of $G$
				\end{proof}

				\begin{lemma}
					3-Coloring $\le_P$ 4-Coloring
				\end{lemma}

				\begin{proof}
					For a graph $G = (V, E)$, define a new graph $G^\prime = (V \cup \{u\}, E \cup \{(u, v) | \forall ~ v \in V\})$. Solving the 3-Coloring problem is equivalent to solving the 4-Coloring problem in $G^\prime$
				\end{proof}

				\begin{lemma}
					Independent Set $\le_P$ Set Packing
				\end{lemma}

			\section{NP-Completeness}
				\begin{definition}[NP-Completeness]
					A problem $X$ is called \textbf{NP-Complete} if
					\begin{itemize}
						\item $X \in NP$, and
						\item $Y \le_P X, \forall ~ Y \in NP$
					\end{itemize}
				\end{definition}

				An intuitive explanation will be, we can regard problems that is NP-Complete to be the most difficult problems in NP. If any of those can be solved in polynomial time, then all problems in NP can be solved in polynomial time.

				\begin{theorem}[Cook's Theorem]
					Circuit Satisfiability is NP-Complete.
				\end{theorem}

				\begin{proof}
					
				\end{proof}

				\begin{theorem}
					If $X$ is NP-Complete and $X \in P$, then $P = NP$
				\end{theorem}

				\begin{proof}
					Direct result from Cook's theorem.
				\end{proof}

			\section{NP-Complete Problems}

	\part*{Special Topic: Computational Geometry}\label{CG}
		\addcontentsline{toc}{part}{Special Topic: Computational Geometry}
		\chapter{Convex Hull}
			\section{Computing Slope Statistics}

			\section{Convexity}

			\section{Graham's Scan}

			\section{Turning and orientations}

		\chapter{Intersections}

		\chapter{Triangulation and Partitioning}
			\section{Polygon Triangulation}
				\subsection{Types of Polygons}
					\begin{definition}[simple polygon]
						A \textbf{simple polygon} is a closed polygonal curve without self-intersection.
					\end{definition}

					\begin{figure}[h!]
						\centering
						\begin{tikzpicture}[scale=0.6]
							\draw (0, 0) -- (3, -1) -- (4, 3) -- (2, 4) -- (0, 0);
							\draw (6, 3) -- (8, -1) -- (9, 2) -- (5.5, 0) -- (6, 3);
							\node at (2, -1.5) [below] {Simple Polygon};
							\node at (7.5, -1.5) [below] {Non-simple Polygon};
						\end{tikzpicture}
					\end{figure}

					Polygons are basic building blocks in most geometric applications. It can model arbitrarily complex shapes, and apply simple algorithms and algebraic representation/manipulation.

				\subsection{Triangulation}
					\begin{definition}[Triangulation]
						\textbf{Triangulation} is to partition polygon $P$ into non-overlapping triangles using diagonals only. It reduces complex shapes to collection of simpler shapes. Every simple $n$-gon admits a triangulation which has $n-2$ triangles.				
					\end{definition}

					\begin{figure}[h!]
						\centering
						\begin{tikzpicture}[scale=0.6]
							\draw [thick] (0, 0) -- (3, 4) -- (2, 5) -- (3.5, 5.5) -- (1, 6.4) -- (-1, 5) -- (-2, 4.5) -- (-1.3, 2.4) -- (-2, 1) -- (0, 0);
							\draw (-2, 1) -- (3, 4);
							\draw (-1.3, 2.4) -- (3, 4);
							\draw (-2, 4.5) -- (3, 4);
							\draw (-1, 5) -- (3, 4);
							\draw (-1, 5) -- (2, 5);
							\draw (2, 5) -- (1, 6.4);
							\node at (0, 0) [below] {Triangulation};
						\end{tikzpicture}
					\end{figure}

					\begin{theorem}
						Every polygon has a triangulation				
					\end{theorem}

					\begin{lemma}
						Every polygon with more than three vertices has a diagonal.
					\end{lemma}

					\begin{proof}
						(by Meisters, 1975) Let $P$ be a polygon with more than three vertices. Every vertex of a $P$ is either \textit{convex} or \textit{concave}. W.L.O.G.(any polygon must has convex corner) Assume $p$ is a convex vertex. Denote the neighbors of $p$ as $q$ and $r$. If $\bar{qr}$ is a diagonal, done, and we call $\triangle{pqr}$ is an \textit{ear}. If $\triangle{pqr}$ is not an ear, it means at least one vertex is inside $\triangle{pqr}$, assume among those vertexes inside $\triangle{pqr}$, $s$ is a vertex closest to $p$, then $\bar{ps}$ is a diagonal.
					\end{proof}

				\subsection{Art Gallery Theorem}
					\begin{theorem}
						Every $n$-gon can be guarded with $\lfloor \frac{n}{3} \rfloor$ vertex guards
					\end{theorem}

					\begin{lemma}
						Triangulation graph can be 3-colored.
					\end{lemma}

					\begin{problem}
						The floor plan of an art gallery modeled as a simple polygon with $n$ vertices, there are guards which is stationed at fixed positions with 360 degree vision but cannot see through the walls. How many guards does the art gallery need for the security? (Fun fact: This problem was posted to Vasek Chvatal by Victor Klee in 1973).				
					\end{problem}

					\begin{proof}
						- $P$ plus triangulation is a planar graph\\
						- 3-coloring means there exist a 3-partition for vertices that no edge or diagonal has both endpoints within the same set of vertices.\\
						- Proof by Induction:\\
						\indent - Remove an ear (there will always exist ear) \\
						\indent - Inductively 3-color the rest\\
						\indent - Put ear back, coloring new vertex with the label not used by the boundary diagonal.
					\end{proof}

				\subsection{Triangulation Algorithms}

		\chapter{Voronoi Diagrams}

		\chapter{Arrangement and Duality}

		\chapter{Delaunay Triangulations}

		\chapter{Search}

		\chapter{Motion Planning}

		\chapter{Quadtrees}

		\chapter{Visibility Graphs}

	\part{Stochastic Methods}\label{Stochastic}
		\chapter{Markov Chain}

		\chapter{Queueing Theory}

	\part{Inventory Theory}\label{Inventory}

	\part{Game Theory}\label{Game}

	\part{Simulation}\label{Sim}
		\chapter{Random Numbers}

		\chapter{Monte Carlo Simulation}

		\chapter{Discrete Event Simulation}

\end{document}